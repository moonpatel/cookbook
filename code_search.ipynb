{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DEF_PREFIXES = [\"def \", \"async def \"]\n",
    "NEWLINE = \"\\n\"\n",
    "\n",
    "\n",
    "def get_function_name(code: str):\n",
    "    \"\"\"\n",
    "    Extract function name from a line beginning with 'def' or 'async def'.\n",
    "    \"\"\"\n",
    "    for prefix in DEF_PREFIXES:\n",
    "        if code.startswith(prefix):\n",
    "            return code[len(prefix) : code.index(\"(\")]\n",
    "\n",
    "\n",
    "def get_until_no_space(all_lines, i):\n",
    "    \"\"\"\n",
    "    Get all lines until a line outside the function definition is found.\n",
    "    \"\"\"\n",
    "    ret = [all_lines[i]]\n",
    "    for j in range(i + 1, len(all_lines)):\n",
    "        if len(all_lines[j]) == 0 or all_lines[j][0] in [\" \", \"\\t\", \")\"]:\n",
    "            ret.append(all_lines[j])\n",
    "        else:\n",
    "            break\n",
    "    return NEWLINE.join(ret)\n",
    "\n",
    "\n",
    "def get_functions(filepath):\n",
    "    \"\"\"\n",
    "    Get all functions in a Python file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        all_lines = file.read().replace(\"\\r\", NEWLINE).split(NEWLINE)\n",
    "        for i, l in enumerate(all_lines):\n",
    "            for prefix in DEF_PREFIXES:\n",
    "                if l.startswith(prefix):\n",
    "                    code = get_until_no_space(all_lines, i)\n",
    "                    function_name = get_function_name(code)\n",
    "                    yield {\n",
    "                        \"code\": code,\n",
    "                        \"function_name\": function_name,\n",
    "                        \"filepath\": filepath,\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "\n",
    "def extract_functions_from_repo(code_root):\n",
    "    \"\"\"\n",
    "    Extract all .py functions from the repository.\n",
    "    \"\"\"\n",
    "    code_files = list(code_root.glob(\"**/*.py\"))\n",
    "\n",
    "    num_files = len(code_files)\n",
    "    print(f\"Total number of .py files: {num_files}\")\n",
    "\n",
    "    if num_files == 0:\n",
    "        print(\"Verify openai-python repo exists and code_root is set correctly.\")\n",
    "        return None\n",
    "\n",
    "    all_funcs = [\n",
    "        func for code_file in code_files for func in get_functions(str(code_file))\n",
    "    ]\n",
    "\n",
    "    num_funcs = len(all_funcs)\n",
    "    print(f\"Total number of functions extracted: {num_funcs}\")\n",
    "\n",
    "    return all_funcs\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of .py files: 321\n",
      "Total number of functions extracted: 350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'code': 'def test_pydantic_v1(session: nox.Session) -> None:\\n    session.install(\"-r\", \"requirements-dev.lock\")\\n    session.install(\"pydantic<2\")\\n\\n    session.run(\"pytest\", \"--showlocals\", \"--ignore=tests/functional\", *session.posargs)\\n',\n",
       "  'function_name': 'test_pydantic_v1',\n",
       "  'filepath': '/home/moonpatel/openai-python/noxfile.py'},\n",
       " {'code': 'async def transform(\\n    data: _T,\\n    expected_type: object,\\n    use_async: bool,\\n) -> _T:\\n    if use_async:\\n        return await _async_transform(data, expected_type=expected_type)\\n\\n    return _transform(data, expected_type=expected_type)\\n\\n',\n",
       "  'function_name': 'transform',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_top_level_alias(use_async: bool) -> None:\\n    assert await transform({\"foo_bar\": \"hello\"}, expected_type=Foo1, use_async=use_async) == {\"fooBar\": \"hello\"}\\n\\n',\n",
       "  'function_name': 'test_top_level_alias',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_recursive_typeddict(use_async: bool) -> None:\\n    assert await transform({\"bar\": {\"this_thing\": 1}}, Foo2, use_async) == {\"bar\": {\"this__thing\": 1}}\\n    assert await transform({\"bar\": {\"baz\": {\"my_baz\": \"foo\"}}}, Foo2, use_async) == {\"bar\": {\"Baz\": {\"myBaz\": \"foo\"}}}\\n\\n',\n",
       "  'function_name': 'test_recursive_typeddict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_list_of_typeddict(use_async: bool) -> None:\\n    result = await transform({\"things\": [{\"my_field\": \"foo\"}, {\"my_field\": \"foo2\"}]}, Foo3, use_async)\\n    assert result == {\"things\": [{\"myField\": \"foo\"}, {\"myField\": \"foo2\"}]}\\n\\n',\n",
       "  'function_name': 'test_list_of_typeddict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_union_of_typeddict(use_async: bool) -> None:\\n    assert await transform({\"foo\": {\"foo_bar\": \"bar\"}}, Foo4, use_async) == {\"foo\": {\"fooBar\": \"bar\"}}\\n    assert await transform({\"foo\": {\"foo_baz\": \"baz\"}}, Foo4, use_async) == {\"foo\": {\"fooBaz\": \"baz\"}}\\n    assert await transform({\"foo\": {\"foo_baz\": \"baz\", \"foo_bar\": \"bar\"}}, Foo4, use_async) == {\\n        \"foo\": {\"fooBaz\": \"baz\", \"fooBar\": \"bar\"}\\n    }\\n\\n',\n",
       "  'function_name': 'test_union_of_typeddict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_union_of_list(use_async: bool) -> None:\\n    assert await transform({\"foo\": {\"foo_bar\": \"bar\"}}, Foo5, use_async) == {\"FOO\": {\"fooBar\": \"bar\"}}\\n    assert await transform(\\n        {\\n            \"foo\": [\\n                {\"foo_baz\": \"baz\"},\\n                {\"foo_baz\": \"baz\"},\\n            ]\\n        },\\n        Foo5,\\n        use_async,\\n    ) == {\"FOO\": [{\"fooBaz\": \"baz\"}, {\"fooBaz\": \"baz\"}]}\\n\\n',\n",
       "  'function_name': 'test_union_of_list',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_includes_unknown_keys(use_async: bool) -> None:\\n    assert await transform({\"bar\": \"bar\", \"baz_\": {\"FOO\": 1}}, Foo6, use_async) == {\\n        \"Bar\": \"bar\",\\n        \"baz_\": {\"FOO\": 1},\\n    }\\n\\n',\n",
       "  'function_name': 'test_includes_unknown_keys',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_ignores_invalid_input(use_async: bool) -> None:\\n    assert await transform({\"bar\": \"<foo>\"}, Foo7, use_async) == {\"bAr\": \"<foo>\"}\\n    assert await transform({\"foo\": \"<foo>\"}, Foo7, use_async) == {\"foo\": \"<foo>\"}\\n\\n',\n",
       "  'function_name': 'test_ignores_invalid_input',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_iso8601_format(use_async: bool) -> None:\\n    dt = datetime.fromisoformat(\"2023-02-23T14:16:36.337692+00:00\")\\n    assert await transform({\"foo\": dt}, DatetimeDict, use_async) == {\"foo\": \"2023-02-23T14:16:36.337692+00:00\"}  # type: ignore[comparison-overlap]\\n\\n    dt = dt.replace(tzinfo=None)\\n    assert await transform({\"foo\": dt}, DatetimeDict, use_async) == {\"foo\": \"2023-02-23T14:16:36.337692\"}  # type: ignore[comparison-overlap]\\n\\n    assert await transform({\"foo\": None}, DateDict, use_async) == {\"foo\": None}  # type: ignore[comparison-overlap]\\n    assert await transform({\"foo\": date.fromisoformat(\"2023-02-23\")}, DateDict, use_async) == {\"foo\": \"2023-02-23\"}  # type: ignore[comparison-overlap]\\n\\n',\n",
       "  'function_name': 'test_iso8601_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_optional_iso8601_format(use_async: bool) -> None:\\n    dt = datetime.fromisoformat(\"2023-02-23T14:16:36.337692+00:00\")\\n    assert await transform({\"bar\": dt}, DatetimeDict, use_async) == {\"bar\": \"2023-02-23T14:16:36.337692+00:00\"}  # type: ignore[comparison-overlap]\\n\\n    assert await transform({\"bar\": None}, DatetimeDict, use_async) == {\"bar\": None}\\n\\n',\n",
       "  'function_name': 'test_optional_iso8601_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_required_iso8601_format(use_async: bool) -> None:\\n    dt = datetime.fromisoformat(\"2023-02-23T14:16:36.337692+00:00\")\\n    assert await transform({\"required\": dt}, DatetimeDict, use_async) == {\\n        \"required\": \"2023-02-23T14:16:36.337692+00:00\"\\n    }  # type: ignore[comparison-overlap]\\n\\n    assert await transform({\"required\": None}, DatetimeDict, use_async) == {\"required\": None}\\n\\n',\n",
       "  'function_name': 'test_required_iso8601_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_union_datetime(use_async: bool) -> None:\\n    dt = datetime.fromisoformat(\"2023-02-23T14:16:36.337692+00:00\")\\n    assert await transform({\"union\": dt}, DatetimeDict, use_async) == {  # type: ignore[comparison-overlap]\\n        \"union\": \"2023-02-23T14:16:36.337692+00:00\"\\n    }\\n\\n    assert await transform({\"union\": \"foo\"}, DatetimeDict, use_async) == {\"union\": \"foo\"}\\n\\n',\n",
       "  'function_name': 'test_union_datetime',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_nested_list_iso6801_format(use_async: bool) -> None:\\n    dt1 = datetime.fromisoformat(\"2023-02-23T14:16:36.337692+00:00\")\\n    dt2 = parse_datetime(\"2022-01-15T06:34:23Z\")\\n    assert await transform({\"list_\": [dt1, dt2]}, DatetimeDict, use_async) == {  # type: ignore[comparison-overlap]\\n        \"list_\": [\"2023-02-23T14:16:36.337692+00:00\", \"2022-01-15T06:34:23+00:00\"]\\n    }\\n\\n',\n",
       "  'function_name': 'test_nested_list_iso6801_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_datetime_custom_format(use_async: bool) -> None:\\n    dt = parse_datetime(\"2022-01-15T06:34:23Z\")\\n\\n    result = await transform(dt, Annotated[datetime, PropertyInfo(format=\"custom\", format_template=\"%H\")], use_async)\\n    assert result == \"06\"  # type: ignore[comparison-overlap]\\n\\n',\n",
       "  'function_name': 'test_datetime_custom_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_datetime_with_alias(use_async: bool) -> None:\\n    assert await transform({\"required_prop\": None}, DateDictWithRequiredAlias, use_async) == {\"prop\": None}  # type: ignore[comparison-overlap]\\n    assert await transform(\\n        {\"required_prop\": date.fromisoformat(\"2023-02-23\")}, DateDictWithRequiredAlias, use_async\\n    ) == {\"prop\": \"2023-02-23\"}  # type: ignore[comparison-overlap]\\n\\n',\n",
       "  'function_name': 'test_datetime_with_alias',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_model_to_dictionary(use_async: bool) -> None:\\n    assert await transform(MyModel(foo=\"hi!\"), Any, use_async) == {\"foo\": \"hi!\"}\\n    assert await transform(MyModel.construct(foo=\"hi!\"), Any, use_async) == {\"foo\": \"hi!\"}\\n\\n',\n",
       "  'function_name': 'test_pydantic_model_to_dictionary',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_empty_model(use_async: bool) -> None:\\n    assert await transform(MyModel.construct(), Any, use_async) == {}\\n\\n',\n",
       "  'function_name': 'test_pydantic_empty_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_unknown_field(use_async: bool) -> None:\\n    assert await transform(MyModel.construct(my_untyped_field=True), Any, use_async) == {\"my_untyped_field\": True}\\n\\n',\n",
       "  'function_name': 'test_pydantic_unknown_field',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_mismatched_types(use_async: bool) -> None:\\n    model = MyModel.construct(foo=True)\\n    if PYDANTIC_V2:\\n        with pytest.warns(UserWarning):\\n            params = await transform(model, Any, use_async)\\n    else:\\n        params = await transform(model, Any, use_async)\\n    assert params == {\"foo\": True}\\n\\n',\n",
       "  'function_name': 'test_pydantic_mismatched_types',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_mismatched_object_type(use_async: bool) -> None:\\n    model = MyModel.construct(foo=MyModel.construct(hello=\"world\"))\\n    if PYDANTIC_V2:\\n        with pytest.warns(UserWarning):\\n            params = await transform(model, Any, use_async)\\n    else:\\n        params = await transform(model, Any, use_async)\\n    assert params == {\"foo\": {\"hello\": \"world\"}}\\n\\n',\n",
       "  'function_name': 'test_pydantic_mismatched_object_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_nested_objects(use_async: bool) -> None:\\n    model = ModelNestedObjects.construct(nested={\"foo\": \"stainless\"})\\n    assert isinstance(model.nested, MyModel)\\n    assert await transform(model, Any, use_async) == {\"nested\": {\"foo\": \"stainless\"}}\\n\\n',\n",
       "  'function_name': 'test_pydantic_nested_objects',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_pydantic_default_field(use_async: bool) -> None:\\n    # should be excluded when defaults are used\\n    model = ModelWithDefaultField.construct()\\n    assert model.with_none_default is None\\n    assert model.with_str_default == \"foo\"\\n    assert await transform(model, Any, use_async) == {}\\n\\n    # should be included when the default value is explicitly given\\n    model = ModelWithDefaultField.construct(with_none_default=None, with_str_default=\"foo\")\\n    assert model.with_none_default is None\\n    assert model.with_str_default == \"foo\"\\n    assert await transform(model, Any, use_async) == {\"with_none_default\": None, \"with_str_default\": \"foo\"}\\n\\n    # should be included when a non-default value is explicitly given\\n    model = ModelWithDefaultField.construct(with_none_default=\"bar\", with_str_default=\"baz\")\\n    assert model.with_none_default == \"bar\"\\n    assert model.with_str_default == \"baz\"\\n    assert await transform(model, Any, use_async) == {\"with_none_default\": \"bar\", \"with_str_default\": \"baz\"}\\n\\n',\n",
       "  'function_name': 'test_pydantic_default_field',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_iterable_of_dictionaries(use_async: bool) -> None:\\n    assert await transform({\"foo\": [{\"foo_baz\": \"bar\"}]}, TypedDictIterableUnion, use_async) == {\\n        \"FOO\": [{\"fooBaz\": \"bar\"}]\\n    }\\n    assert cast(Any, await transform({\"foo\": ({\"foo_baz\": \"bar\"},)}, TypedDictIterableUnion, use_async)) == {\\n        \"FOO\": [{\"fooBaz\": \"bar\"}]\\n    }\\n\\n    def my_iter() -> Iterable[Baz8]:\\n        yield {\"foo_baz\": \"hello\"}\\n        yield {\"foo_baz\": \"world\"}\\n\\n    assert await transform({\"foo\": my_iter()}, TypedDictIterableUnion, use_async) == {\\n        \"FOO\": [{\"fooBaz\": \"hello\"}, {\"fooBaz\": \"world\"}]\\n    }\\n\\n',\n",
       "  'function_name': 'test_iterable_of_dictionaries',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_iterable_union_str(use_async: bool) -> None:\\n    assert await transform({\"foo\": \"bar\"}, TypedDictIterableUnionStr, use_async) == {\"FOO\": \"bar\"}\\n    assert cast(Any, await transform(iter([{\"foo_baz\": \"bar\"}]), Union[str, Iterable[Baz8]], use_async)) == [\\n        {\"fooBaz\": \"bar\"}\\n    ]\\n\\n',\n",
       "  'function_name': 'test_iterable_union_str',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'async def test_base64_file_input(use_async: bool) -> None:\\n    # strings are left as-is\\n    assert await transform({\"foo\": \"bar\"}, TypedDictBase64Input, use_async) == {\"foo\": \"bar\"}\\n\\n    # pathlib.Path is automatically converted to base64\\n    assert await transform({\"foo\": SAMPLE_FILE_PATH}, TypedDictBase64Input, use_async) == {\\n        \"foo\": \"SGVsbG8sIHdvcmxkIQo=\"\\n    }  # type: ignore[comparison-overlap]\\n\\n    # io instances are automatically converted to base64\\n    assert await transform({\"foo\": io.StringIO(\"Hello, world!\")}, TypedDictBase64Input, use_async) == {\\n        \"foo\": \"SGVsbG8sIHdvcmxkIQ==\"\\n    }  # type: ignore[comparison-overlap]\\n    assert await transform({\"foo\": io.BytesIO(b\"Hello, world!\")}, TypedDictBase64Input, use_async) == {\\n        \"foo\": \"SGVsbG8sIHdvcmxkIQ==\"\\n    }  # type: ignore[comparison-overlap]\\n',\n",
       "  'function_name': 'test_base64_file_input',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_transform.py'},\n",
       " {'code': 'def test_removes_files_from_input() -> None:\\n    query = {\"foo\": \"bar\"}\\n    assert extract_files(query, paths=[]) == []\\n    assert query == {\"foo\": \"bar\"}\\n\\n    query2 = {\"foo\": b\"Bar\", \"hello\": \"world\"}\\n    assert extract_files(query2, paths=[[\"foo\"]]) == [(\"foo\", b\"Bar\")]\\n    assert query2 == {\"hello\": \"world\"}\\n\\n    query3 = {\"foo\": {\"foo\": {\"bar\": b\"Bar\"}}, \"hello\": \"world\"}\\n    assert extract_files(query3, paths=[[\"foo\", \"foo\", \"bar\"]]) == [(\"foo[foo][bar]\", b\"Bar\")]\\n    assert query3 == {\"foo\": {\"foo\": {}}, \"hello\": \"world\"}\\n\\n    query4 = {\"foo\": {\"bar\": b\"Bar\", \"baz\": \"foo\"}, \"hello\": \"world\"}\\n    assert extract_files(query4, paths=[[\"foo\", \"bar\"]]) == [(\"foo[bar]\", b\"Bar\")]\\n    assert query4 == {\"hello\": \"world\", \"foo\": {\"baz\": \"foo\"}}\\n\\n',\n",
       "  'function_name': 'test_removes_files_from_input',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_extract_files.py'},\n",
       " {'code': 'def test_multiple_files() -> None:\\n    query = {\"documents\": [{\"file\": b\"My first file\"}, {\"file\": b\"My second file\"}]}\\n    assert extract_files(query, paths=[[\"documents\", \"<array>\", \"file\"]]) == [\\n        (\"documents[][file]\", b\"My first file\"),\\n        (\"documents[][file]\", b\"My second file\"),\\n    ]\\n    assert query == {\"documents\": [{}, {}]}\\n\\n',\n",
       "  'function_name': 'test_multiple_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_extract_files.py'},\n",
       " {'code': 'def test_ignores_incorrect_paths(\\n    query: dict[str, object],\\n    paths: Sequence[Sequence[str]],\\n    expected: list[tuple[str, FileTypes]],\\n) -> None:\\n    assert extract_files(query, paths=paths) == expected\\n',\n",
       "  'function_name': 'test_ignores_incorrect_paths',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_extract_files.py'},\n",
       " {'code': 'def test_response_parse_mismatched_basemodel(client: OpenAI) -> None:\\n    response = LegacyAPIResponse(\\n        raw=httpx.Response(200, content=b\"foo\"),\\n        client=client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    with pytest.raises(\\n        TypeError,\\n        match=\"Pydantic models must subclass our base model type, e.g. `from openai import BaseModel`\",\\n    ):\\n        response.parse(to=PydanticModel)\\n\\n',\n",
       "  'function_name': 'test_response_parse_mismatched_basemodel',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_legacy_response.py'},\n",
       " {'code': 'def test_response_parse_custom_stream(client: OpenAI) -> None:\\n    response = LegacyAPIResponse(\\n        raw=httpx.Response(200, content=b\"foo\"),\\n        client=client,\\n        stream=True,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    stream = response.parse(to=Stream[int])\\n    assert stream._cast_to == int\\n\\n',\n",
       "  'function_name': 'test_response_parse_custom_stream',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_legacy_response.py'},\n",
       " {'code': 'def test_response_parse_custom_model(client: OpenAI) -> None:\\n    response = LegacyAPIResponse(\\n        raw=httpx.Response(200, content=json.dumps({\"foo\": \"hello!\", \"bar\": 2})),\\n        client=client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    obj = response.parse(to=CustomModel)\\n    assert obj.foo == \"hello!\"\\n    assert obj.bar == 2\\n\\n',\n",
       "  'function_name': 'test_response_parse_custom_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_legacy_response.py'},\n",
       " {'code': 'def test_response_parse_annotated_type(client: OpenAI) -> None:\\n    response = LegacyAPIResponse(\\n        raw=httpx.Response(200, content=json.dumps({\"foo\": \"hello!\", \"bar\": 2})),\\n        client=client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    obj = response.parse(\\n        to=cast(\"type[CustomModel]\", Annotated[CustomModel, \"random metadata\"]),\\n    )\\n    assert obj.foo == \"hello!\"\\n    assert obj.bar == 2\\n',\n",
       "  'function_name': 'test_response_parse_annotated_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_legacy_response.py'},\n",
       " {'code': 'def test_basic(value: object) -> None:\\n    m = BasicModel.construct(foo=value)\\n    assert m.foo == value\\n\\n',\n",
       "  'function_name': 'test_basic',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_directly_nested_model() -> None:\\n    class NestedModel(BaseModel):\\n        nested: BasicModel\\n\\n    m = NestedModel.construct(nested={\"foo\": \"Foo!\"})\\n    assert m.nested.foo == \"Foo!\"\\n\\n    # mismatched types\\n    m = NestedModel.construct(nested=\"hello!\")\\n    assert m.nested == \"hello!\"\\n\\n',\n",
       "  'function_name': 'test_directly_nested_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_optional_nested_model() -> None:\\n    class NestedModel(BaseModel):\\n        nested: Optional[BasicModel]\\n\\n    m1 = NestedModel.construct(nested=None)\\n    assert m1.nested is None\\n\\n    m2 = NestedModel.construct(nested={\"foo\": \"bar\"})\\n    assert m2.nested is not None\\n    assert m2.nested.foo == \"bar\"\\n\\n    # mismatched types\\n    m3 = NestedModel.construct(nested={\"foo\"})\\n    assert isinstance(cast(Any, m3.nested), set)\\n    assert m3.nested == {\"foo\"}\\n\\n',\n",
       "  'function_name': 'test_optional_nested_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_list_nested_model() -> None:\\n    class NestedModel(BaseModel):\\n        nested: List[BasicModel]\\n\\n    m = NestedModel.construct(nested=[{\"foo\": \"bar\"}, {\"foo\": \"2\"}])\\n    assert m.nested is not None\\n    assert isinstance(m.nested, list)\\n    assert len(m.nested) == 2\\n    assert m.nested[0].foo == \"bar\"\\n    assert m.nested[1].foo == \"2\"\\n\\n    # mismatched types\\n    m = NestedModel.construct(nested=True)\\n    assert cast(Any, m.nested) is True\\n\\n    m = NestedModel.construct(nested=[False])\\n    assert cast(Any, m.nested) == [False]\\n\\n',\n",
       "  'function_name': 'test_list_nested_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_optional_list_nested_model() -> None:\\n    class NestedModel(BaseModel):\\n        nested: Optional[List[BasicModel]]\\n\\n    m1 = NestedModel.construct(nested=[{\"foo\": \"bar\"}, {\"foo\": \"2\"}])\\n    assert m1.nested is not None\\n    assert isinstance(m1.nested, list)\\n    assert len(m1.nested) == 2\\n    assert m1.nested[0].foo == \"bar\"\\n    assert m1.nested[1].foo == \"2\"\\n\\n    m2 = NestedModel.construct(nested=None)\\n    assert m2.nested is None\\n\\n    # mismatched types\\n    m3 = NestedModel.construct(nested={1})\\n    assert cast(Any, m3.nested) == {1}\\n\\n    m4 = NestedModel.construct(nested=[False])\\n    assert cast(Any, m4.nested) == [False]\\n\\n',\n",
       "  'function_name': 'test_optional_list_nested_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_list_optional_items_nested_model() -> None:\\n    class NestedModel(BaseModel):\\n        nested: List[Optional[BasicModel]]\\n\\n    m = NestedModel.construct(nested=[None, {\"foo\": \"bar\"}])\\n    assert m.nested is not None\\n    assert isinstance(m.nested, list)\\n    assert len(m.nested) == 2\\n    assert m.nested[0] is None\\n    assert m.nested[1] is not None\\n    assert m.nested[1].foo == \"bar\"\\n\\n    # mismatched types\\n    m3 = NestedModel.construct(nested=\"foo\")\\n    assert cast(Any, m3.nested) == \"foo\"\\n\\n    m4 = NestedModel.construct(nested=[False])\\n    assert cast(Any, m4.nested) == [False]\\n\\n',\n",
       "  'function_name': 'test_list_optional_items_nested_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_list_mismatched_type() -> None:\\n    class NestedModel(BaseModel):\\n        nested: List[str]\\n\\n    m = NestedModel.construct(nested=False)\\n    assert cast(Any, m.nested) is False\\n\\n',\n",
       "  'function_name': 'test_list_mismatched_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_raw_dictionary() -> None:\\n    class NestedModel(BaseModel):\\n        nested: Dict[str, str]\\n\\n    m = NestedModel.construct(nested={\"hello\": \"world\"})\\n    assert m.nested == {\"hello\": \"world\"}\\n\\n    # mismatched types\\n    m = NestedModel.construct(nested=False)\\n    assert cast(Any, m.nested) is False\\n\\n',\n",
       "  'function_name': 'test_raw_dictionary',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_nested_dictionary_model() -> None:\\n    class NestedModel(BaseModel):\\n        nested: Dict[str, BasicModel]\\n\\n    m = NestedModel.construct(nested={\"hello\": {\"foo\": \"bar\"}})\\n    assert isinstance(m.nested, dict)\\n    assert m.nested[\"hello\"].foo == \"bar\"\\n\\n    # mismatched types\\n    m = NestedModel.construct(nested={\"hello\": False})\\n    assert cast(Any, m.nested[\"hello\"]) is False\\n\\n',\n",
       "  'function_name': 'test_nested_dictionary_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_unknown_fields() -> None:\\n    m1 = BasicModel.construct(foo=\"foo\", unknown=1)\\n    assert m1.foo == \"foo\"\\n    assert cast(Any, m1).unknown == 1\\n\\n    m2 = BasicModel.construct(foo=\"foo\", unknown={\"foo_bar\": True})\\n    assert m2.foo == \"foo\"\\n    assert cast(Any, m2).unknown == {\"foo_bar\": True}\\n\\n    assert model_dump(m2) == {\"foo\": \"foo\", \"unknown\": {\"foo_bar\": True}}\\n\\n',\n",
       "  'function_name': 'test_unknown_fields',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_strict_validation_unknown_fields() -> None:\\n    class Model(BaseModel):\\n        foo: str\\n\\n    model = parse_obj(Model, dict(foo=\"hello!\", user=\"Robert\"))\\n    assert model.foo == \"hello!\"\\n    assert cast(Any, model).user == \"Robert\"\\n\\n    assert model_dump(model) == {\"foo\": \"hello!\", \"user\": \"Robert\"}\\n\\n',\n",
       "  'function_name': 'test_strict_validation_unknown_fields',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_aliases() -> None:\\n    class Model(BaseModel):\\n        my_field: int = Field(alias=\"myField\")\\n\\n    m = Model.construct(myField=1)\\n    assert m.my_field == 1\\n\\n    # mismatched types\\n    m = Model.construct(myField={\"hello\": False})\\n    assert cast(Any, m.my_field) == {\"hello\": False}\\n\\n',\n",
       "  'function_name': 'test_aliases',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_repr() -> None:\\n    model = BasicModel(foo=\"bar\")\\n    assert str(model) == \"BasicModel(foo=\\'bar\\')\"\\n    assert repr(model) == \"BasicModel(foo=\\'bar\\')\"\\n\\n',\n",
       "  'function_name': 'test_repr',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_repr_nested_model() -> None:\\n    class Child(BaseModel):\\n        name: str\\n        age: int\\n\\n    class Parent(BaseModel):\\n        name: str\\n        child: Child\\n\\n    model = Parent(name=\"Robert\", child=Child(name=\"Foo\", age=5))\\n    assert str(model) == \"Parent(name=\\'Robert\\', child=Child(name=\\'Foo\\', age=5))\"\\n    assert repr(model) == \"Parent(name=\\'Robert\\', child=Child(name=\\'Foo\\', age=5))\"\\n\\n',\n",
       "  'function_name': 'test_repr_nested_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_optional_list() -> None:\\n    class Submodel(BaseModel):\\n        name: str\\n\\n    class Model(BaseModel):\\n        items: Optional[List[Submodel]]\\n\\n    m = Model.construct(items=None)\\n    assert m.items is None\\n\\n    m = Model.construct(items=[])\\n    assert m.items == []\\n\\n    m = Model.construct(items=[{\"name\": \"Robert\"}])\\n    assert m.items is not None\\n    assert len(m.items) == 1\\n    assert m.items[0].name == \"Robert\"\\n\\n',\n",
       "  'function_name': 'test_optional_list',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_nested_union_of_models() -> None:\\n    class Submodel1(BaseModel):\\n        bar: bool\\n\\n    class Submodel2(BaseModel):\\n        thing: str\\n\\n    class Model(BaseModel):\\n        foo: Union[Submodel1, Submodel2]\\n\\n    m = Model.construct(foo={\"thing\": \"hello\"})\\n    assert isinstance(m.foo, Submodel2)\\n    assert m.foo.thing == \"hello\"\\n\\n',\n",
       "  'function_name': 'test_nested_union_of_models',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_nested_union_of_mixed_types() -> None:\\n    class Submodel1(BaseModel):\\n        bar: bool\\n\\n    class Model(BaseModel):\\n        foo: Union[Submodel1, Literal[True], Literal[\"CARD_HOLDER\"]]\\n\\n    m = Model.construct(foo=True)\\n    assert m.foo is True\\n\\n    m = Model.construct(foo=\"CARD_HOLDER\")\\n    assert m.foo is \"CARD_HOLDER\"\\n\\n    m = Model.construct(foo={\"bar\": False})\\n    assert isinstance(m.foo, Submodel1)\\n    assert m.foo.bar is False\\n\\n',\n",
       "  'function_name': 'test_nested_union_of_mixed_types',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_nested_union_multiple_variants() -> None:\\n    class Submodel1(BaseModel):\\n        bar: bool\\n\\n    class Submodel2(BaseModel):\\n        thing: str\\n\\n    class Submodel3(BaseModel):\\n        foo: int\\n\\n    class Model(BaseModel):\\n        foo: Union[Submodel1, Submodel2, None, Submodel3]\\n\\n    m = Model.construct(foo={\"thing\": \"hello\"})\\n    assert isinstance(m.foo, Submodel2)\\n    assert m.foo.thing == \"hello\"\\n\\n    m = Model.construct(foo=None)\\n    assert m.foo is None\\n\\n    m = Model.construct()\\n    assert m.foo is None\\n\\n    m = Model.construct(foo={\"foo\": \"1\"})\\n    assert isinstance(m.foo, Submodel3)\\n    assert m.foo.foo == 1\\n\\n',\n",
       "  'function_name': 'test_nested_union_multiple_variants',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_nested_union_invalid_data() -> None:\\n    class Submodel1(BaseModel):\\n        level: int\\n\\n    class Submodel2(BaseModel):\\n        name: str\\n\\n    class Model(BaseModel):\\n        foo: Union[Submodel1, Submodel2]\\n\\n    m = Model.construct(foo=True)\\n    assert cast(bool, m.foo) is True\\n\\n    m = Model.construct(foo={\"name\": 3})\\n    if PYDANTIC_V2:\\n        assert isinstance(m.foo, Submodel1)\\n        assert m.foo.name == 3  # type: ignore\\n    else:\\n        assert isinstance(m.foo, Submodel2)\\n        assert m.foo.name == \"3\"\\n\\n',\n",
       "  'function_name': 'test_nested_union_invalid_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_list_of_unions() -> None:\\n    class Submodel1(BaseModel):\\n        level: int\\n\\n    class Submodel2(BaseModel):\\n        name: str\\n\\n    class Model(BaseModel):\\n        items: List[Union[Submodel1, Submodel2]]\\n\\n    m = Model.construct(items=[{\"level\": 1}, {\"name\": \"Robert\"}])\\n    assert len(m.items) == 2\\n    assert isinstance(m.items[0], Submodel1)\\n    assert m.items[0].level == 1\\n    assert isinstance(m.items[1], Submodel2)\\n    assert m.items[1].name == \"Robert\"\\n\\n    m = Model.construct(items=[{\"level\": -1}, 156])\\n    assert len(m.items) == 2\\n    assert isinstance(m.items[0], Submodel1)\\n    assert m.items[0].level == -1\\n    assert m.items[1] == 156\\n\\n',\n",
       "  'function_name': 'test_list_of_unions',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_union_of_lists() -> None:\\n    class SubModel1(BaseModel):\\n        level: int\\n\\n    class SubModel2(BaseModel):\\n        name: str\\n\\n    class Model(BaseModel):\\n        items: Union[List[SubModel1], List[SubModel2]]\\n\\n    # with one valid entry\\n    m = Model.construct(items=[{\"name\": \"Robert\"}])\\n    assert len(m.items) == 1\\n    assert isinstance(m.items[0], SubModel2)\\n    assert m.items[0].name == \"Robert\"\\n\\n    # with two entries pointing to different types\\n    m = Model.construct(items=[{\"level\": 1}, {\"name\": \"Robert\"}])\\n    assert len(m.items) == 2\\n    assert isinstance(m.items[0], SubModel1)\\n    assert m.items[0].level == 1\\n    assert isinstance(m.items[1], SubModel1)\\n    assert cast(Any, m.items[1]).name == \"Robert\"\\n\\n    # with two entries pointing to *completely* different types\\n    m = Model.construct(items=[{\"level\": -1}, 156])\\n    assert len(m.items) == 2\\n    assert isinstance(m.items[0], SubModel1)\\n    assert m.items[0].level == -1\\n    assert m.items[1] == 156\\n\\n',\n",
       "  'function_name': 'test_union_of_lists',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_dict_of_union() -> None:\\n    class SubModel1(BaseModel):\\n        name: str\\n\\n    class SubModel2(BaseModel):\\n        foo: str\\n\\n    class Model(BaseModel):\\n        data: Dict[str, Union[SubModel1, SubModel2]]\\n\\n    m = Model.construct(data={\"hello\": {\"name\": \"there\"}, \"foo\": {\"foo\": \"bar\"}})\\n    assert len(list(m.data.keys())) == 2\\n    assert isinstance(m.data[\"hello\"], SubModel1)\\n    assert m.data[\"hello\"].name == \"there\"\\n    assert isinstance(m.data[\"foo\"], SubModel2)\\n    assert m.data[\"foo\"].foo == \"bar\"\\n\\n    # TODO: test mismatched type\\n\\n',\n",
       "  'function_name': 'test_dict_of_union',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_double_nested_union() -> None:\\n    class SubModel1(BaseModel):\\n        name: str\\n\\n    class SubModel2(BaseModel):\\n        bar: str\\n\\n    class Model(BaseModel):\\n        data: Dict[str, List[Union[SubModel1, SubModel2]]]\\n\\n    m = Model.construct(data={\"foo\": [{\"bar\": \"baz\"}, {\"name\": \"Robert\"}]})\\n    assert len(m.data[\"foo\"]) == 2\\n\\n    entry1 = m.data[\"foo\"][0]\\n    assert isinstance(entry1, SubModel2)\\n    assert entry1.bar == \"baz\"\\n\\n    entry2 = m.data[\"foo\"][1]\\n    assert isinstance(entry2, SubModel1)\\n    assert entry2.name == \"Robert\"\\n\\n    # TODO: test mismatched type\\n\\n',\n",
       "  'function_name': 'test_double_nested_union',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_union_of_dict() -> None:\\n    class SubModel1(BaseModel):\\n        name: str\\n\\n    class SubModel2(BaseModel):\\n        foo: str\\n\\n    class Model(BaseModel):\\n        data: Union[Dict[str, SubModel1], Dict[str, SubModel2]]\\n\\n    m = Model.construct(data={\"hello\": {\"name\": \"there\"}, \"foo\": {\"foo\": \"bar\"}})\\n    assert len(list(m.data.keys())) == 2\\n    assert isinstance(m.data[\"hello\"], SubModel1)\\n    assert m.data[\"hello\"].name == \"there\"\\n    assert isinstance(m.data[\"foo\"], SubModel1)\\n    assert cast(Any, m.data[\"foo\"]).foo == \"bar\"\\n\\n',\n",
       "  'function_name': 'test_union_of_dict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_iso8601_datetime() -> None:\\n    class Model(BaseModel):\\n        created_at: datetime\\n\\n    expected = datetime(2019, 12, 27, 18, 11, 19, 117000, tzinfo=timezone.utc)\\n\\n    if PYDANTIC_V2:\\n        expected_json = \\'{\"created_at\":\"2019-12-27T18:11:19.117000Z\"}\\'\\n    else:\\n        expected_json = \\'{\"created_at\": \"2019-12-27T18:11:19.117000+00:00\"}\\'\\n\\n    model = Model.construct(created_at=\"2019-12-27T18:11:19.117Z\")\\n    assert model.created_at == expected\\n    assert model_json(model) == expected_json\\n\\n    model = parse_obj(Model, dict(created_at=\"2019-12-27T18:11:19.117Z\"))\\n    assert model.created_at == expected\\n    assert model_json(model) == expected_json\\n\\n',\n",
       "  'function_name': 'test_iso8601_datetime',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_does_not_coerce_int() -> None:\\n    class Model(BaseModel):\\n        bar: int\\n\\n    assert Model.construct(bar=1).bar == 1\\n    assert Model.construct(bar=10.9).bar == 10.9\\n    assert Model.construct(bar=\"19\").bar == \"19\"  # type: ignore[comparison-overlap]\\n    assert Model.construct(bar=False).bar is False\\n\\n',\n",
       "  'function_name': 'test_does_not_coerce_int',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_int_to_float_safe_conversion() -> None:\\n    class Model(BaseModel):\\n        float_field: float\\n\\n    m = Model.construct(float_field=10)\\n    assert m.float_field == 10.0\\n    assert isinstance(m.float_field, float)\\n\\n    m = Model.construct(float_field=10.12)\\n    assert m.float_field == 10.12\\n    assert isinstance(m.float_field, float)\\n\\n    # number too big\\n    m = Model.construct(float_field=2**53 + 1)\\n    assert m.float_field == 2**53 + 1\\n    assert isinstance(m.float_field, int)\\n\\n',\n",
       "  'function_name': 'test_int_to_float_safe_conversion',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_deprecated_alias() -> None:\\n    class Model(BaseModel):\\n        resource_id: str = Field(alias=\"model_id\")\\n\\n        @property\\n        def model_id(self) -> str:\\n            return self.resource_id\\n\\n    m = Model.construct(model_id=\"id\")\\n    assert m.model_id == \"id\"\\n    assert m.resource_id == \"id\"\\n    assert m.resource_id is m.model_id\\n\\n    m = parse_obj(Model, {\"model_id\": \"id\"})\\n    assert m.model_id == \"id\"\\n    assert m.resource_id == \"id\"\\n    assert m.resource_id is m.model_id\\n\\n',\n",
       "  'function_name': 'test_deprecated_alias',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_omitted_fields() -> None:\\n    class Model(BaseModel):\\n        resource_id: Optional[str] = None\\n\\n    m = Model.construct()\\n    assert \"resource_id\" not in m.model_fields_set\\n\\n    m = Model.construct(resource_id=None)\\n    assert \"resource_id\" in m.model_fields_set\\n\\n    m = Model.construct(resource_id=\"foo\")\\n    assert \"resource_id\" in m.model_fields_set\\n\\n',\n",
       "  'function_name': 'test_omitted_fields',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_to_dict() -> None:\\n    class Model(BaseModel):\\n        foo: Optional[str] = Field(alias=\"FOO\", default=None)\\n\\n    m = Model(FOO=\"hello\")\\n    assert m.to_dict() == {\"FOO\": \"hello\"}\\n    assert m.to_dict(use_api_names=False) == {\"foo\": \"hello\"}\\n\\n    m2 = Model()\\n    assert m2.to_dict() == {}\\n    assert m2.to_dict(exclude_unset=False) == {\"FOO\": None}\\n    assert m2.to_dict(exclude_unset=False, exclude_none=True) == {}\\n    assert m2.to_dict(exclude_unset=False, exclude_defaults=True) == {}\\n\\n    m3 = Model(FOO=None)\\n    assert m3.to_dict() == {\"FOO\": None}\\n    assert m3.to_dict(exclude_none=True) == {}\\n    assert m3.to_dict(exclude_defaults=True) == {}\\n\\n    if PYDANTIC_V2:\\n\\n        class Model2(BaseModel):\\n            created_at: datetime\\n\\n        time_str = \"2024-03-21T11:39:01.275859\"\\n        m4 = Model2.construct(created_at=time_str)\\n        assert m4.to_dict(mode=\"python\") == {\"created_at\": datetime.fromisoformat(time_str)}\\n        assert m4.to_dict(mode=\"json\") == {\"created_at\": time_str}\\n    else:\\n        with pytest.raises(ValueError, match=\"mode is only supported in Pydantic v2\"):\\n            m.to_dict(mode=\"json\")\\n\\n        with pytest.raises(ValueError, match=\"warnings is only supported in Pydantic v2\"):\\n            m.to_dict(warnings=False)\\n\\n',\n",
       "  'function_name': 'test_to_dict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_forwards_compat_model_dump_method() -> None:\\n    class Model(BaseModel):\\n        foo: Optional[str] = Field(alias=\"FOO\", default=None)\\n\\n    m = Model(FOO=\"hello\")\\n    assert m.model_dump() == {\"foo\": \"hello\"}\\n    assert m.model_dump(include={\"bar\"}) == {}\\n    assert m.model_dump(exclude={\"foo\"}) == {}\\n    assert m.model_dump(by_alias=True) == {\"FOO\": \"hello\"}\\n\\n    m2 = Model()\\n    assert m2.model_dump() == {\"foo\": None}\\n    assert m2.model_dump(exclude_unset=True) == {}\\n    assert m2.model_dump(exclude_none=True) == {}\\n    assert m2.model_dump(exclude_defaults=True) == {}\\n\\n    m3 = Model(FOO=None)\\n    assert m3.model_dump() == {\"foo\": None}\\n    assert m3.model_dump(exclude_none=True) == {}\\n\\n    if not PYDANTIC_V2:\\n        with pytest.raises(ValueError, match=\"mode is only supported in Pydantic v2\"):\\n            m.model_dump(mode=\"json\")\\n\\n        with pytest.raises(ValueError, match=\"round_trip is only supported in Pydantic v2\"):\\n            m.model_dump(round_trip=True)\\n\\n        with pytest.raises(ValueError, match=\"warnings is only supported in Pydantic v2\"):\\n            m.model_dump(warnings=False)\\n\\n',\n",
       "  'function_name': 'test_forwards_compat_model_dump_method',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_to_json() -> None:\\n    class Model(BaseModel):\\n        foo: Optional[str] = Field(alias=\"FOO\", default=None)\\n\\n    m = Model(FOO=\"hello\")\\n    assert json.loads(m.to_json()) == {\"FOO\": \"hello\"}\\n    assert json.loads(m.to_json(use_api_names=False)) == {\"foo\": \"hello\"}\\n\\n    if PYDANTIC_V2:\\n        assert m.to_json(indent=None) == \\'{\"FOO\":\"hello\"}\\'\\n    else:\\n        assert m.to_json(indent=None) == \\'{\"FOO\": \"hello\"}\\'\\n\\n    m2 = Model()\\n    assert json.loads(m2.to_json()) == {}\\n    assert json.loads(m2.to_json(exclude_unset=False)) == {\"FOO\": None}\\n    assert json.loads(m2.to_json(exclude_unset=False, exclude_none=True)) == {}\\n    assert json.loads(m2.to_json(exclude_unset=False, exclude_defaults=True)) == {}\\n\\n    m3 = Model(FOO=None)\\n    assert json.loads(m3.to_json()) == {\"FOO\": None}\\n    assert json.loads(m3.to_json(exclude_none=True)) == {}\\n\\n    if not PYDANTIC_V2:\\n        with pytest.raises(ValueError, match=\"warnings is only supported in Pydantic v2\"):\\n            m.to_json(warnings=False)\\n\\n',\n",
       "  'function_name': 'test_to_json',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_forwards_compat_model_dump_json_method() -> None:\\n    class Model(BaseModel):\\n        foo: Optional[str] = Field(alias=\"FOO\", default=None)\\n\\n    m = Model(FOO=\"hello\")\\n    assert json.loads(m.model_dump_json()) == {\"foo\": \"hello\"}\\n    assert json.loads(m.model_dump_json(include={\"bar\"})) == {}\\n    assert json.loads(m.model_dump_json(include={\"foo\"})) == {\"foo\": \"hello\"}\\n    assert json.loads(m.model_dump_json(by_alias=True)) == {\"FOO\": \"hello\"}\\n\\n    assert m.model_dump_json(indent=2) == \\'{\\\\n  \"foo\": \"hello\"\\\\n}\\'\\n\\n    m2 = Model()\\n    assert json.loads(m2.model_dump_json()) == {\"foo\": None}\\n    assert json.loads(m2.model_dump_json(exclude_unset=True)) == {}\\n    assert json.loads(m2.model_dump_json(exclude_none=True)) == {}\\n    assert json.loads(m2.model_dump_json(exclude_defaults=True)) == {}\\n\\n    m3 = Model(FOO=None)\\n    assert json.loads(m3.model_dump_json()) == {\"foo\": None}\\n    assert json.loads(m3.model_dump_json(exclude_none=True)) == {}\\n\\n    if not PYDANTIC_V2:\\n        with pytest.raises(ValueError, match=\"round_trip is only supported in Pydantic v2\"):\\n            m.model_dump_json(round_trip=True)\\n\\n        with pytest.raises(ValueError, match=\"warnings is only supported in Pydantic v2\"):\\n            m.model_dump_json(warnings=False)\\n\\n',\n",
       "  'function_name': 'test_forwards_compat_model_dump_json_method',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': \"def test_type_compat() -> None:\\n    # our model type can be assigned to Pydantic's model type\\n\\n    def takes_pydantic(model: pydantic.BaseModel) -> None:  # noqa: ARG001\\n        ...\\n\\n    class OurModel(BaseModel):\\n        foo: Optional[str] = None\\n\\n    takes_pydantic(OurModel())\\n\\n\",\n",
       "  'function_name': 'test_type_compat',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_annotated_types() -> None:\\n    class Model(BaseModel):\\n        value: str\\n\\n    m = construct_type(\\n        value={\"value\": \"foo\"},\\n        type_=cast(Any, Annotated[Model, \"random metadata\"]),\\n    )\\n    assert isinstance(m, Model)\\n    assert m.value == \"foo\"\\n\\n',\n",
       "  'function_name': 'test_annotated_types',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_discriminated_unions_invalid_data() -> None:\\n    class A(BaseModel):\\n        type: Literal[\"a\"]\\n\\n        data: str\\n\\n    class B(BaseModel):\\n        type: Literal[\"b\"]\\n\\n        data: int\\n\\n    m = construct_type(\\n        value={\"type\": \"b\", \"data\": \"foo\"},\\n        type_=cast(Any, Annotated[Union[A, B], PropertyInfo(discriminator=\"type\")]),\\n    )\\n    assert isinstance(m, B)\\n    assert m.type == \"b\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n    m = construct_type(\\n        value={\"type\": \"a\", \"data\": 100},\\n        type_=cast(Any, Annotated[Union[A, B], PropertyInfo(discriminator=\"type\")]),\\n    )\\n    assert isinstance(m, A)\\n    assert m.type == \"a\"\\n    if PYDANTIC_V2:\\n        assert m.data == 100  # type: ignore[comparison-overlap]\\n    else:\\n        # pydantic v1 automatically converts inputs to strings\\n        # if the expected type is a str\\n        assert m.data == \"100\"\\n\\n',\n",
       "  'function_name': 'test_discriminated_unions_invalid_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_discriminated_unions_unknown_variant() -> None:\\n    class A(BaseModel):\\n        type: Literal[\"a\"]\\n\\n        data: str\\n\\n    class B(BaseModel):\\n        type: Literal[\"b\"]\\n\\n        data: int\\n\\n    m = construct_type(\\n        value={\"type\": \"c\", \"data\": None, \"new_thing\": \"bar\"},\\n        type_=cast(Any, Annotated[Union[A, B], PropertyInfo(discriminator=\"type\")]),\\n    )\\n\\n    # just chooses the first variant\\n    assert isinstance(m, A)\\n    assert m.type == \"c\"  # type: ignore[comparison-overlap]\\n    assert m.data == None  # type: ignore[unreachable]\\n    assert m.new_thing == \"bar\"\\n\\n',\n",
       "  'function_name': 'test_discriminated_unions_unknown_variant',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_discriminated_unions_invalid_data_nested_unions() -> None:\\n    class A(BaseModel):\\n        type: Literal[\"a\"]\\n\\n        data: str\\n\\n    class B(BaseModel):\\n        type: Literal[\"b\"]\\n\\n        data: int\\n\\n    class C(BaseModel):\\n        type: Literal[\"c\"]\\n\\n        data: bool\\n\\n    m = construct_type(\\n        value={\"type\": \"b\", \"data\": \"foo\"},\\n        type_=cast(Any, Annotated[Union[Union[A, B], C], PropertyInfo(discriminator=\"type\")]),\\n    )\\n    assert isinstance(m, B)\\n    assert m.type == \"b\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n    m = construct_type(\\n        value={\"type\": \"c\", \"data\": \"foo\"},\\n        type_=cast(Any, Annotated[Union[Union[A, B], C], PropertyInfo(discriminator=\"type\")]),\\n    )\\n    assert isinstance(m, C)\\n    assert m.type == \"c\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n',\n",
       "  'function_name': 'test_discriminated_unions_invalid_data_nested_unions',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_discriminated_unions_with_aliases_invalid_data() -> None:\\n    class A(BaseModel):\\n        foo_type: Literal[\"a\"] = Field(alias=\"type\")\\n\\n        data: str\\n\\n    class B(BaseModel):\\n        foo_type: Literal[\"b\"] = Field(alias=\"type\")\\n\\n        data: int\\n\\n    m = construct_type(\\n        value={\"type\": \"b\", \"data\": \"foo\"},\\n        type_=cast(Any, Annotated[Union[A, B], PropertyInfo(discriminator=\"foo_type\")]),\\n    )\\n    assert isinstance(m, B)\\n    assert m.foo_type == \"b\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n    m = construct_type(\\n        value={\"type\": \"a\", \"data\": 100},\\n        type_=cast(Any, Annotated[Union[A, B], PropertyInfo(discriminator=\"foo_type\")]),\\n    )\\n    assert isinstance(m, A)\\n    assert m.foo_type == \"a\"\\n    if PYDANTIC_V2:\\n        assert m.data == 100  # type: ignore[comparison-overlap]\\n    else:\\n        # pydantic v1 automatically converts inputs to strings\\n        # if the expected type is a str\\n        assert m.data == \"100\"\\n\\n',\n",
       "  'function_name': 'test_discriminated_unions_with_aliases_invalid_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_discriminated_unions_overlapping_discriminators_invalid_data() -> None:\\n    class A(BaseModel):\\n        type: Literal[\"a\"]\\n\\n        data: bool\\n\\n    class B(BaseModel):\\n        type: Literal[\"a\"]\\n\\n        data: int\\n\\n    m = construct_type(\\n        value={\"type\": \"a\", \"data\": \"foo\"},\\n        type_=cast(Any, Annotated[Union[A, B], PropertyInfo(discriminator=\"type\")]),\\n    )\\n    assert isinstance(m, B)\\n    assert m.type == \"a\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n',\n",
       "  'function_name': 'test_discriminated_unions_overlapping_discriminators_invalid_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def test_discriminated_unions_invalid_data_uses_cache() -> None:\\n    class A(BaseModel):\\n        type: Literal[\"a\"]\\n\\n        data: str\\n\\n    class B(BaseModel):\\n        type: Literal[\"b\"]\\n\\n        data: int\\n\\n    UnionType = cast(Any, Union[A, B])\\n\\n    assert not hasattr(UnionType, \"__discriminator__\")\\n\\n    m = construct_type(\\n        value={\"type\": \"b\", \"data\": \"foo\"}, type_=cast(Any, Annotated[UnionType, PropertyInfo(discriminator=\"type\")])\\n    )\\n    assert isinstance(m, B)\\n    assert m.type == \"b\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n    discriminator = UnionType.__discriminator__\\n    assert discriminator is not None\\n\\n    m = construct_type(\\n        value={\"type\": \"b\", \"data\": \"foo\"}, type_=cast(Any, Annotated[UnionType, PropertyInfo(discriminator=\"type\")])\\n    )\\n    assert isinstance(m, B)\\n    assert m.type == \"b\"\\n    assert m.data == \"foo\"  # type: ignore[comparison-overlap]\\n\\n    # if the discriminator details object stays the same between invocations then\\n    # we hit the cache\\n    assert UnionType.__discriminator__ is discriminator\\n',\n",
       "  'function_name': 'test_discriminated_unions_invalid_data_uses_cache',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_models.py'},\n",
       " {'code': 'def assert_matches_model(model: type[BaseModelT], value: BaseModelT, *, path: list[str]) -> bool:\\n    for name, field in get_model_fields(model).items():\\n        field_value = getattr(value, name)\\n        if PYDANTIC_V2:\\n            allow_none = False\\n        else:\\n            # in v1 nullability was structured differently\\n            # https://docs.pydantic.dev/2.0/migration/#required-optional-and-nullable-fields\\n            allow_none = getattr(field, \"allow_none\", False)\\n\\n        assert_matches_type(\\n            field_outer_type(field),\\n            field_value,\\n            path=[*path, name],\\n            allow_none=allow_none,\\n        )\\n\\n    return True\\n\\n',\n",
       "  'function_name': 'assert_matches_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/utils.py'},\n",
       " {'code': 'def assert_matches_type(\\n    type_: Any,\\n    value: object,\\n    *,\\n    path: list[str],\\n    allow_none: bool = False,\\n) -> None:\\n    # unwrap `Annotated[T, ...]` -> `T`\\n    if is_annotated_type(type_):\\n        type_ = extract_type_arg(type_, 0)\\n\\n    if allow_none and value is None:\\n        return\\n\\n    if type_ is None or type_ is NoneType:\\n        assert value is None\\n        return\\n\\n    origin = get_origin(type_) or type_\\n\\n    if is_list_type(type_):\\n        return _assert_list_type(type_, value)\\n\\n    if origin == str:\\n        assert isinstance(value, str)\\n    elif origin == int:\\n        assert isinstance(value, int)\\n    elif origin == bool:\\n        assert isinstance(value, bool)\\n    elif origin == float:\\n        assert isinstance(value, float)\\n    elif origin == bytes:\\n        assert isinstance(value, bytes)\\n    elif origin == datetime:\\n        assert isinstance(value, datetime)\\n    elif origin == date:\\n        assert isinstance(value, date)\\n    elif origin == object:\\n        # nothing to do here, the expected type is unknown\\n        pass\\n    elif origin == Literal:\\n        assert value in get_args(type_)\\n    elif origin == dict:\\n        assert is_dict(value)\\n\\n        args = get_args(type_)\\n        key_type = args[0]\\n        items_type = args[1]\\n\\n        for key, item in value.items():\\n            assert_matches_type(key_type, key, path=[*path, \"<dict key>\"])\\n            assert_matches_type(items_type, item, path=[*path, \"<dict item>\"])\\n    elif is_union_type(type_):\\n        variants = get_args(type_)\\n\\n        try:\\n            none_index = variants.index(type(None))\\n        except ValueError:\\n            pass\\n        else:\\n            # special case Optional[T] for better error messages\\n            if len(variants) == 2:\\n                if value is None:\\n                    # valid\\n                    return\\n\\n                return assert_matches_type(type_=variants[not none_index], value=value, path=path)\\n\\n        for i, variant in enumerate(variants):\\n            try:\\n                assert_matches_type(variant, value, path=[*path, f\"variant {i}\"])\\n                return\\n            except AssertionError:\\n                traceback.print_exc()\\n                continue\\n\\n        raise AssertionError(\"Did not match any variants\")\\n    elif issubclass(origin, BaseModel):\\n        assert isinstance(value, type_)\\n        assert assert_matches_model(type_, cast(Any, value), path=path)\\n    elif inspect.isclass(origin) and origin.__name__ == \"HttpxBinaryResponseContent\":\\n        assert value.__class__.__name__ == \"HttpxBinaryResponseContent\"\\n    else:\\n        assert None, f\"Unhandled field type: {type_}\"\\n\\n',\n",
       "  'function_name': 'assert_matches_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/utils.py'},\n",
       " {'code': 'def _assert_list_type(type_: type[object], value: object) -> None:\\n    assert is_list(value)\\n\\n    inner_type = get_args(type_)[0]\\n    for entry in value:\\n        assert_type(inner_type, entry)  # type: ignore\\n\\n',\n",
       "  'function_name': '_assert_list_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/utils.py'},\n",
       " {'code': 'def update_env(**new_env: str) -> Iterator[None]:\\n    old = os.environ.copy()\\n\\n    try:\\n        os.environ.update(new_env)\\n\\n        yield None\\n    finally:\\n        os.environ.clear()\\n        os.environ.update(old)\\n',\n",
       "  'function_name': 'update_env',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/utils.py'},\n",
       " {'code': 'def event_loop() -> Iterator[asyncio.AbstractEventLoop]:\\n    loop = asyncio.new_event_loop()\\n    yield loop\\n    loop.close()\\n\\n',\n",
       "  'function_name': 'event_loop',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/conftest.py'},\n",
       " {'code': 'def client(request: FixtureRequest) -> Iterator[OpenAI]:\\n    strict = getattr(request, \"param\", True)\\n    if not isinstance(strict, bool):\\n        raise TypeError(f\"Unexpected fixture parameter type {type(strict)}, expected {bool}\")\\n\\n    with OpenAI(base_url=base_url, api_key=api_key, _strict_response_validation=strict) as client:\\n        yield client\\n\\n',\n",
       "  'function_name': 'client',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/conftest.py'},\n",
       " {'code': 'async def async_client(request: FixtureRequest) -> AsyncIterator[AsyncOpenAI]:\\n    strict = getattr(request, \"param\", True)\\n    if not isinstance(strict, bool):\\n        raise TypeError(f\"Unexpected fixture parameter type {type(strict)}, expected {bool}\")\\n\\n    async with AsyncOpenAI(base_url=base_url, api_key=api_key, _strict_response_validation=strict) as client:\\n        yield client\\n',\n",
       "  'function_name': 'async_client',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/conftest.py'},\n",
       " {'code': 'def test_extract_response_type_direct_classes() -> None:\\n    assert extract_response_type(BaseAPIResponse[str]) == str\\n    assert extract_response_type(APIResponse[str]) == str\\n    assert extract_response_type(AsyncAPIResponse[str]) == str\\n\\n',\n",
       "  'function_name': 'test_extract_response_type_direct_classes',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_extract_response_type_direct_class_missing_type_arg() -> None:\\n    with pytest.raises(\\n        RuntimeError,\\n        match=\"Expected type <class \\'openai._response.AsyncAPIResponse\\'> to have a type argument at index 0 but it did not\",\\n    ):\\n        extract_response_type(AsyncAPIResponse)\\n\\n',\n",
       "  'function_name': 'test_extract_response_type_direct_class_missing_type_arg',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_extract_response_type_concrete_subclasses() -> None:\\n    assert extract_response_type(ConcreteBaseAPIResponse) == bytes\\n    assert extract_response_type(ConcreteAPIResponse) == List[str]\\n    assert extract_response_type(ConcreteAsyncAPIResponse) == httpx.Response\\n\\n',\n",
       "  'function_name': 'test_extract_response_type_concrete_subclasses',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_extract_response_type_binary_response() -> None:\\n    assert extract_response_type(BinaryAPIResponse) == bytes\\n    assert extract_response_type(AsyncBinaryAPIResponse) == bytes\\n\\n',\n",
       "  'function_name': 'test_extract_response_type_binary_response',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_response_parse_mismatched_basemodel(client: OpenAI) -> None:\\n    response = APIResponse(\\n        raw=httpx.Response(200, content=b\"foo\"),\\n        client=client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    with pytest.raises(\\n        TypeError,\\n        match=\"Pydantic models must subclass our base model type, e.g. `from openai import BaseModel`\",\\n    ):\\n        response.parse(to=PydanticModel)\\n\\n',\n",
       "  'function_name': 'test_response_parse_mismatched_basemodel',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'async def test_async_response_parse_mismatched_basemodel(async_client: AsyncOpenAI) -> None:\\n    response = AsyncAPIResponse(\\n        raw=httpx.Response(200, content=b\"foo\"),\\n        client=async_client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    with pytest.raises(\\n        TypeError,\\n        match=\"Pydantic models must subclass our base model type, e.g. `from openai import BaseModel`\",\\n    ):\\n        await response.parse(to=PydanticModel)\\n\\n',\n",
       "  'function_name': 'test_async_response_parse_mismatched_basemodel',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_response_parse_custom_stream(client: OpenAI) -> None:\\n    response = APIResponse(\\n        raw=httpx.Response(200, content=b\"foo\"),\\n        client=client,\\n        stream=True,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    stream = response.parse(to=Stream[int])\\n    assert stream._cast_to == int\\n\\n',\n",
       "  'function_name': 'test_response_parse_custom_stream',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'async def test_async_response_parse_custom_stream(async_client: AsyncOpenAI) -> None:\\n    response = AsyncAPIResponse(\\n        raw=httpx.Response(200, content=b\"foo\"),\\n        client=async_client,\\n        stream=True,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    stream = await response.parse(to=Stream[int])\\n    assert stream._cast_to == int\\n\\n',\n",
       "  'function_name': 'test_async_response_parse_custom_stream',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_response_parse_custom_model(client: OpenAI) -> None:\\n    response = APIResponse(\\n        raw=httpx.Response(200, content=json.dumps({\"foo\": \"hello!\", \"bar\": 2})),\\n        client=client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    obj = response.parse(to=CustomModel)\\n    assert obj.foo == \"hello!\"\\n    assert obj.bar == 2\\n\\n',\n",
       "  'function_name': 'test_response_parse_custom_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'async def test_async_response_parse_custom_model(async_client: AsyncOpenAI) -> None:\\n    response = AsyncAPIResponse(\\n        raw=httpx.Response(200, content=json.dumps({\"foo\": \"hello!\", \"bar\": 2})),\\n        client=async_client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    obj = await response.parse(to=CustomModel)\\n    assert obj.foo == \"hello!\"\\n    assert obj.bar == 2\\n\\n',\n",
       "  'function_name': 'test_async_response_parse_custom_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def test_response_parse_annotated_type(client: OpenAI) -> None:\\n    response = APIResponse(\\n        raw=httpx.Response(200, content=json.dumps({\"foo\": \"hello!\", \"bar\": 2})),\\n        client=client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    obj = response.parse(\\n        to=cast(\"type[CustomModel]\", Annotated[CustomModel, \"random metadata\"]),\\n    )\\n    assert obj.foo == \"hello!\"\\n    assert obj.bar == 2\\n\\n',\n",
       "  'function_name': 'test_response_parse_annotated_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'async def test_async_response_parse_annotated_type(async_client: AsyncOpenAI) -> None:\\n    response = AsyncAPIResponse(\\n        raw=httpx.Response(200, content=json.dumps({\"foo\": \"hello!\", \"bar\": 2})),\\n        client=async_client,\\n        stream=False,\\n        stream_cls=None,\\n        cast_to=str,\\n        options=FinalRequestOptions.construct(method=\"get\", url=\"/foo\"),\\n    )\\n\\n    obj = await response.parse(\\n        to=cast(\"type[CustomModel]\", Annotated[CustomModel, \"random metadata\"]),\\n    )\\n    assert obj.foo == \"hello!\"\\n    assert obj.bar == 2\\n',\n",
       "  'function_name': 'test_async_response_parse_annotated_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_response.py'},\n",
       " {'code': 'def _get_params(client: BaseClient[Any, Any]) -> dict[str, str]:\\n    request = client._build_request(FinalRequestOptions(method=\"get\", url=\"/foo\"))\\n    url = httpx.URL(request.url)\\n    return dict(url.params)\\n\\n',\n",
       "  'function_name': '_get_params',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_client.py'},\n",
       " {'code': 'def _low_retry_timeout(*_args: Any, **_kwargs: Any) -> float:\\n    return 0.1\\n\\n',\n",
       "  'function_name': '_low_retry_timeout',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_client.py'},\n",
       " {'code': 'def _get_open_connections(client: OpenAI | AsyncOpenAI) -> int:\\n    transport = client._client._transport\\n    assert isinstance(transport, httpx.HTTPTransport) or isinstance(transport, httpx.AsyncHTTPTransport)\\n\\n    pool = transport._pool\\n    return len(pool._requests)\\n\\n',\n",
       "  'function_name': '_get_open_connections',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_client.py'},\n",
       " {'code': 'def reset_state() -> None:\\n    openai._reset_client()\\n    openai.api_key = None or \"My API Key\"\\n    openai.organization = None\\n    openai.project = None\\n    openai.base_url = None\\n    openai.timeout = DEFAULT_TIMEOUT\\n    openai.max_retries = DEFAULT_MAX_RETRIES\\n    openai.default_headers = None\\n    openai.default_query = None\\n    openai.http_client = None\\n    openai.api_type = _os.environ.get(\"OPENAI_API_TYPE\")  # type: ignore\\n    openai.api_version = None\\n    openai.azure_endpoint = None\\n    openai.azure_ad_token = None\\n    openai.azure_ad_token_provider = None\\n\\n',\n",
       "  'function_name': 'reset_state',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def reset_state_fixture() -> None:\\n    reset_state()\\n\\n',\n",
       "  'function_name': 'reset_state_fixture',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_base_url_option() -> None:\\n    assert openai.base_url is None\\n    assert openai.completions._client.base_url == URL(\"https://api.openai.com/v1/\")\\n\\n    openai.base_url = \"http://foo.com\"\\n\\n    assert openai.base_url == URL(\"http://foo.com\")\\n    assert openai.completions._client.base_url == URL(\"http://foo.com\")\\n\\n',\n",
       "  'function_name': 'test_base_url_option',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_timeout_option() -> None:\\n    assert openai.timeout == openai.DEFAULT_TIMEOUT\\n    assert openai.completions._client.timeout == openai.DEFAULT_TIMEOUT\\n\\n    openai.timeout = 3\\n\\n    assert openai.timeout == 3\\n    assert openai.completions._client.timeout == 3\\n\\n',\n",
       "  'function_name': 'test_timeout_option',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_max_retries_option() -> None:\\n    assert openai.max_retries == openai.DEFAULT_MAX_RETRIES\\n    assert openai.completions._client.max_retries == openai.DEFAULT_MAX_RETRIES\\n\\n    openai.max_retries = 1\\n\\n    assert openai.max_retries == 1\\n    assert openai.completions._client.max_retries == 1\\n\\n',\n",
       "  'function_name': 'test_max_retries_option',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_default_headers_option() -> None:\\n    assert openai.default_headers == None\\n\\n    openai.default_headers = {\"Foo\": \"Bar\"}\\n\\n    assert openai.default_headers[\"Foo\"] == \"Bar\"\\n    assert openai.completions._client.default_headers[\"Foo\"] == \"Bar\"\\n\\n',\n",
       "  'function_name': 'test_default_headers_option',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_default_query_option() -> None:\\n    assert openai.default_query is None\\n    assert openai.completions._client._custom_query == {}\\n\\n    openai.default_query = {\"Foo\": {\"nested\": 1}}\\n\\n    assert openai.default_query[\"Foo\"] == {\"nested\": 1}\\n    assert openai.completions._client._custom_query[\"Foo\"] == {\"nested\": 1}\\n\\n',\n",
       "  'function_name': 'test_default_query_option',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_http_client_option() -> None:\\n    assert openai.http_client is None\\n\\n    original_http_client = openai.completions._client._client\\n    assert original_http_client is not None\\n\\n    new_client = httpx.Client()\\n    openai.http_client = new_client\\n\\n    assert openai.completions._client._client is new_client\\n\\n',\n",
       "  'function_name': 'test_http_client_option',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def fresh_env() -> Iterator[None]:\\n    old = _os.environ.copy()\\n\\n    try:\\n        _os.environ.clear()\\n        yield\\n    finally:\\n        _os.environ.update(old)\\n\\n',\n",
       "  'function_name': 'fresh_env',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_only_api_key_results_in_openai_api() -> None:\\n    with fresh_env():\\n        openai.api_type = None\\n        openai.api_key = \"example API key\"\\n\\n        assert type(openai.completions._client).__name__ == \"_ModuleClient\"\\n\\n',\n",
       "  'function_name': 'test_only_api_key_results_in_openai_api',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_azure_api_key_env_without_api_version() -> None:\\n    with fresh_env():\\n        openai.api_type = None\\n        _os.environ[\"AZURE_OPENAI_API_KEY\"] = \"example API key\"\\n\\n        with pytest.raises(\\n            ValueError,\\n            match=r\"Must provide either the `api_version` argument or the `OPENAI_API_VERSION` environment variable\",\\n        ):\\n            openai.completions._client  # noqa: B018\\n\\n',\n",
       "  'function_name': 'test_azure_api_key_env_without_api_version',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_azure_api_key_and_version_env() -> None:\\n    with fresh_env():\\n        openai.api_type = None\\n        _os.environ[\"AZURE_OPENAI_API_KEY\"] = \"example API key\"\\n        _os.environ[\"OPENAI_API_VERSION\"] = \"example-version\"\\n\\n        with pytest.raises(\\n            ValueError,\\n            match=r\"Must provide one of the `base_url` or `azure_endpoint` arguments, or the `AZURE_OPENAI_ENDPOINT` environment variable\",\\n        ):\\n            openai.completions._client  # noqa: B018\\n\\n',\n",
       "  'function_name': 'test_azure_api_key_and_version_env',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_azure_api_key_version_and_endpoint_env() -> None:\\n    with fresh_env():\\n        openai.api_type = None\\n        _os.environ[\"AZURE_OPENAI_API_KEY\"] = \"example API key\"\\n        _os.environ[\"OPENAI_API_VERSION\"] = \"example-version\"\\n        _os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://www.example\"\\n\\n        openai.completions._client  # noqa: B018\\n\\n        assert openai.api_type == \"azure\"\\n\\n',\n",
       "  'function_name': 'test_azure_api_key_version_and_endpoint_env',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_azure_azure_ad_token_version_and_endpoint_env() -> None:\\n    with fresh_env():\\n        openai.api_type = None\\n        _os.environ[\"AZURE_OPENAI_AD_TOKEN\"] = \"example AD token\"\\n        _os.environ[\"OPENAI_API_VERSION\"] = \"example-version\"\\n        _os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://www.example\"\\n\\n        client = openai.completions._client\\n        assert isinstance(client, AzureOpenAI)\\n        assert client._azure_ad_token == \"example AD token\"\\n\\n',\n",
       "  'function_name': 'test_azure_azure_ad_token_version_and_endpoint_env',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_azure_azure_ad_token_provider_version_and_endpoint_env() -> None:\\n    with fresh_env():\\n        openai.api_type = None\\n        _os.environ[\"OPENAI_API_VERSION\"] = \"example-version\"\\n        _os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://www.example\"\\n        openai.azure_ad_token_provider = lambda: \"token\"\\n\\n        client = openai.completions._client\\n        assert isinstance(client, AzureOpenAI)\\n        assert client._azure_ad_token_provider is not None\\n        assert client._azure_ad_token_provider() == \"token\"\\n',\n",
       "  'function_name': 'test_azure_azure_ad_token_provider_version_and_endpoint_env',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_module_client.py'},\n",
       " {'code': 'def test_empty() -> None:\\n    assert stringify({}) == \"\"\\n    assert stringify({\"a\": {}}) == \"\"\\n    assert stringify({\"a\": {\"b\": {\"c\": {}}}}) == \"\"\\n\\n',\n",
       "  'function_name': 'test_empty',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_basic() -> None:\\n    assert stringify({\"a\": 1}) == \"a=1\"\\n    assert stringify({\"a\": \"b\"}) == \"a=b\"\\n    assert stringify({\"a\": True}) == \"a=true\"\\n    assert stringify({\"a\": False}) == \"a=false\"\\n    assert stringify({\"a\": 1.23456}) == \"a=1.23456\"\\n    assert stringify({\"a\": None}) == \"\"\\n\\n',\n",
       "  'function_name': 'test_basic',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_nested_dotted(method: str) -> None:\\n    if method == \"class\":\\n        serialise = Querystring(nested_format=\"dots\").stringify\\n    else:\\n        serialise = partial(stringify, nested_format=\"dots\")\\n\\n    assert unquote(serialise({\"a\": {\"b\": \"c\"}})) == \"a.b=c\"\\n    assert unquote(serialise({\"a\": {\"b\": \"c\", \"d\": \"e\", \"f\": \"g\"}})) == \"a.b=c&a.d=e&a.f=g\"\\n    assert unquote(serialise({\"a\": {\"b\": {\"c\": {\"d\": \"e\"}}}})) == \"a.b.c.d=e\"\\n    assert unquote(serialise({\"a\": {\"b\": True}})) == \"a.b=true\"\\n\\n',\n",
       "  'function_name': 'test_nested_dotted',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_nested_brackets() -> None:\\n    assert unquote(stringify({\"a\": {\"b\": \"c\"}})) == \"a[b]=c\"\\n    assert unquote(stringify({\"a\": {\"b\": \"c\", \"d\": \"e\", \"f\": \"g\"}})) == \"a[b]=c&a[d]=e&a[f]=g\"\\n    assert unquote(stringify({\"a\": {\"b\": {\"c\": {\"d\": \"e\"}}}})) == \"a[b][c][d]=e\"\\n    assert unquote(stringify({\"a\": {\"b\": True}})) == \"a[b]=true\"\\n\\n',\n",
       "  'function_name': 'test_nested_brackets',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_array_comma(method: str) -> None:\\n    if method == \"class\":\\n        serialise = Querystring(array_format=\"comma\").stringify\\n    else:\\n        serialise = partial(stringify, array_format=\"comma\")\\n\\n    assert unquote(serialise({\"in\": [\"foo\", \"bar\"]})) == \"in=foo,bar\"\\n    assert unquote(serialise({\"a\": {\"b\": [True, False]}})) == \"a[b]=true,false\"\\n    assert unquote(serialise({\"a\": {\"b\": [True, False, None, True]}})) == \"a[b]=true,false,true\"\\n\\n',\n",
       "  'function_name': 'test_array_comma',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_array_repeat() -> None:\\n    assert unquote(stringify({\"in\": [\"foo\", \"bar\"]})) == \"in=foo&in=bar\"\\n    assert unquote(stringify({\"a\": {\"b\": [True, False]}})) == \"a[b]=true&a[b]=false\"\\n    assert unquote(stringify({\"a\": {\"b\": [True, False, None, True]}})) == \"a[b]=true&a[b]=false&a[b]=true\"\\n    assert unquote(stringify({\"in\": [\"foo\", {\"b\": {\"c\": [\"d\", \"e\"]}}]})) == \"in=foo&in[b][c]=d&in[b][c]=e\"\\n\\n',\n",
       "  'function_name': 'test_array_repeat',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_array_brackets(method: str) -> None:\\n    if method == \"class\":\\n        serialise = Querystring(array_format=\"brackets\").stringify\\n    else:\\n        serialise = partial(stringify, array_format=\"brackets\")\\n\\n    assert unquote(serialise({\"in\": [\"foo\", \"bar\"]})) == \"in[]=foo&in[]=bar\"\\n    assert unquote(serialise({\"a\": {\"b\": [True, False]}})) == \"a[b][]=true&a[b][]=false\"\\n    assert unquote(serialise({\"a\": {\"b\": [True, False, None, True]}})) == \"a[b][]=true&a[b][]=false&a[b][]=true\"\\n\\n',\n",
       "  'function_name': 'test_array_brackets',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_unknown_array_format() -> None:\\n    with pytest.raises(NotImplementedError, match=\"Unknown array_format value: foo, choose from comma, repeat\"):\\n        stringify({\"a\": [\"foo\", \"bar\"]}, array_format=cast(Any, \"foo\"))\\n',\n",
       "  'function_name': 'test_unknown_array_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_qs.py'},\n",
       " {'code': 'def test_too_many_positional_params() -> None:\\n    @required_args([\"a\"])\\n    def foo(a: str | None = None) -> str | None:\\n        return a\\n\\n    with pytest.raises(TypeError, match=r\"foo\\\\(\\\\) takes 1 argument\\\\(s\\\\) but 2 were given\"):\\n        foo(\"a\", \"b\")  # type: ignore\\n\\n',\n",
       "  'function_name': 'test_too_many_positional_params',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_required_args.py'},\n",
       " {'code': 'def test_positional_param() -> None:\\n    @required_args([\"a\"])\\n    def foo(a: str | None = None) -> str | None:\\n        return a\\n\\n    assert foo(\"a\") == \"a\"\\n    assert foo(None) is None\\n    assert foo(a=\"b\") == \"b\"\\n\\n    with pytest.raises(TypeError, match=\"Missing required argument: \\'a\\'\"):\\n        foo()\\n\\n',\n",
       "  'function_name': 'test_positional_param',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_required_args.py'},\n",
       " {'code': 'def test_keyword_only_param() -> None:\\n    @required_args([\"a\"])\\n    def foo(*, a: str | None = None) -> str | None:\\n        return a\\n\\n    assert foo(a=\"a\") == \"a\"\\n    assert foo(a=None) is None\\n    assert foo(a=\"b\") == \"b\"\\n\\n    with pytest.raises(TypeError, match=\"Missing required argument: \\'a\\'\"):\\n        foo()\\n\\n',\n",
       "  'function_name': 'test_keyword_only_param',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_required_args.py'},\n",
       " {'code': 'def test_multiple_params() -> None:\\n    @required_args([\"a\", \"b\", \"c\"])\\n    def foo(a: str = \"\", *, b: str = \"\", c: str = \"\") -> str | None:\\n        return f\"{a} {b} {c}\"\\n\\n    assert foo(a=\"a\", b=\"b\", c=\"c\") == \"a b c\"\\n\\n    error_message = r\"Missing required arguments.*\"\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo()\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo(a=\"a\")\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo(b=\"b\")\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo(c=\"c\")\\n\\n    with pytest.raises(TypeError, match=r\"Missing required argument: \\'a\\'\"):\\n        foo(b=\"a\", c=\"c\")\\n\\n    with pytest.raises(TypeError, match=r\"Missing required argument: \\'b\\'\"):\\n        foo(\"a\", c=\"c\")\\n\\n',\n",
       "  'function_name': 'test_multiple_params',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_required_args.py'},\n",
       " {'code': 'def test_multiple_variants() -> None:\\n    @required_args([\"a\"], [\"b\"])\\n    def foo(*, a: str | None = None, b: str | None = None) -> str | None:\\n        return a if a is not None else b\\n\\n    assert foo(a=\"foo\") == \"foo\"\\n    assert foo(b=\"bar\") == \"bar\"\\n    assert foo(a=None) is None\\n    assert foo(b=None) is None\\n\\n    # TODO: this error message could probably be improved\\n    with pytest.raises(\\n        TypeError,\\n        match=r\"Missing required arguments; Expected either \\\\(\\'a\\'\\\\) or \\\\(\\'b\\'\\\\) arguments to be given\",\\n    ):\\n        foo()\\n\\n',\n",
       "  'function_name': 'test_multiple_variants',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_required_args.py'},\n",
       " {'code': 'def test_multiple_params_multiple_variants() -> None:\\n    @required_args([\"a\", \"b\"], [\"c\"])\\n    def foo(*, a: str | None = None, b: str | None = None, c: str | None = None) -> str | None:\\n        if a is not None:\\n            return a\\n        if b is not None:\\n            return b\\n        return c\\n\\n    error_message = r\"Missing required arguments; Expected either \\\\(\\'a\\' and \\'b\\'\\\\) or \\\\(\\'c\\'\\\\) arguments to be given\"\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo(a=\"foo\")\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo(b=\"bar\")\\n\\n    with pytest.raises(TypeError, match=error_message):\\n        foo()\\n\\n    assert foo(a=None, b=\"bar\") == \"bar\"\\n    assert foo(c=None) is None\\n    assert foo(c=\"foo\") == \"foo\"\\n',\n",
       "  'function_name': 'test_multiple_params_multiple_variants',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_required_args.py'},\n",
       " {'code': 'def test_pathlib_includes_file_name() -> None:\\n    result = to_httpx_files({\"file\": readme_path})\\n    print(result)\\n    assert result == IsDict({\"file\": IsTuple(\"README.md\", IsBytes())})\\n\\n',\n",
       "  'function_name': 'test_pathlib_includes_file_name',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_files.py'},\n",
       " {'code': 'def test_tuple_input() -> None:\\n    result = to_httpx_files([(\"file\", readme_path)])\\n    print(result)\\n    assert result == IsList(IsTuple(\"file\", IsTuple(\"README.md\", IsBytes())))\\n\\n',\n",
       "  'function_name': 'test_tuple_input',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_files.py'},\n",
       " {'code': 'async def test_async_pathlib_includes_file_name() -> None:\\n    result = await async_to_httpx_files({\"file\": readme_path})\\n    print(result)\\n    assert result == IsDict({\"file\": IsTuple(\"README.md\", IsBytes())})\\n\\n',\n",
       "  'function_name': 'test_async_pathlib_includes_file_name',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_files.py'},\n",
       " {'code': 'async def test_async_supports_anyio_path() -> None:\\n    result = await async_to_httpx_files({\"file\": anyio.Path(readme_path)})\\n    print(result)\\n    assert result == IsDict({\"file\": IsTuple(\"README.md\", IsBytes())})\\n\\n',\n",
       "  'function_name': 'test_async_supports_anyio_path',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_files.py'},\n",
       " {'code': 'async def test_async_tuple_input() -> None:\\n    result = await async_to_httpx_files([(\"file\", readme_path)])\\n    print(result)\\n    assert result == IsList(IsTuple(\"file\", IsTuple(\"README.md\", IsBytes())))\\n\\n',\n",
       "  'function_name': 'test_async_tuple_input',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_files.py'},\n",
       " {'code': 'def test_string_not_allowed() -> None:\\n    with pytest.raises(TypeError, match=\"Expected file types input to be a FileContent type or to be a tuple\"):\\n        to_httpx_files(\\n            {\\n                \"file\": \"foo\",  # type: ignore\\n            }\\n        )\\n',\n",
       "  'function_name': 'test_string_not_allowed',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_files.py'},\n",
       " {'code': 'async def test_basic(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: completion\\\\n\"\\n        yield b\\'data: {\"foo\":true}\\\\n\\'\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"completion\"\\n    assert sse.json() == {\"foo\": True}\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_basic',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_data_missing_event(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\\'data: {\"foo\":true}\\\\n\\'\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event is None\\n    assert sse.json() == {\"foo\": True}\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_data_missing_event',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_event_missing_data(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: ping\\\\n\"\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"ping\"\\n    assert sse.data == \"\"\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_event_missing_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_multiple_events(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: ping\\\\n\"\\n        yield b\"\\\\n\"\\n        yield b\"event: completion\\\\n\"\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"ping\"\\n    assert sse.data == \"\"\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"completion\"\\n    assert sse.data == \"\"\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_multiple_events',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_multiple_events_with_data(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: ping\\\\n\"\\n        yield b\\'data: {\"foo\":true}\\\\n\\'\\n        yield b\"\\\\n\"\\n        yield b\"event: completion\\\\n\"\\n        yield b\\'data: {\"bar\":false}\\\\n\\'\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"ping\"\\n    assert sse.json() == {\"foo\": True}\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"completion\"\\n    assert sse.json() == {\"bar\": False}\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_multiple_events_with_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_multiple_data_lines_with_empty_line(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: ping\\\\n\"\\n        yield b\"data: {\\\\n\"\\n        yield b\\'data: \"foo\":\\\\n\\'\\n        yield b\"data: \\\\n\"\\n        yield b\"data:\\\\n\"\\n        yield b\"data: true}\\\\n\"\\n        yield b\"\\\\n\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"ping\"\\n    assert sse.json() == {\"foo\": True}\\n    assert sse.data == \\'{\\\\n\"foo\":\\\\n\\\\n\\\\ntrue}\\'\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_multiple_data_lines_with_empty_line',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_data_json_escaped_double_new_line(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: ping\\\\n\"\\n        yield b\\'data: {\"foo\": \"my long\\\\\\\\n\\\\\\\\ncontent\"}\\'\\n        yield b\"\\\\n\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"ping\"\\n    assert sse.json() == {\"foo\": \"my long\\\\n\\\\ncontent\"}\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_data_json_escaped_double_new_line',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_multiple_data_lines(sync: bool, client: OpenAI, async_client: AsyncOpenAI) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\"event: ping\\\\n\"\\n        yield b\"data: {\\\\n\"\\n        yield b\\'data: \"foo\":\\\\n\\'\\n        yield b\"data: true}\\\\n\"\\n        yield b\"\\\\n\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event == \"ping\"\\n    assert sse.json() == {\"foo\": True}\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_multiple_data_lines',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_special_new_line_character(\\n    sync: bool,\\n    client: OpenAI,\\n    async_client: AsyncOpenAI,\\n) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\\'data: {\"content\":\" culpa\"}\\\\n\\'\\n        yield b\"\\\\n\"\\n        yield b\\'data: {\"content\":\" \\\\xe2\\\\x80\\\\xa8\"}\\\\n\\'\\n        yield b\"\\\\n\"\\n        yield b\\'data: {\"content\":\"foo\"}\\\\n\\'\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event is None\\n    assert sse.json() == {\"content\": \" culpa\"}\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event is None\\n    assert sse.json() == {\"content\": \" \\u2028\"}\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event is None\\n    assert sse.json() == {\"content\": \"foo\"}\\n\\n    await assert_empty_iter(iterator)\\n\\n',\n",
       "  'function_name': 'test_special_new_line_character',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def test_multi_byte_character_multiple_chunks(\\n    sync: bool,\\n    client: OpenAI,\\n    async_client: AsyncOpenAI,\\n) -> None:\\n    def body() -> Iterator[bytes]:\\n        yield b\\'data: {\"content\":\"\\'\\n        # bytes taken from the string \\'известни\\' and arbitrarily split\\n        # so that some multi-byte characters span multiple chunks\\n        yield b\"\\\\xd0\"\\n        yield b\"\\\\xb8\\\\xd0\\\\xb7\\\\xd0\"\\n        yield b\"\\\\xb2\\\\xd0\\\\xb5\\\\xd1\\\\x81\\\\xd1\\\\x82\\\\xd0\\\\xbd\\\\xd0\\\\xb8\"\\n        yield b\\'\"}\\\\n\\'\\n        yield b\"\\\\n\"\\n\\n    iterator = make_event_iterator(content=body(), sync=sync, client=client, async_client=async_client)\\n\\n    sse = await iter_next(iterator)\\n    assert sse.event is None\\n    assert sse.json() == {\"content\": \"известни\"}\\n\\n',\n",
       "  'function_name': 'test_multi_byte_character_multiple_chunks',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def to_aiter(iter: Iterator[bytes]) -> AsyncIterator[bytes]:\\n    for chunk in iter:\\n        yield chunk\\n\\n',\n",
       "  'function_name': 'to_aiter',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def iter_next(iter: Iterator[ServerSentEvent] | AsyncIterator[ServerSentEvent]) -> ServerSentEvent:\\n    if isinstance(iter, AsyncIterator):\\n        return await iter.__anext__()\\n\\n    return next(iter)\\n\\n',\n",
       "  'function_name': 'iter_next',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'async def assert_empty_iter(iter: Iterator[ServerSentEvent] | AsyncIterator[ServerSentEvent]) -> None:\\n    with pytest.raises((StopAsyncIteration, RuntimeError)):\\n        await iter_next(iter)\\n\\n',\n",
       "  'function_name': 'assert_empty_iter',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'def make_event_iterator(\\n    content: Iterator[bytes],\\n    *,\\n    sync: bool,\\n    client: OpenAI,\\n    async_client: AsyncOpenAI,\\n) -> Iterator[ServerSentEvent] | AsyncIterator[ServerSentEvent]:\\n    if sync:\\n        return Stream(cast_to=object, client=client, response=httpx.Response(200, content=content))._iter_events()\\n\\n    return AsyncStream(\\n        cast_to=object, client=async_client, response=httpx.Response(200, content=to_aiter(content))\\n    )._iter_events()\\n',\n",
       "  'function_name': 'make_event_iterator',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_streaming.py'},\n",
       " {'code': 'def assert_different_identities(obj1: object, obj2: object) -> None:\\n    assert obj1 == obj2\\n    assert id(obj1) != id(obj2)\\n\\n',\n",
       "  'function_name': 'assert_different_identities',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_simple_dict() -> None:\\n    obj1 = {\"foo\": \"bar\"}\\n    obj2 = deepcopy_minimal(obj1)\\n    assert_different_identities(obj1, obj2)\\n\\n',\n",
       "  'function_name': 'test_simple_dict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_nested_dict() -> None:\\n    obj1 = {\"foo\": {\"bar\": True}}\\n    obj2 = deepcopy_minimal(obj1)\\n    assert_different_identities(obj1, obj2)\\n    assert_different_identities(obj1[\"foo\"], obj2[\"foo\"])\\n\\n',\n",
       "  'function_name': 'test_nested_dict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_complex_nested_dict() -> None:\\n    obj1 = {\"foo\": {\"bar\": [{\"hello\": \"world\"}]}}\\n    obj2 = deepcopy_minimal(obj1)\\n    assert_different_identities(obj1, obj2)\\n    assert_different_identities(obj1[\"foo\"], obj2[\"foo\"])\\n    assert_different_identities(obj1[\"foo\"][\"bar\"], obj2[\"foo\"][\"bar\"])\\n    assert_different_identities(obj1[\"foo\"][\"bar\"][0], obj2[\"foo\"][\"bar\"][0])\\n\\n',\n",
       "  'function_name': 'test_complex_nested_dict',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_simple_list() -> None:\\n    obj1 = [\"a\", \"b\", \"c\"]\\n    obj2 = deepcopy_minimal(obj1)\\n    assert_different_identities(obj1, obj2)\\n\\n',\n",
       "  'function_name': 'test_simple_list',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_nested_list() -> None:\\n    obj1 = [\"a\", [1, 2, 3]]\\n    obj2 = deepcopy_minimal(obj1)\\n    assert_different_identities(obj1, obj2)\\n    assert_different_identities(obj1[1], obj2[1])\\n\\n',\n",
       "  'function_name': 'test_nested_list',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_ignores_other_types() -> None:\\n    # custom classes\\n    my_obj = MyObject()\\n    obj1 = {\"foo\": my_obj}\\n    obj2 = deepcopy_minimal(obj1)\\n    assert_different_identities(obj1, obj2)\\n    assert obj1[\"foo\"] is my_obj\\n\\n    # tuples\\n    obj3 = (\"a\", \"b\")\\n    obj4 = deepcopy_minimal(obj3)\\n    assert obj3 is obj4\\n',\n",
       "  'function_name': 'test_ignores_other_types',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_deepcopy.py'},\n",
       " {'code': 'def test_implicit_deployment_path(client: Client) -> None:\\n    req = client._build_request(\\n        FinalRequestOptions.construct(\\n            method=\"post\",\\n            url=\"/chat/completions\",\\n            json_data={\"model\": \"my-deployment-model\"},\\n        )\\n    )\\n    assert (\\n        req.url\\n        == \"https://example-resource.azure.openai.com/openai/deployments/my-deployment-model/chat/completions?api-version=2023-07-01\"\\n    )\\n\\n',\n",
       "  'function_name': 'test_implicit_deployment_path',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/lib/test_azure.py'},\n",
       " {'code': 'def test_client_copying(client: Client, method: Literal[\"copy\", \"with_options\"]) -> None:\\n    if method == \"copy\":\\n        copied = client.copy()\\n    else:\\n        copied = client.with_options()\\n\\n    assert copied._custom_query == {\"api-version\": \"2023-07-01\"}\\n\\n',\n",
       "  'function_name': 'test_client_copying',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/lib/test_azure.py'},\n",
       " {'code': 'def test_client_copying_override_options(client: Client) -> None:\\n    copied = client.copy(\\n        api_version=\"2022-05-01\",\\n    )\\n    assert copied._custom_query == {\"api-version\": \"2022-05-01\"}\\n',\n",
       "  'function_name': 'test_client_copying_override_options',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/lib/test_azure.py'},\n",
       " {'code': 'def test_basic_attribute_access_works() -> None:\\n    for attr in dir(openai):\\n        dir(getattr(openai, attr))\\n\\n',\n",
       "  'function_name': 'test_basic_attribute_access_works',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/lib/test_old_api.py'},\n",
       " {'code': 'def test_helpful_error_is_raised() -> None:\\n    with pytest.raises(APIRemovedInV1):\\n        openai.Completion.create()  # type: ignore\\n\\n    with pytest.raises(APIRemovedInV1):\\n        openai.ChatCompletion.create()  # type: ignore\\n',\n",
       "  'function_name': 'test_helpful_error_is_raised',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/lib/test_old_api.py'},\n",
       " {'code': 'def test_extract_type_var() -> None:\\n    assert (\\n        extract_type_var_from_base(\\n            BaseGeneric[int],\\n            index=0,\\n            generic_bases=cast(\"tuple[type, ...]\", (BaseGeneric,)),\\n        )\\n        == int\\n    )\\n\\n',\n",
       "  'function_name': 'test_extract_type_var',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_utils/test_typing.py'},\n",
       " {'code': 'def test_extract_type_var_generic_subclass() -> None:\\n    assert (\\n        extract_type_var_from_base(\\n            SubclassGeneric[int],\\n            index=0,\\n            generic_bases=cast(\"tuple[type, ...]\", (BaseGeneric,)),\\n        )\\n        == int\\n    )\\n\\n',\n",
       "  'function_name': 'test_extract_type_var_generic_subclass',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_utils/test_typing.py'},\n",
       " {'code': 'def test_extract_type_var_multiple() -> None:\\n    typ = BaseGenericMultipleTypeArgs[int, str, None]\\n\\n    generic_bases = cast(\"tuple[type, ...]\", (BaseGenericMultipleTypeArgs,))\\n    assert extract_type_var_from_base(typ, index=0, generic_bases=generic_bases) == int\\n    assert extract_type_var_from_base(typ, index=1, generic_bases=generic_bases) == str\\n    assert extract_type_var_from_base(typ, index=2, generic_bases=generic_bases) == type(None)\\n\\n',\n",
       "  'function_name': 'test_extract_type_var_multiple',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_utils/test_typing.py'},\n",
       " {'code': 'def test_extract_type_var_generic_subclass_multiple() -> None:\\n    typ = SubclassGenericMultipleTypeArgs[int, str, None]\\n\\n    generic_bases = cast(\"tuple[type, ...]\", (BaseGenericMultipleTypeArgs,))\\n    assert extract_type_var_from_base(typ, index=0, generic_bases=generic_bases) == int\\n    assert extract_type_var_from_base(typ, index=1, generic_bases=generic_bases) == str\\n    assert extract_type_var_from_base(typ, index=2, generic_bases=generic_bases) == type(None)\\n\\n',\n",
       "  'function_name': 'test_extract_type_var_generic_subclass_multiple',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_utils/test_typing.py'},\n",
       " {'code': 'def test_extract_type_var_generic_subclass_different_ordering_multiple() -> None:\\n    typ = SubclassDifferentOrderGenericMultipleTypeArgs[int, str, None]\\n\\n    generic_bases = cast(\"tuple[type, ...]\", (BaseGenericMultipleTypeArgs,))\\n    assert extract_type_var_from_base(typ, index=0, generic_bases=generic_bases) == int\\n    assert extract_type_var_from_base(typ, index=1, generic_bases=generic_bases) == str\\n    assert extract_type_var_from_base(typ, index=2, generic_bases=generic_bases) == type(None)\\n',\n",
       "  'function_name': 'test_extract_type_var_generic_subclass_different_ordering_multiple',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_utils/test_typing.py'},\n",
       " {'code': 'def test_recursive_proxy() -> None:\\n    proxy = RecursiveLazyProxy()\\n    assert repr(proxy) == \"RecursiveLazyProxy\"\\n    assert str(proxy) == \"RecursiveLazyProxy\"\\n    assert dir(proxy) == []\\n    assert type(proxy).__name__ == \"RecursiveLazyProxy\"\\n    assert type(operator.attrgetter(\"name.foo.bar.baz\")(proxy)).__name__ == \"RecursiveLazyProxy\"\\n',\n",
       "  'function_name': 'test_recursive_proxy',\n",
       "  'filepath': '/home/moonpatel/openai-python/tests/test_utils/test_proxy.py'},\n",
       " {'code': 'def _construct_field(value: object, field: FieldInfo, key: str) -> object:\\n    if value is None:\\n        return field_get_default(field)\\n\\n    if PYDANTIC_V2:\\n        type_ = field.annotation\\n    else:\\n        type_ = cast(type, field.outer_type_)  # type: ignore\\n\\n    if type_ is None:\\n        raise RuntimeError(f\"Unexpected field type is None for {key}\")\\n\\n    return construct_type(value=value, type_=type_)\\n\\n',\n",
       "  'function_name': '_construct_field',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def is_basemodel(type_: type) -> bool:\\n    \"\"\"Returns whether or not the given type is either a `BaseModel` or a union of `BaseModel`\"\"\"\\n    if is_union(type_):\\n        for variant in get_args(type_):\\n            if is_basemodel(variant):\\n                return True\\n\\n        return False\\n\\n    return is_basemodel_type(type_)\\n\\n',\n",
       "  'function_name': 'is_basemodel',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def is_basemodel_type(type_: type) -> TypeGuard[type[BaseModel] | type[GenericModel]]:\\n    origin = get_origin(type_) or type_\\n    return issubclass(origin, BaseModel) or issubclass(origin, GenericModel)\\n\\n',\n",
       "  'function_name': 'is_basemodel_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def construct_type(*, value: object, type_: object) -> object:\\n    \"\"\"Loose coercion to the expected type with construction of nested values.\\n\\n    If the given value does not match the expected type then it is returned as-is.\\n    \"\"\"\\n    # we allow `object` as the input type because otherwise, passing things like\\n    # `Literal[\\'value\\']` will be reported as a type error by type checkers\\n    type_ = cast(\"type[object]\", type_)\\n\\n    # unwrap `Annotated[T, ...]` -> `T`\\n    if is_annotated_type(type_):\\n        meta: tuple[Any, ...] = get_args(type_)[1:]\\n        type_ = extract_type_arg(type_, 0)\\n    else:\\n        meta = tuple()\\n\\n    # we need to use the origin class for any types that are subscripted generics\\n    # e.g. Dict[str, object]\\n    origin = get_origin(type_) or type_\\n    args = get_args(type_)\\n\\n    if is_union(origin):\\n        try:\\n            return validate_type(type_=cast(\"type[object]\", type_), value=value)\\n        except Exception:\\n            pass\\n\\n        # if the type is a discriminated union then we want to construct the right variant\\n        # in the union, even if the data doesn\\'t match exactly, otherwise we\\'d break code\\n        # that relies on the constructed class types, e.g.\\n        #\\n        # class FooType:\\n        #   kind: Literal[\\'foo\\']\\n        #   value: str\\n        #\\n        # class BarType:\\n        #   kind: Literal[\\'bar\\']\\n        #   value: int\\n        #\\n        # without this block, if the data we get is something like `{\\'kind\\': \\'bar\\', \\'value\\': \\'foo\\'}` then\\n        # we\\'d end up constructing `FooType` when it should be `BarType`.\\n        discriminator = _build_discriminated_union_meta(union=type_, meta_annotations=meta)\\n        if discriminator and is_mapping(value):\\n            variant_value = value.get(discriminator.field_alias_from or discriminator.field_name)\\n            if variant_value and isinstance(variant_value, str):\\n                variant_type = discriminator.mapping.get(variant_value)\\n                if variant_type:\\n                    return construct_type(type_=variant_type, value=value)\\n\\n        # if the data is not valid, use the first variant that doesn\\'t fail while deserializing\\n        for variant in args:\\n            try:\\n                return construct_type(value=value, type_=variant)\\n            except Exception:\\n                continue\\n\\n        raise RuntimeError(f\"Could not convert data into a valid instance of {type_}\")\\n\\n    if origin == dict:\\n        if not is_mapping(value):\\n            return value\\n\\n        _, items_type = get_args(type_)  # Dict[_, items_type]\\n        return {key: construct_type(value=item, type_=items_type) for key, item in value.items()}\\n\\n    if not is_literal_type(type_) and (issubclass(origin, BaseModel) or issubclass(origin, GenericModel)):\\n        if is_list(value):\\n            return [cast(Any, type_).construct(**entry) if is_mapping(entry) else entry for entry in value]\\n\\n        if is_mapping(value):\\n            if issubclass(type_, BaseModel):\\n                return type_.construct(**value)  # type: ignore[arg-type]\\n\\n            return cast(Any, type_).construct(**value)\\n\\n    if origin == list:\\n        if not is_list(value):\\n            return value\\n\\n        inner_type = args[0]  # List[inner_type]\\n        return [construct_type(value=entry, type_=inner_type) for entry in value]\\n\\n    if origin == float:\\n        if isinstance(value, int):\\n            coerced = float(value)\\n            if coerced != value:\\n                return value\\n            return coerced\\n\\n        return value\\n\\n    if type_ == datetime:\\n        try:\\n            return parse_datetime(value)  # type: ignore\\n        except Exception:\\n            return value\\n\\n    if type_ == date:\\n        try:\\n            return parse_date(value)  # type: ignore\\n        except Exception:\\n            return value\\n\\n    return value\\n\\n',\n",
       "  'function_name': 'construct_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def _build_discriminated_union_meta(*, union: type, meta_annotations: tuple[Any, ...]) -> DiscriminatorDetails | None:\\n    if isinstance(union, CachedDiscriminatorType):\\n        return union.__discriminator__\\n\\n    discriminator_field_name: str | None = None\\n\\n    for annotation in meta_annotations:\\n        if isinstance(annotation, PropertyInfo) and annotation.discriminator is not None:\\n            discriminator_field_name = annotation.discriminator\\n            break\\n\\n    if not discriminator_field_name:\\n        return None\\n\\n    mapping: dict[str, type] = {}\\n    discriminator_alias: str | None = None\\n\\n    for variant in get_args(union):\\n        variant = strip_annotated_type(variant)\\n        if is_basemodel_type(variant):\\n            if PYDANTIC_V2:\\n                field = _extract_field_schema_pv2(variant, discriminator_field_name)\\n                if not field:\\n                    continue\\n\\n                # Note: if one variant defines an alias then they all should\\n                discriminator_alias = field.get(\"serialization_alias\")\\n\\n                field_schema = field[\"schema\"]\\n\\n                if field_schema[\"type\"] == \"literal\":\\n                    for entry in field_schema[\"expected\"]:\\n                        if isinstance(entry, str):\\n                            mapping[entry] = variant\\n            else:\\n                field_info = cast(\"dict[str, FieldInfo]\", variant.__fields__).get(discriminator_field_name)  # pyright: ignore[reportDeprecated, reportUnnecessaryCast]\\n                if not field_info:\\n                    continue\\n\\n                # Note: if one variant defines an alias then they all should\\n                discriminator_alias = field_info.alias\\n\\n                if field_info.annotation and is_literal_type(field_info.annotation):\\n                    for entry in get_args(field_info.annotation):\\n                        if isinstance(entry, str):\\n                            mapping[entry] = variant\\n\\n    if not mapping:\\n        return None\\n\\n    details = DiscriminatorDetails(\\n        mapping=mapping,\\n        discriminator_field=discriminator_field_name,\\n        discriminator_alias=discriminator_alias,\\n    )\\n    cast(CachedDiscriminatorType, union).__discriminator__ = details\\n    return details\\n\\n',\n",
       "  'function_name': '_build_discriminated_union_meta',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def _extract_field_schema_pv2(model: type[BaseModel], field_name: str) -> ModelField | None:\\n    schema = model.__pydantic_core_schema__\\n    if schema[\"type\"] != \"model\":\\n        return None\\n\\n    fields_schema = schema[\"schema\"]\\n    if fields_schema[\"type\"] != \"model-fields\":\\n        return None\\n\\n    fields_schema = cast(\"ModelFieldsSchema\", fields_schema)\\n\\n    field = fields_schema[\"fields\"].get(field_name)\\n    if not field:\\n        return None\\n\\n    return cast(\"ModelField\", field)  # pyright: ignore[reportUnnecessaryCast]\\n\\n',\n",
       "  'function_name': '_extract_field_schema_pv2',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def validate_type(*, type_: type[_T], value: object) -> _T:\\n    \"\"\"Strict validation that the given value matches the expected type\"\"\"\\n    if inspect.isclass(type_) and issubclass(type_, pydantic.BaseModel):\\n        return cast(_T, parse_obj(type_, value))\\n\\n    return cast(_T, _validate_non_model_type(type_=type_, value=value))\\n\\n',\n",
       "  'function_name': 'validate_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_models.py'},\n",
       " {'code': 'def to_raw_response_wrapper(func: Callable[P, R]) -> Callable[P, LegacyAPIResponse[R]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and wraps it\\n    to support returning the raw `APIResponse` object directly.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> LegacyAPIResponse[R]:\\n        extra_headers: dict[str, str] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"true\"\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        return cast(LegacyAPIResponse[R], func(*args, **kwargs))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'to_raw_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_legacy_response.py'},\n",
       " {'code': 'def async_to_raw_response_wrapper(func: Callable[P, Awaitable[R]]) -> Callable[P, Awaitable[LegacyAPIResponse[R]]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and wraps it\\n    to support returning the raw `APIResponse` object directly.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    async def wrapped(*args: P.args, **kwargs: P.kwargs) -> LegacyAPIResponse[R]:\\n        extra_headers: dict[str, str] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"true\"\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'async_to_raw_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_legacy_response.py'},\n",
       " {'code': 'def make_request_options(\\n    *,\\n    query: Query | None = None,\\n    extra_headers: Headers | None = None,\\n    extra_query: Query | None = None,\\n    extra_body: Body | None = None,\\n    idempotency_key: str | None = None,\\n    timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\\n    post_parser: PostParser | NotGiven = NOT_GIVEN,\\n) -> RequestOptions:\\n    \"\"\"Create a dict of type RequestOptions without keys of NotGiven values.\"\"\"\\n    options: RequestOptions = {}\\n    if extra_headers is not None:\\n        options[\"headers\"] = extra_headers\\n\\n    if extra_body is not None:\\n        options[\"extra_json\"] = cast(AnyMapping, extra_body)\\n\\n    if query is not None:\\n        options[\"params\"] = query\\n\\n    if extra_query is not None:\\n        options[\"params\"] = {**options.get(\"params\", {}), **extra_query}\\n\\n    if not isinstance(timeout, NotGiven):\\n        options[\"timeout\"] = timeout\\n\\n    if idempotency_key is not None:\\n        options[\"idempotency_key\"] = idempotency_key\\n\\n    if is_given(post_parser):\\n        # internal\\n        options[\"post_parser\"] = post_parser  # type: ignore\\n\\n    return options\\n\\n',\n",
       "  'function_name': 'make_request_options',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def get_platform() -> Platform:\\n    try:\\n        system = platform.system().lower()\\n        platform_name = platform.platform().lower()\\n    except Exception:\\n        return \"Unknown\"\\n\\n    if \"iphone\" in platform_name or \"ipad\" in platform_name:\\n        # Tested using Python3IDE on an iPhone 11 and Pythonista on an iPad 7\\n        # system is Darwin and platform_name is a string like:\\n        # - Darwin-21.6.0-iPhone12,1-64bit\\n        # - Darwin-21.6.0-iPad7,11-64bit\\n        return \"iOS\"\\n\\n    if system == \"darwin\":\\n        return \"MacOS\"\\n\\n    if system == \"windows\":\\n        return \"Windows\"\\n\\n    if \"android\" in platform_name:\\n        # Tested using Pydroid 3\\n        # system is Linux and platform_name is a string like \\'Linux-5.10.81-android12-9-00001-geba40aecb3b7-ab8534902-aarch64-with-libc\\'\\n        return \"Android\"\\n\\n    if system == \"linux\":\\n        # https://distro.readthedocs.io/en/latest/#distro.id\\n        distro_id = distro.id()\\n        if distro_id == \"freebsd\":\\n            return \"FreeBSD\"\\n\\n        if distro_id == \"openbsd\":\\n            return \"OpenBSD\"\\n\\n        return \"Linux\"\\n\\n    if platform_name:\\n        return OtherPlatform(platform_name)\\n\\n    return \"Unknown\"\\n\\n',\n",
       "  'function_name': 'get_platform',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def platform_headers(version: str) -> Dict[str, str]:\\n    return {\\n        \"X-Stainless-Lang\": \"python\",\\n        \"X-Stainless-Package-Version\": version,\\n        \"X-Stainless-OS\": str(get_platform()),\\n        \"X-Stainless-Arch\": str(get_architecture()),\\n        \"X-Stainless-Runtime\": get_python_runtime(),\\n        \"X-Stainless-Runtime-Version\": get_python_version(),\\n    }\\n\\n',\n",
       "  'function_name': 'platform_headers',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def get_python_runtime() -> str:\\n    try:\\n        return platform.python_implementation()\\n    except Exception:\\n        return \"unknown\"\\n\\n',\n",
       "  'function_name': 'get_python_runtime',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def get_python_version() -> str:\\n    try:\\n        return platform.python_version()\\n    except Exception:\\n        return \"unknown\"\\n\\n',\n",
       "  'function_name': 'get_python_version',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def get_architecture() -> Arch:\\n    try:\\n        python_bitness, _ = platform.architecture()\\n        machine = platform.machine().lower()\\n    except Exception:\\n        return \"unknown\"\\n\\n    if machine in (\"arm64\", \"aarch64\"):\\n        return \"arm64\"\\n\\n    # TODO: untested\\n    if machine == \"arm\":\\n        return \"arm\"\\n\\n    if machine == \"x86_64\":\\n        return \"x64\"\\n\\n    # TODO: untested\\n    if python_bitness == \"32bit\":\\n        return \"x32\"\\n\\n    if machine:\\n        return OtherArch(machine)\\n\\n    return \"unknown\"\\n\\n',\n",
       "  'function_name': 'get_architecture',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def _merge_mappings(\\n    obj1: Mapping[_T_co, Union[_T, Omit]],\\n    obj2: Mapping[_T_co, Union[_T, Omit]],\\n) -> Dict[_T_co, _T]:\\n    \"\"\"Merge two mappings of the same type, removing any values that are instances of `Omit`.\\n\\n    In cases with duplicate keys the second mapping takes precedence.\\n    \"\"\"\\n    merged = {**obj1, **obj2}\\n    return {key: value for key, value in merged.items() if not isinstance(value, Omit)}\\n',\n",
       "  'function_name': '_merge_mappings',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_base_client.py'},\n",
       " {'code': 'def is_base64_file_input(obj: object) -> TypeGuard[Base64FileInput]:\\n    return isinstance(obj, io.IOBase) or isinstance(obj, os.PathLike)\\n\\n',\n",
       "  'function_name': 'is_base64_file_input',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def is_file_content(obj: object) -> TypeGuard[FileContent]:\\n    return (\\n        isinstance(obj, bytes) or isinstance(obj, tuple) or isinstance(obj, io.IOBase) or isinstance(obj, os.PathLike)\\n    )\\n\\n',\n",
       "  'function_name': 'is_file_content',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def assert_is_file_content(obj: object, *, key: str | None = None) -> None:\\n    if not is_file_content(obj):\\n        prefix = f\"Expected entry at `{key}`\" if key is not None else f\"Expected file input `{obj!r}`\"\\n        raise RuntimeError(\\n            f\"{prefix} to be bytes, an io.IOBase instance, PathLike or a tuple but received {type(obj)} instead. See https://github.com/openai/openai-python/tree/main#file-uploads\"\\n        ) from None\\n\\n',\n",
       "  'function_name': 'assert_is_file_content',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def to_httpx_files(files: None) -> None:\\n    ...\\n\\n',\n",
       "  'function_name': 'to_httpx_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def to_httpx_files(files: RequestFiles) -> HttpxRequestFiles:\\n    ...\\n\\n',\n",
       "  'function_name': 'to_httpx_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def to_httpx_files(files: RequestFiles | None) -> HttpxRequestFiles | None:\\n    if files is None:\\n        return None\\n\\n    if is_mapping_t(files):\\n        files = {key: _transform_file(file) for key, file in files.items()}\\n    elif is_sequence_t(files):\\n        files = [(key, _transform_file(file)) for key, file in files]\\n    else:\\n        raise TypeError(f\"Unexpected file type input {type(files)}, expected mapping or sequence\")\\n\\n    return files\\n\\n',\n",
       "  'function_name': 'to_httpx_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def _transform_file(file: FileTypes) -> HttpxFileTypes:\\n    if is_file_content(file):\\n        if isinstance(file, os.PathLike):\\n            path = pathlib.Path(file)\\n            return (path.name, path.read_bytes())\\n\\n        return file\\n\\n    if is_tuple_t(file):\\n        return (file[0], _read_file_content(file[1]), *file[2:])\\n\\n    raise TypeError(f\"Expected file types input to be a FileContent type or to be a tuple\")\\n\\n',\n",
       "  'function_name': '_transform_file',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def _read_file_content(file: FileContent) -> HttpxFileContent:\\n    if isinstance(file, os.PathLike):\\n        return pathlib.Path(file).read_bytes()\\n    return file\\n\\n',\n",
       "  'function_name': '_read_file_content',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'async def async_to_httpx_files(files: None) -> None:\\n    ...\\n\\n',\n",
       "  'function_name': 'async_to_httpx_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'async def async_to_httpx_files(files: RequestFiles) -> HttpxRequestFiles:\\n    ...\\n\\n',\n",
       "  'function_name': 'async_to_httpx_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'async def async_to_httpx_files(files: RequestFiles | None) -> HttpxRequestFiles | None:\\n    if files is None:\\n        return None\\n\\n    if is_mapping_t(files):\\n        files = {key: await _async_transform_file(file) for key, file in files.items()}\\n    elif is_sequence_t(files):\\n        files = [(key, await _async_transform_file(file)) for key, file in files]\\n    else:\\n        raise TypeError(\"Unexpected file type input {type(files)}, expected mapping or sequence\")\\n\\n    return files\\n\\n',\n",
       "  'function_name': 'async_to_httpx_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'async def _async_transform_file(file: FileTypes) -> HttpxFileTypes:\\n    if is_file_content(file):\\n        if isinstance(file, os.PathLike):\\n            path = anyio.Path(file)\\n            return (path.name, await path.read_bytes())\\n\\n        return file\\n\\n    if is_tuple_t(file):\\n        return (file[0], await _async_read_file_content(file[1]), *file[2:])\\n\\n    raise TypeError(f\"Expected file types input to be a FileContent type or to be a tuple\")\\n\\n',\n",
       "  'function_name': '_async_transform_file',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'async def _async_read_file_content(file: FileContent) -> HttpxFileContent:\\n    if isinstance(file, os.PathLike):\\n        return await anyio.Path(file).read_bytes()\\n\\n    return file\\n',\n",
       "  'function_name': '_async_read_file_content',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_files.py'},\n",
       " {'code': 'def _has_openai_credentials() -> bool:\\n    return _os.environ.get(\"OPENAI_API_KEY\") is not None\\n\\n',\n",
       "  'function_name': '_has_openai_credentials',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/__init__.py'},\n",
       " {'code': 'def _has_azure_credentials() -> bool:\\n    return azure_endpoint is not None or _os.environ.get(\"AZURE_OPENAI_API_KEY\") is not None\\n\\n',\n",
       "  'function_name': '_has_azure_credentials',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/__init__.py'},\n",
       " {'code': 'def _has_azure_ad_credentials() -> bool:\\n    return (\\n        _os.environ.get(\"AZURE_OPENAI_AD_TOKEN\") is not None\\n        or azure_ad_token is not None\\n        or azure_ad_token_provider is not None\\n    )\\n\\n',\n",
       "  'function_name': '_has_azure_ad_credentials',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/__init__.py'},\n",
       " {'code': 'def _load_client() -> OpenAI:  # type: ignore[reportUnusedFunction]\\n    global _client\\n\\n    if _client is None:\\n        global api_type, azure_endpoint, azure_ad_token, api_version\\n\\n        if azure_endpoint is None:\\n            azure_endpoint = _os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\\n\\n        if azure_ad_token is None:\\n            azure_ad_token = _os.environ.get(\"AZURE_OPENAI_AD_TOKEN\")\\n\\n        if api_version is None:\\n            api_version = _os.environ.get(\"OPENAI_API_VERSION\")\\n\\n        if api_type is None:\\n            has_openai = _has_openai_credentials()\\n            has_azure = _has_azure_credentials()\\n            has_azure_ad = _has_azure_ad_credentials()\\n\\n            if has_openai and (has_azure or has_azure_ad):\\n                raise _AmbiguousModuleClientUsageError()\\n\\n            if (azure_ad_token is not None or azure_ad_token_provider is not None) and _os.environ.get(\\n                \"AZURE_OPENAI_API_KEY\"\\n            ) is not None:\\n                raise _AmbiguousModuleClientUsageError()\\n\\n            if has_azure or has_azure_ad:\\n                api_type = \"azure\"\\n            else:\\n                api_type = \"openai\"\\n\\n        if api_type == \"azure\":\\n            _client = _AzureModuleClient(  # type: ignore\\n                api_version=api_version,\\n                azure_endpoint=azure_endpoint,\\n                api_key=api_key,\\n                azure_ad_token=azure_ad_token,\\n                azure_ad_token_provider=azure_ad_token_provider,\\n                organization=organization,\\n                base_url=base_url,\\n                timeout=timeout,\\n                max_retries=max_retries,\\n                default_headers=default_headers,\\n                default_query=default_query,\\n                http_client=http_client,\\n            )\\n            return _client\\n\\n        _client = _ModuleClient(\\n            api_key=api_key,\\n            organization=organization,\\n            project=project,\\n            base_url=base_url,\\n            timeout=timeout,\\n            max_retries=max_retries,\\n            default_headers=default_headers,\\n            default_query=default_query,\\n            http_client=http_client,\\n        )\\n        return _client\\n\\n    return _client\\n\\n',\n",
       "  'function_name': '_load_client',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/__init__.py'},\n",
       " {'code': 'def _reset_client() -> None:  # type: ignore[reportUnusedFunction]\\n    global _client\\n\\n    _client = None\\n\\n',\n",
       "  'function_name': '_reset_client',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/__init__.py'},\n",
       " {'code': 'def parse_obj(model: type[_ModelT], value: object) -> _ModelT:\\n    if PYDANTIC_V2:\\n        return model.model_validate(value)\\n    else:\\n        return cast(_ModelT, model.parse_obj(value))  # pyright: ignore[reportDeprecated, reportUnnecessaryCast]\\n\\n',\n",
       "  'function_name': 'parse_obj',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def field_is_required(field: FieldInfo) -> bool:\\n    if PYDANTIC_V2:\\n        return field.is_required()\\n    return field.required  # type: ignore\\n\\n',\n",
       "  'function_name': 'field_is_required',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def field_get_default(field: FieldInfo) -> Any:\\n    value = field.get_default()\\n    if PYDANTIC_V2:\\n        from pydantic_core import PydanticUndefined\\n\\n        if value == PydanticUndefined:\\n            return None\\n        return value\\n    return value\\n\\n',\n",
       "  'function_name': 'field_get_default',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def field_outer_type(field: FieldInfo) -> Any:\\n    if PYDANTIC_V2:\\n        return field.annotation\\n    return field.outer_type_  # type: ignore\\n\\n',\n",
       "  'function_name': 'field_outer_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def get_model_config(model: type[pydantic.BaseModel]) -> Any:\\n    if PYDANTIC_V2:\\n        return model.model_config\\n    return model.__config__  # type: ignore\\n\\n',\n",
       "  'function_name': 'get_model_config',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def get_model_fields(model: type[pydantic.BaseModel]) -> dict[str, FieldInfo]:\\n    if PYDANTIC_V2:\\n        return model.model_fields\\n    return model.__fields__  # type: ignore\\n\\n',\n",
       "  'function_name': 'get_model_fields',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def model_copy(model: _ModelT) -> _ModelT:\\n    if PYDANTIC_V2:\\n        return model.model_copy()\\n    return model.copy()  # type: ignore\\n\\n',\n",
       "  'function_name': 'model_copy',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def model_json(model: pydantic.BaseModel, *, indent: int | None = None) -> str:\\n    if PYDANTIC_V2:\\n        return model.model_dump_json(indent=indent)\\n    return model.json(indent=indent)  # type: ignore\\n\\n',\n",
       "  'function_name': 'model_json',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def model_dump(\\n    model: pydantic.BaseModel,\\n    *,\\n    exclude_unset: bool = False,\\n    exclude_defaults: bool = False,\\n) -> dict[str, Any]:\\n    if PYDANTIC_V2:\\n        return model.model_dump(\\n            exclude_unset=exclude_unset,\\n            exclude_defaults=exclude_defaults,\\n        )\\n    return cast(\\n        \"dict[str, Any]\",\\n        model.dict(  # pyright: ignore[reportDeprecated, reportUnnecessaryCast]\\n            exclude_unset=exclude_unset,\\n            exclude_defaults=exclude_defaults,\\n        ),\\n    )\\n\\n',\n",
       "  'function_name': 'model_dump',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def model_parse(model: type[_ModelT], data: Any) -> _ModelT:\\n    if PYDANTIC_V2:\\n        return model.model_validate(data)\\n    return model.parse_obj(data)  # pyright: ignore[reportDeprecated]\\n\\n',\n",
       "  'function_name': 'model_parse',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_compat.py'},\n",
       " {'code': 'def to_streamed_response_wrapper(func: Callable[P, R]) -> Callable[P, ResponseContextManager[APIResponse[R]]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and wraps it\\n    to support streaming and returning the raw `APIResponse` object directly.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> ResponseContextManager[APIResponse[R]]:\\n        extra_headers: dict[str, str] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"stream\"\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        make_request = functools.partial(func, *args, **kwargs)\\n\\n        return ResponseContextManager(cast(Callable[[], APIResponse[R]], make_request))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'to_streamed_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def async_to_streamed_response_wrapper(\\n    func: Callable[P, Awaitable[R]],\\n) -> Callable[P, AsyncResponseContextManager[AsyncAPIResponse[R]]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and wraps it\\n    to support streaming and returning the raw `APIResponse` object directly.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> AsyncResponseContextManager[AsyncAPIResponse[R]]:\\n        extra_headers: dict[str, str] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"stream\"\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        make_request = func(*args, **kwargs)\\n\\n        return AsyncResponseContextManager(cast(Awaitable[AsyncAPIResponse[R]], make_request))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'async_to_streamed_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def to_custom_streamed_response_wrapper(\\n    func: Callable[P, object],\\n    response_cls: type[_APIResponseT],\\n) -> Callable[P, ResponseContextManager[_APIResponseT]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and an `APIResponse` class\\n    and wraps the method to support streaming and returning the given response class directly.\\n\\n    Note: the given `response_cls` *must* be concrete, e.g. `class BinaryAPIResponse(APIResponse[bytes])`\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> ResponseContextManager[_APIResponseT]:\\n        extra_headers: dict[str, Any] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"stream\"\\n        extra_headers[OVERRIDE_CAST_TO_HEADER] = response_cls\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        make_request = functools.partial(func, *args, **kwargs)\\n\\n        return ResponseContextManager(cast(Callable[[], _APIResponseT], make_request))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'to_custom_streamed_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def async_to_custom_streamed_response_wrapper(\\n    func: Callable[P, Awaitable[object]],\\n    response_cls: type[_AsyncAPIResponseT],\\n) -> Callable[P, AsyncResponseContextManager[_AsyncAPIResponseT]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and an `APIResponse` class\\n    and wraps the method to support streaming and returning the given response class directly.\\n\\n    Note: the given `response_cls` *must* be concrete, e.g. `class BinaryAPIResponse(APIResponse[bytes])`\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> AsyncResponseContextManager[_AsyncAPIResponseT]:\\n        extra_headers: dict[str, Any] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"stream\"\\n        extra_headers[OVERRIDE_CAST_TO_HEADER] = response_cls\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        make_request = func(*args, **kwargs)\\n\\n        return AsyncResponseContextManager(cast(Awaitable[_AsyncAPIResponseT], make_request))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'async_to_custom_streamed_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def to_raw_response_wrapper(func: Callable[P, R]) -> Callable[P, APIResponse[R]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and wraps it\\n    to support returning the raw `APIResponse` object directly.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> APIResponse[R]:\\n        extra_headers: dict[str, str] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"raw\"\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        return cast(APIResponse[R], func(*args, **kwargs))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'to_raw_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def async_to_raw_response_wrapper(func: Callable[P, Awaitable[R]]) -> Callable[P, Awaitable[AsyncAPIResponse[R]]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and wraps it\\n    to support returning the raw `APIResponse` object directly.\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    async def wrapped(*args: P.args, **kwargs: P.kwargs) -> AsyncAPIResponse[R]:\\n        extra_headers: dict[str, str] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"raw\"\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        return cast(AsyncAPIResponse[R], await func(*args, **kwargs))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'async_to_raw_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def to_custom_raw_response_wrapper(\\n    func: Callable[P, object],\\n    response_cls: type[_APIResponseT],\\n) -> Callable[P, _APIResponseT]:\\n    \"\"\"Higher order function that takes one of our bound API methods and an `APIResponse` class\\n    and wraps the method to support returning the given response class directly.\\n\\n    Note: the given `response_cls` *must* be concrete, e.g. `class BinaryAPIResponse(APIResponse[bytes])`\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> _APIResponseT:\\n        extra_headers: dict[str, Any] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"raw\"\\n        extra_headers[OVERRIDE_CAST_TO_HEADER] = response_cls\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        return cast(_APIResponseT, func(*args, **kwargs))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'to_custom_raw_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def async_to_custom_raw_response_wrapper(\\n    func: Callable[P, Awaitable[object]],\\n    response_cls: type[_AsyncAPIResponseT],\\n) -> Callable[P, Awaitable[_AsyncAPIResponseT]]:\\n    \"\"\"Higher order function that takes one of our bound API methods and an `APIResponse` class\\n    and wraps the method to support returning the given response class directly.\\n\\n    Note: the given `response_cls` *must* be concrete, e.g. `class BinaryAPIResponse(APIResponse[bytes])`\\n    \"\"\"\\n\\n    @functools.wraps(func)\\n    def wrapped(*args: P.args, **kwargs: P.kwargs) -> Awaitable[_AsyncAPIResponseT]:\\n        extra_headers: dict[str, Any] = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\\n        extra_headers[RAW_RESPONSE_HEADER] = \"raw\"\\n        extra_headers[OVERRIDE_CAST_TO_HEADER] = response_cls\\n\\n        kwargs[\"extra_headers\"] = extra_headers\\n\\n        return cast(Awaitable[_AsyncAPIResponseT], func(*args, **kwargs))\\n\\n    return wrapped\\n\\n',\n",
       "  'function_name': 'async_to_custom_raw_response_wrapper',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def extract_response_type(typ: type[BaseAPIResponse[Any]]) -> type:\\n    \"\"\"Given a type like `APIResponse[T]`, returns the generic type variable `T`.\\n\\n    This also handles the case where a concrete subclass is given, e.g.\\n    ```py\\n    class MyResponse(APIResponse[bytes]):\\n        ...\\n\\n    extract_response_type(MyResponse) -> bytes\\n    ```\\n    \"\"\"\\n    return extract_type_var_from_base(\\n        typ,\\n        generic_bases=cast(\"tuple[type, ...]\", (BaseAPIResponse, APIResponse, AsyncAPIResponse)),\\n        index=0,\\n    )\\n',\n",
       "  'function_name': 'extract_response_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_response.py'},\n",
       " {'code': 'def is_stream_class_type(typ: type) -> TypeGuard[type[Stream[object]] | type[AsyncStream[object]]]:\\n    \"\"\"TypeGuard for determining whether or not the given type is a subclass of `Stream` / `AsyncStream`\"\"\"\\n    origin = get_origin(typ) or typ\\n    return inspect.isclass(origin) and issubclass(origin, (Stream, AsyncStream))\\n\\n',\n",
       "  'function_name': 'is_stream_class_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_streaming.py'},\n",
       " {'code': 'def extract_stream_chunk_type(\\n    stream_cls: type,\\n    *,\\n    failure_message: str | None = None,\\n) -> type:\\n    \"\"\"Given a type like `Stream[T]`, returns the generic type variable `T`.\\n\\n    This also handles the case where a concrete subclass is given, e.g.\\n    ```py\\n    class MyStream(Stream[bytes]):\\n        ...\\n\\n    extract_stream_chunk_type(MyStream) -> bytes\\n    ```\\n    \"\"\"\\n    from ._base_client import Stream, AsyncStream\\n\\n    return extract_type_var_from_base(\\n        stream_cls,\\n        index=0,\\n        generic_bases=cast(\"tuple[type, ...]\", (Stream, AsyncStream)),\\n        failure_message=failure_message,\\n    )\\n',\n",
       "  'function_name': 'extract_stream_chunk_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_streaming.py'},\n",
       " {'code': 'def has_numpy() -> bool:\\n    try:\\n        import numpy  # noqa: F401  # pyright: ignore[reportUnusedImport]\\n    except ImportError:\\n        return False\\n\\n    return True\\n',\n",
       "  'function_name': 'has_numpy',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_extras/numpy_proxy.py'},\n",
       " {'code': 'def format_instructions(*, library: str, extra: str) -> str:\\n    return INSTRUCTIONS.format(library=library, extra=extra)\\n\\n',\n",
       "  'function_name': 'format_instructions',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_extras/_common.py'},\n",
       " {'code': 'def progress(total: float, desc: str | None) -> Callable[[float], None]:\\n    import tqdm\\n\\n    meter = tqdm.tqdm(total=total, unit_scale=True, desc=desc)\\n\\n    def incr(progress: float) -> None:\\n        meter.n = progress\\n        if progress == total:\\n            meter.close()\\n        else:\\n            meter.refresh()\\n\\n    return incr\\n\\n',\n",
       "  'function_name': 'progress',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_progress.py'},\n",
       " {'code': 'def MB(i: int) -> int:\\n    return int(i // 1024**2)\\n',\n",
       "  'function_name': 'MB',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_progress.py'},\n",
       " {'code': 'def _build_parser() -> argparse.ArgumentParser:\\n    parser = argparse.ArgumentParser(description=None, prog=\"openai\")\\n    parser.add_argument(\\n        \"-v\",\\n        \"--verbose\",\\n        action=\"count\",\\n        dest=\"verbosity\",\\n        default=0,\\n        help=\"Set verbosity.\",\\n    )\\n    parser.add_argument(\"-b\", \"--api-base\", help=\"What API base url to use.\")\\n    parser.add_argument(\"-k\", \"--api-key\", help=\"What API key to use.\")\\n    parser.add_argument(\"-p\", \"--proxy\", nargs=\"+\", help=\"What proxy to use.\")\\n    parser.add_argument(\\n        \"-o\",\\n        \"--organization\",\\n        help=\"Which organization to run as (will use your default organization if not specified)\",\\n    )\\n    parser.add_argument(\\n        \"-t\",\\n        \"--api-type\",\\n        type=str,\\n        choices=(\"openai\", \"azure\"),\\n        help=\"The backend API to call, must be `openai` or `azure`\",\\n    )\\n    parser.add_argument(\\n        \"--api-version\",\\n        help=\"The Azure API version, e.g. \\'https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\\'\",\\n    )\\n\\n    # azure\\n    parser.add_argument(\\n        \"--azure-endpoint\",\\n        help=\"The Azure endpoint, e.g. \\'https://endpoint.openai.azure.com\\'\",\\n    )\\n    parser.add_argument(\\n        \"--azure-ad-token\",\\n        help=\"A token from Azure Active Directory, https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id\",\\n    )\\n\\n    # prints the package version\\n    parser.add_argument(\\n        \"-V\",\\n        \"--version\",\\n        action=\"version\",\\n        version=\"%(prog)s \" + __version__,\\n    )\\n\\n    def help() -> None:\\n        parser.print_help()\\n\\n    parser.set_defaults(func=help)\\n\\n    subparsers = parser.add_subparsers()\\n    sub_api = subparsers.add_parser(\"api\", help=\"Direct API calls\")\\n\\n    register_commands(sub_api)\\n\\n    sub_tools = subparsers.add_parser(\"tools\", help=\"Client side tools for convenience\")\\n    _tools.register_commands(sub_tools, subparsers)\\n\\n    return parser\\n\\n',\n",
       "  'function_name': '_build_parser',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_cli.py'},\n",
       " {'code': 'def main() -> int:\\n    try:\\n        _main()\\n    except (APIError, CLIError, pydantic.ValidationError) as err:\\n        display_error(err)\\n        return 1\\n    except KeyboardInterrupt:\\n        sys.stderr.write(\"\\\\n\")\\n        return 1\\n    return 0\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_cli.py'},\n",
       " {'code': 'def _parse_args(parser: argparse.ArgumentParser) -> tuple[argparse.Namespace, Arguments, list[str]]:\\n    # argparse by default will strip out the `--` but we want to keep it for unknown arguments\\n    if \"--\" in sys.argv:\\n        idx = sys.argv.index(\"--\")\\n        known_args = sys.argv[1:idx]\\n        unknown_args = sys.argv[idx:]\\n    else:\\n        known_args = sys.argv[1:]\\n        unknown_args = []\\n\\n    parsed, remaining_unknown = parser.parse_known_args(known_args)\\n\\n    # append any remaining unknown arguments from the initial parsing\\n    remaining_unknown.extend(unknown_args)\\n\\n    args = model_parse(Arguments, vars(parsed))\\n    if not args.allow_unknown_args:\\n        # we have to parse twice to ensure any unknown arguments\\n        # result in an error if that behaviour is desired\\n        parser.parse_args()\\n\\n    return parsed, args, remaining_unknown\\n\\n',\n",
       "  'function_name': '_parse_args',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_cli.py'},\n",
       " {'code': 'def _main() -> None:\\n    parser = _build_parser()\\n    parsed, args, unknown = _parse_args(parser)\\n\\n    if args.verbosity != 0:\\n        sys.stderr.write(\"Warning: --verbosity isn\\'t supported yet\\\\n\")\\n\\n    proxies: ProxiesDict = {}\\n    if args.proxy is not None:\\n        for proxy in args.proxy:\\n            key = \"https://\" if proxy.startswith(\"https\") else \"http://\"\\n            if key in proxies:\\n                raise CLIError(f\"Multiple {key} proxies given - only the last one would be used\")\\n\\n            proxies[key] = proxy\\n\\n    http_client = httpx.Client(\\n        proxies=proxies or None,\\n        http2=can_use_http2(),\\n    )\\n    openai.http_client = http_client\\n\\n    if args.organization:\\n        openai.organization = args.organization\\n\\n    if args.api_key:\\n        openai.api_key = args.api_key\\n\\n    if args.api_base:\\n        openai.base_url = args.api_base\\n\\n    # azure\\n    if args.api_type is not None:\\n        openai.api_type = args.api_type\\n\\n    if args.azure_endpoint is not None:\\n        openai.azure_endpoint = args.azure_endpoint\\n\\n    if args.api_version is not None:\\n        openai.api_version = args.api_version\\n\\n    if args.azure_ad_token is not None:\\n        openai.azure_ad_token = args.azure_ad_token\\n\\n    try:\\n        if args.args_model:\\n            parsed.func(\\n                model_parse(\\n                    args.args_model,\\n                    {\\n                        **{\\n                            # we omit None values so that they can be defaulted to `NotGiven`\\n                            # and we\\'ll strip it from the API request\\n                            key: value\\n                            for key, value in vars(parsed).items()\\n                            if value is not None\\n                        },\\n                        \"unknown_args\": unknown,\\n                    },\\n                )\\n            )\\n        else:\\n            parsed.func()\\n    finally:\\n        try:\\n            http_client.close()\\n        except Exception:\\n            pass\\n\\n',\n",
       "  'function_name': '_main',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_cli.py'},\n",
       " {'code': 'def get_client() -> OpenAI:\\n    return _load_client()\\n\\n',\n",
       "  'function_name': 'get_client',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_utils.py'},\n",
       " {'code': 'def organization_info() -> str:\\n    organization = openai.organization\\n    if organization is not None:\\n        return \"[organization={}] \".format(organization)\\n\\n    return \"\"\\n\\n',\n",
       "  'function_name': 'organization_info',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_utils.py'},\n",
       " {'code': 'def print_model(model: BaseModel) -> None:\\n    sys.stdout.write(model_json(model, indent=2) + \"\\\\n\")\\n\\n',\n",
       "  'function_name': 'print_model',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_utils.py'},\n",
       " {'code': 'def can_use_http2() -> bool:\\n    try:\\n        import h2  # type: ignore  # noqa\\n    except ImportError:\\n        return False\\n\\n    return True\\n',\n",
       "  'function_name': 'can_use_http2',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_utils.py'},\n",
       " {'code': 'def display_error(err: CLIError | APIError | pydantic.ValidationError) -> None:\\n    if isinstance(err, SilentCLIError):\\n        return\\n\\n    sys.stderr.write(\"{}{}Error:{} {}\\\\n\".format(organization_info(), Colors.FAIL, Colors.ENDC, err))\\n',\n",
       "  'function_name': 'display_error',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_errors.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"fine_tunes.prepare_data\")\\n    sub.add_argument(\\n        \"-f\",\\n        \"--file\",\\n        required=True,\\n        help=\"JSONL, JSON, CSV, TSV, TXT or XLSX file containing prompt-completion examples to be analyzed.\"\\n        \"This should be the local file path.\",\\n    )\\n    sub.add_argument(\\n        \"-q\",\\n        \"--quiet\",\\n        required=False,\\n        action=\"store_true\",\\n        help=\"Auto accepts all suggestions, without asking for user input. To be used within scripts.\",\\n    )\\n    sub.set_defaults(func=prepare_data, args_model=PrepareDataArgs)\\n\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/fine_tunes.py'},\n",
       " {'code': 'def prepare_data(args: PrepareDataArgs) -> None:\\n    sys.stdout.write(\"Analyzing...\\\\n\")\\n    fname = args.file\\n    auto_accept = args.quiet\\n    df, remediation = read_any_format(fname)\\n    apply_necessary_remediation(None, remediation)\\n\\n    validators = get_validators()\\n\\n    assert df is not None\\n\\n    apply_validators(\\n        df,\\n        fname,\\n        remediation,\\n        validators,\\n        auto_accept,\\n        write_out_file_func=write_out_file,\\n    )\\n',\n",
       "  'function_name': 'prepare_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/fine_tunes.py'},\n",
       " {'code': 'def register_commands(parser: ArgumentParser, subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    migrate.register(subparser)\\n\\n    namespaced = parser.add_subparsers(title=\"Tools\", help=\"Convenience client side tools\")\\n\\n    fine_tunes.register(namespaced)\\n',\n",
       "  'function_name': 'register_commands',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/_main.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"migrate\")\\n    sub.set_defaults(func=migrate, args_model=MigrateArgs, allow_unknown_args=True)\\n\\n    sub = subparser.add_parser(\"grit\")\\n    sub.set_defaults(func=grit, args_model=GritArgs, allow_unknown_args=True)\\n\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def grit(args: GritArgs) -> None:\\n    grit_path = install()\\n\\n    try:\\n        subprocess.check_call([grit_path, *args.unknown_args])\\n    except subprocess.CalledProcessError:\\n        # stdout and stderr are forwarded by subprocess so an error will already\\n        # have been displayed\\n        raise SilentCLIError() from None\\n\\n',\n",
       "  'function_name': 'grit',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def migrate(args: MigrateArgs) -> None:\\n    grit_path = install()\\n\\n    try:\\n        subprocess.check_call([grit_path, \"apply\", \"openai\", *args.unknown_args])\\n    except subprocess.CalledProcessError:\\n        # stdout and stderr are forwarded by subprocess so an error will already\\n        # have been displayed\\n        raise SilentCLIError() from None\\n\\n',\n",
       "  'function_name': 'migrate',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def _cache_dir() -> Path:\\n    xdg = os.environ.get(\"XDG_CACHE_HOME\")\\n    if xdg is not None:\\n        return Path(xdg)\\n\\n    return Path.home() / \".cache\"\\n\\n',\n",
       "  'function_name': '_cache_dir',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def _debug(message: str) -> None:\\n    if not os.environ.get(\"DEBUG\"):\\n        return\\n\\n    sys.stdout.write(f\"[DEBUG]: {message}\\\\n\")\\n\\n',\n",
       "  'function_name': '_debug',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def install() -> Path:\\n    \"\"\"Installs the Grit CLI and returns the location of the binary\"\"\"\\n    if sys.platform == \"win32\":\\n        raise CLIError(\"Windows is not supported yet in the migration CLI\")\\n\\n    platform = \"macos\" if sys.platform == \"darwin\" else \"linux\"\\n\\n    dir_name = _cache_dir() / \"openai-python\"\\n    install_dir = dir_name / \".install\"\\n    target_dir = install_dir / \"bin\"\\n\\n    target_path = target_dir / \"marzano\"\\n    temp_file = target_dir / \"marzano.tmp\"\\n\\n    if target_path.exists():\\n        _debug(f\"{target_path} already exists\")\\n        sys.stdout.flush()\\n        return target_path\\n\\n    _debug(f\"Using Grit CLI path: {target_path}\")\\n\\n    target_dir.mkdir(parents=True, exist_ok=True)\\n\\n    if temp_file.exists():\\n        temp_file.unlink()\\n\\n    arch = _get_arch()\\n    _debug(f\"Using architecture {arch}\")\\n\\n    file_name = f\"marzano-{platform}-{arch}\"\\n    meta_url = f\"https://api.keygen.sh/v1/accounts/{KEYGEN_ACCOUNT}/artifacts/{file_name}\"\\n\\n    sys.stdout.write(f\"Retrieving Grit CLI metadata from {meta_url}\\\\n\")\\n    with httpx.Client() as client:\\n        response = client.get(meta_url)  # pyright: ignore[reportUnknownMemberType]\\n\\n        data = response.json()\\n        errors = data.get(\"errors\")\\n        if errors:\\n            for error in errors:\\n                sys.stdout.write(f\"{error}\\\\n\")\\n\\n            raise CLIError(\"Could not locate Grit CLI binary - see above errors\")\\n\\n        write_manifest(install_dir, data[\"data\"][\"relationships\"][\"release\"][\"data\"][\"id\"])\\n\\n        link = data[\"data\"][\"links\"][\"redirect\"]\\n        _debug(f\"Redirect URL {link}\")\\n\\n        download_response = client.get(link)  # pyright: ignore[reportUnknownMemberType]\\n        with open(temp_file, \"wb\") as file:\\n            for chunk in download_response.iter_bytes():\\n                file.write(chunk)\\n\\n    unpacked_dir = target_dir / \"cli-bin\"\\n    unpacked_dir.mkdir(parents=True, exist_ok=True)\\n\\n    with tarfile.open(temp_file, \"r:gz\") as archive:\\n        archive.extractall(unpacked_dir, filter=\"data\")\\n\\n    for item in unpacked_dir.iterdir():\\n        item.rename(target_dir / item.name)\\n\\n    shutil.rmtree(unpacked_dir)\\n    os.remove(temp_file)\\n    os.chmod(target_path, 0o755)\\n\\n    sys.stdout.flush()\\n\\n    return target_path\\n\\n',\n",
       "  'function_name': 'install',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def _get_arch() -> str:\\n    architecture = platform.machine().lower()\\n\\n    # Map the architecture names to Node.js equivalents\\n    arch_map = {\\n        \"x86_64\": \"x64\",\\n        \"amd64\": \"x64\",\\n        \"armv7l\": \"arm\",\\n        \"aarch64\": \"arm64\",\\n    }\\n\\n    return arch_map.get(architecture, architecture)\\n\\n',\n",
       "  'function_name': '_get_arch',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def write_manifest(install_path: Path, release: str) -> None:\\n    manifest = {\\n        \"installPath\": str(install_path),\\n        \"binaries\": {\\n            \"marzano\": {\\n                \"name\": \"marzano\",\\n                \"release\": release,\\n            },\\n        },\\n    }\\n    manifest_path = Path(install_path) / \"manifests.json\"\\n    with open(manifest_path, \"w\") as f:\\n        json.dump(manifest, f, indent=2)\\n',\n",
       "  'function_name': 'write_manifest',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_tools/migrate.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"files.create\")\\n\\n    sub.add_argument(\\n        \"-f\",\\n        \"--file\",\\n        required=True,\\n        help=\"File to upload\",\\n    )\\n    sub.add_argument(\\n        \"-p\",\\n        \"--purpose\",\\n        help=\"Why are you uploading this file? (see https://platform.openai.com/docs/api-reference/ for purposes)\",\\n        required=True,\\n    )\\n    sub.set_defaults(func=CLIFile.create, args_model=CLIFileCreateArgs)\\n\\n    sub = subparser.add_parser(\"files.retrieve\")\\n    sub.add_argument(\"-i\", \"--id\", required=True, help=\"The files ID\")\\n    sub.set_defaults(func=CLIFile.get, args_model=CLIFileCreateArgs)\\n\\n    sub = subparser.add_parser(\"files.delete\")\\n    sub.add_argument(\"-i\", \"--id\", required=True, help=\"The files ID\")\\n    sub.set_defaults(func=CLIFile.delete, args_model=CLIFileCreateArgs)\\n\\n    sub = subparser.add_parser(\"files.list\")\\n    sub.set_defaults(func=CLIFile.list)\\n\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/files.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"images.generate\")\\n    sub.add_argument(\"-m\", \"--model\", type=str)\\n    sub.add_argument(\"-p\", \"--prompt\", type=str, required=True)\\n    sub.add_argument(\"-n\", \"--num-images\", type=int, default=1)\\n    sub.add_argument(\"-s\", \"--size\", type=str, default=\"1024x1024\", help=\"Size of the output image\")\\n    sub.add_argument(\"--response-format\", type=str, default=\"url\")\\n    sub.set_defaults(func=CLIImage.create, args_model=CLIImageCreateArgs)\\n\\n    sub = subparser.add_parser(\"images.edit\")\\n    sub.add_argument(\"-m\", \"--model\", type=str)\\n    sub.add_argument(\"-p\", \"--prompt\", type=str, required=True)\\n    sub.add_argument(\"-n\", \"--num-images\", type=int, default=1)\\n    sub.add_argument(\\n        \"-I\",\\n        \"--image\",\\n        type=str,\\n        required=True,\\n        help=\"Image to modify. Should be a local path and a PNG encoded image.\",\\n    )\\n    sub.add_argument(\"-s\", \"--size\", type=str, default=\"1024x1024\", help=\"Size of the output image\")\\n    sub.add_argument(\"--response-format\", type=str, default=\"url\")\\n    sub.add_argument(\\n        \"-M\",\\n        \"--mask\",\\n        type=str,\\n        required=False,\\n        help=\"Path to a mask image. It should be the same size as the image you\\'re editing and a RGBA PNG image. The Alpha channel acts as the mask.\",\\n    )\\n    sub.set_defaults(func=CLIImage.edit, args_model=CLIImageEditArgs)\\n\\n    sub = subparser.add_parser(\"images.create_variation\")\\n    sub.add_argument(\"-m\", \"--model\", type=str)\\n    sub.add_argument(\"-n\", \"--num-images\", type=int, default=1)\\n    sub.add_argument(\\n        \"-I\",\\n        \"--image\",\\n        type=str,\\n        required=True,\\n        help=\"Image to modify. Should be a local path and a PNG encoded image.\",\\n    )\\n    sub.add_argument(\"-s\", \"--size\", type=str, default=\"1024x1024\", help=\"Size of the output image\")\\n    sub.add_argument(\"--response-format\", type=str, default=\"url\")\\n    sub.set_defaults(func=CLIImage.create_variation, args_model=CLIImageCreateVariationArgs)\\n\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/image.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"models.list\")\\n    sub.set_defaults(func=CLIModels.list)\\n\\n    sub = subparser.add_parser(\"models.retrieve\")\\n    sub.add_argument(\"-i\", \"--id\", required=True, help=\"The model ID\")\\n    sub.set_defaults(func=CLIModels.get, args_model=CLIModelIDArgs)\\n\\n    sub = subparser.add_parser(\"models.delete\")\\n    sub.add_argument(\"-i\", \"--id\", required=True, help=\"The model ID\")\\n    sub.set_defaults(func=CLIModels.delete, args_model=CLIModelIDArgs)\\n\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/models.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"completions.create\")\\n\\n    # Required\\n    sub.add_argument(\\n        \"-m\",\\n        \"--model\",\\n        help=\"The model to use\",\\n        required=True,\\n    )\\n\\n    # Optional\\n    sub.add_argument(\"-p\", \"--prompt\", help=\"An optional prompt to complete from\")\\n    sub.add_argument(\"--stream\", help=\"Stream tokens as they\\'re ready.\", action=\"store_true\")\\n    sub.add_argument(\"-M\", \"--max-tokens\", help=\"The maximum number of tokens to generate\", type=int)\\n    sub.add_argument(\\n        \"-t\",\\n        \"--temperature\",\\n        help=\"\"\"What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/completions.py'},\n",
       " {'code': 'def register_commands(parser: ArgumentParser) -> None:\\n    subparsers = parser.add_subparsers(help=\"All API subcommands\")\\n\\n    chat.register(subparsers)\\n    image.register(subparsers)\\n    audio.register(subparsers)\\n    files.register(subparsers)\\n    models.register(subparsers)\\n    completions.register(subparsers)\\n',\n",
       "  'function_name': 'register_commands',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/_main.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    # transcriptions\\n    sub = subparser.add_parser(\"audio.transcriptions.create\")\\n\\n    # Required\\n    sub.add_argument(\"-m\", \"--model\", type=str, default=\"whisper-1\")\\n    sub.add_argument(\"-f\", \"--file\", type=str, required=True)\\n    # Optional\\n    sub.add_argument(\"--response-format\", type=str)\\n    sub.add_argument(\"--language\", type=str)\\n    sub.add_argument(\"-t\", \"--temperature\", type=float)\\n    sub.add_argument(\"--prompt\", type=str)\\n    sub.set_defaults(func=CLIAudio.transcribe, args_model=CLITranscribeArgs)\\n\\n    # translations\\n    sub = subparser.add_parser(\"audio.translations.create\")\\n\\n    # Required\\n    sub.add_argument(\"-f\", \"--file\", type=str, required=True)\\n    # Optional\\n    sub.add_argument(\"-m\", \"--model\", type=str, default=\"whisper-1\")\\n    sub.add_argument(\"--response-format\", type=str)\\n    # TODO: doesn\\'t seem to be supported by the API\\n    # sub.add_argument(\"--language\", type=str)\\n    sub.add_argument(\"-t\", \"--temperature\", type=float)\\n    sub.add_argument(\"--prompt\", type=str)\\n    sub.set_defaults(func=CLIAudio.translate, args_model=CLITranslationArgs)\\n\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/audio.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    completions.register(subparser)\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/chat/__init__.py'},\n",
       " {'code': 'def register(subparser: _SubParsersAction[ArgumentParser]) -> None:\\n    sub = subparser.add_parser(\"chat.completions.create\")\\n\\n    sub._action_groups.pop()\\n    req = sub.add_argument_group(\"required arguments\")\\n    opt = sub.add_argument_group(\"optional arguments\")\\n\\n    req.add_argument(\\n        \"-g\",\\n        \"--message\",\\n        action=\"append\",\\n        nargs=2,\\n        metavar=(\"ROLE\", \"CONTENT\"),\\n        help=\"A message in `{role} {content}` format. Use this argument multiple times to add multiple messages.\",\\n        required=True,\\n    )\\n    req.add_argument(\\n        \"-m\",\\n        \"--model\",\\n        help=\"The model to use.\",\\n        required=True,\\n    )\\n\\n    opt.add_argument(\\n        \"-n\",\\n        \"--n\",\\n        help=\"How many completions to generate for the conversation.\",\\n        type=int,\\n    )\\n    opt.add_argument(\"-M\", \"--max-tokens\", help=\"The maximum number of tokens to generate.\", type=int)\\n    opt.add_argument(\\n        \"-t\",\\n        \"--temperature\",\\n        help=\"\"\"What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\\n',\n",
       "  'function_name': 'register',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/cli/_api/chat/completions.py'},\n",
       " {'code': 'def num_examples_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will only print out the number of examples and recommend to the user to increase the number of examples if less than 100.\\n    \"\"\"\\n    MIN_EXAMPLES = 100\\n    optional_suggestion = (\\n        \"\"\\n        if len(df) >= MIN_EXAMPLES\\n        else \". In general, we recommend having at least a few hundred examples. We\\'ve found that performance tends to linearly increase for every doubling of the number of examples\"\\n    )\\n    immediate_msg = f\"\\\\n- Your file contains {len(df)} prompt-completion pairs{optional_suggestion}\"\\n    return Remediation(name=\"num_examples\", immediate_msg=immediate_msg)\\n\\n',\n",
       "  'function_name': 'num_examples_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def necessary_column_validator(df: pd.DataFrame, necessary_column: str) -> Remediation:\\n    \"\"\"\\n    This validator will ensure that the necessary column is present in the dataframe.\\n    \"\"\"\\n\\n    def lower_case_column(df: pd.DataFrame, column: Any) -> pd.DataFrame:\\n        cols = [c for c in df.columns if str(c).lower() == column]\\n        df.rename(columns={cols[0]: column.lower()}, inplace=True)\\n        return df\\n\\n    immediate_msg = None\\n    necessary_fn = None\\n    necessary_msg = None\\n    error_msg = None\\n\\n    if necessary_column not in df.columns:\\n        if necessary_column in [str(c).lower() for c in df.columns]:\\n\\n            def lower_case_column_creator(df: pd.DataFrame) -> pd.DataFrame:\\n                return lower_case_column(df, necessary_column)\\n\\n            necessary_fn = lower_case_column_creator\\n            immediate_msg = f\"\\\\n- The `{necessary_column}` column/key should be lowercase\"\\n            necessary_msg = f\"Lower case column name to `{necessary_column}`\"\\n        else:\\n            error_msg = f\"`{necessary_column}` column/key is missing. Please make sure you name your columns/keys appropriately, then retry\"\\n\\n    return Remediation(\\n        name=\"necessary_column\",\\n        immediate_msg=immediate_msg,\\n        necessary_msg=necessary_msg,\\n        necessary_fn=necessary_fn,\\n        error_msg=error_msg,\\n    )\\n\\n',\n",
       "  'function_name': 'necessary_column_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def additional_column_validator(df: pd.DataFrame, fields: list[str] = [\"prompt\", \"completion\"]) -> Remediation:\\n    \"\"\"\\n    This validator will remove additional columns from the dataframe.\\n    \"\"\"\\n    additional_columns = []\\n    necessary_msg = None\\n    immediate_msg = None\\n    necessary_fn = None  # type: ignore\\n\\n    if len(df.columns) > 2:\\n        additional_columns = [c for c in df.columns if c not in fields]\\n        warn_message = \"\"\\n        for ac in additional_columns:\\n            dups = [c for c in additional_columns if ac in c]\\n            if len(dups) > 0:\\n                warn_message += f\"\\\\n  WARNING: Some of the additional columns/keys contain `{ac}` in their name. These will be ignored, and the column/key `{ac}` will be used instead. This could also result from a duplicate column/key in the provided file.\"\\n        immediate_msg = f\"\\\\n- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: {additional_columns}{warn_message}\"\\n        necessary_msg = f\"Remove additional columns/keys: {additional_columns}\"\\n\\n        def necessary_fn(x: Any) -> Any:\\n            return x[fields]\\n\\n    return Remediation(\\n        name=\"additional_column\",\\n        immediate_msg=immediate_msg,\\n        necessary_msg=necessary_msg,\\n        necessary_fn=necessary_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'additional_column_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def non_empty_field_validator(df: pd.DataFrame, field: str = \"completion\") -> Remediation:\\n    \"\"\"\\n    This validator will ensure that no completion is empty.\\n    \"\"\"\\n    necessary_msg = None\\n    necessary_fn = None  # type: ignore\\n    immediate_msg = None\\n\\n    if df[field].apply(lambda x: x == \"\").any() or df[field].isnull().any():\\n        empty_rows = (df[field] == \"\") | (df[field].isnull())\\n        empty_indexes = df.reset_index().index[empty_rows].tolist()\\n        immediate_msg = f\"\\\\n- `{field}` column/key should not contain empty strings. These are rows: {empty_indexes}\"\\n\\n        def necessary_fn(x: Any) -> Any:\\n            return x[x[field] != \"\"].dropna(subset=[field])\\n\\n        necessary_msg = f\"Remove {len(empty_indexes)} rows with empty {field}s\"\\n\\n    return Remediation(\\n        name=f\"empty_{field}\",\\n        immediate_msg=immediate_msg,\\n        necessary_msg=necessary_msg,\\n        necessary_fn=necessary_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'non_empty_field_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def duplicated_rows_validator(df: pd.DataFrame, fields: list[str] = [\"prompt\", \"completion\"]) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to the user to remove duplicate rows if they exist.\\n    \"\"\"\\n    duplicated_rows = df.duplicated(subset=fields)\\n    duplicated_indexes = df.reset_index().index[duplicated_rows].tolist()\\n    immediate_msg = None\\n    optional_msg = None\\n    optional_fn = None  # type: ignore\\n\\n    if len(duplicated_indexes) > 0:\\n        immediate_msg = f\"\\\\n- There are {len(duplicated_indexes)} duplicated {\\'-\\'.join(fields)} sets. These are rows: {duplicated_indexes}\"\\n        optional_msg = f\"Remove {len(duplicated_indexes)} duplicate rows\"\\n\\n        def optional_fn(x: Any) -> Any:\\n            return x.drop_duplicates(subset=fields)\\n\\n    return Remediation(\\n        name=\"duplicated_rows\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'duplicated_rows_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def long_examples_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to the user to remove examples that are too long.\\n    \"\"\"\\n    immediate_msg = None\\n    optional_msg = None\\n    optional_fn = None  # type: ignore\\n\\n    ft_type = infer_task_type(df)\\n    if ft_type != \"open-ended generation\":\\n\\n        def get_long_indexes(d: pd.DataFrame) -> Any:\\n            long_examples = d.apply(lambda x: len(x.prompt) + len(x.completion) > 10000, axis=1)\\n            return d.reset_index().index[long_examples].tolist()\\n\\n        long_indexes = get_long_indexes(df)\\n\\n        if len(long_indexes) > 0:\\n            immediate_msg = f\"\\\\n- There are {len(long_indexes)} examples that are very long. These are rows: {long_indexes}\\\\nFor conditional generation, and for classification the examples shouldn\\'t be longer than 2048 tokens.\"\\n            optional_msg = f\"Remove {len(long_indexes)} long examples\"\\n\\n            def optional_fn(x: Any) -> Any:\\n                long_indexes_to_drop = get_long_indexes(x)\\n                if long_indexes != long_indexes_to_drop:\\n                    sys.stdout.write(\\n                        f\"The indices of the long examples has changed as a result of a previously applied recommendation.\\\\nThe {len(long_indexes_to_drop)} long examples to be dropped are now at the following indices: {long_indexes_to_drop}\\\\n\"\\n                    )\\n                return x.drop(long_indexes_to_drop)\\n\\n    return Remediation(\\n        name=\"long_examples\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'long_examples_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def common_prompt_suffix_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to add a common suffix to the prompt if one doesn\\'t already exist in case of classification or conditional generation.\\n    \"\"\"\\n    error_msg = None\\n    immediate_msg = None\\n    optional_msg = None\\n    optional_fn = None  # type: ignore\\n\\n    # Find a suffix which is not contained within the prompt otherwise\\n    suggested_suffix = \"\\\\n\\\\n### =>\\\\n\\\\n\"\\n    suffix_options = [\\n        \" ->\",\\n        \"\\\\n\\\\n###\\\\n\\\\n\",\\n        \"\\\\n\\\\n===\\\\n\\\\n\",\\n        \"\\\\n\\\\n---\\\\n\\\\n\",\\n        \"\\\\n\\\\n===>\\\\n\\\\n\",\\n        \"\\\\n\\\\n--->\\\\n\\\\n\",\\n    ]\\n    for suffix_option in suffix_options:\\n        if suffix_option == \" ->\":\\n            if df.prompt.str.contains(\"\\\\n\").any():\\n                continue\\n        if df.prompt.str.contains(suffix_option, regex=False).any():\\n            continue\\n        suggested_suffix = suffix_option\\n        break\\n    display_suggested_suffix = suggested_suffix.replace(\"\\\\n\", \"\\\\\\\\n\")\\n\\n    ft_type = infer_task_type(df)\\n    if ft_type == \"open-ended generation\":\\n        return Remediation(name=\"common_suffix\")\\n\\n    def add_suffix(x: Any, suffix: Any) -> Any:\\n        x[\"prompt\"] += suffix\\n        return x\\n\\n    common_suffix = get_common_xfix(df.prompt, xfix=\"suffix\")\\n    if (df.prompt == common_suffix).all():\\n        error_msg = f\"All prompts are identical: `{common_suffix}`\\\\nConsider leaving the prompts blank if you want to do open-ended generation, otherwise ensure prompts are different\"\\n        return Remediation(name=\"common_suffix\", error_msg=error_msg)\\n\\n    if common_suffix != \"\":\\n        common_suffix_new_line_handled = common_suffix.replace(\"\\\\n\", \"\\\\\\\\n\")\\n        immediate_msg = f\"\\\\n- All prompts end with suffix `{common_suffix_new_line_handled}`\"\\n        if len(common_suffix) > 10:\\n            immediate_msg += f\". This suffix seems very long. Consider replacing with a shorter suffix, such as `{display_suggested_suffix}`\"\\n        if df.prompt.str[: -len(common_suffix)].str.contains(common_suffix, regex=False).any():\\n            immediate_msg += f\"\\\\n  WARNING: Some of your prompts contain the suffix `{common_suffix}` more than once. We strongly suggest that you review your prompts and add a unique suffix\"\\n\\n    else:\\n        immediate_msg = \"\\\\n- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\"\\n\\n    if common_suffix == \"\":\\n        optional_msg = f\"Add a suffix separator `{display_suggested_suffix}` to all prompts\"\\n\\n        def optional_fn(x: Any) -> Any:\\n            return add_suffix(x, suggested_suffix)\\n\\n    return Remediation(\\n        name=\"common_completion_suffix\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n        error_msg=error_msg,\\n    )\\n\\n',\n",
       "  'function_name': 'common_prompt_suffix_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def common_prompt_prefix_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to remove a common prefix from the prompt if a long one exist.\\n    \"\"\"\\n    MAX_PREFIX_LEN = 12\\n\\n    immediate_msg = None\\n    optional_msg = None\\n    optional_fn = None  # type: ignore\\n\\n    common_prefix = get_common_xfix(df.prompt, xfix=\"prefix\")\\n    if common_prefix == \"\":\\n        return Remediation(name=\"common_prefix\")\\n\\n    def remove_common_prefix(x: Any, prefix: Any) -> Any:\\n        x[\"prompt\"] = x[\"prompt\"].str[len(prefix) :]\\n        return x\\n\\n    if (df.prompt == common_prefix).all():\\n        # already handled by common_suffix_validator\\n        return Remediation(name=\"common_prefix\")\\n\\n    if common_prefix != \"\":\\n        immediate_msg = f\"\\\\n- All prompts start with prefix `{common_prefix}`\"\\n        if MAX_PREFIX_LEN < len(common_prefix):\\n            immediate_msg += \". Fine-tuning doesn\\'t require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion\"\\n            optional_msg = f\"Remove prefix `{common_prefix}` from all prompts\"\\n\\n            def optional_fn(x: Any) -> Any:\\n                return remove_common_prefix(x, common_prefix)\\n\\n    return Remediation(\\n        name=\"common_prompt_prefix\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'common_prompt_prefix_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def common_completion_prefix_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to remove a common prefix from the completion if a long one exist.\\n    \"\"\"\\n    MAX_PREFIX_LEN = 5\\n\\n    common_prefix = get_common_xfix(df.completion, xfix=\"prefix\")\\n    ws_prefix = len(common_prefix) > 0 and common_prefix[0] == \" \"\\n    if len(common_prefix) < MAX_PREFIX_LEN:\\n        return Remediation(name=\"common_prefix\")\\n\\n    def remove_common_prefix(x: Any, prefix: Any, ws_prefix: Any) -> Any:\\n        x[\"completion\"] = x[\"completion\"].str[len(prefix) :]\\n        if ws_prefix:\\n            # keep the single whitespace as prefix\\n            x[\"completion\"] = f\" {x[\\'completion\\']}\"\\n        return x\\n\\n    if (df.completion == common_prefix).all():\\n        # already handled by common_suffix_validator\\n        return Remediation(name=\"common_prefix\")\\n\\n    immediate_msg = f\"\\\\n- All completions start with prefix `{common_prefix}`. Most of the time you should only add the output data into the completion, without any prefix\"\\n    optional_msg = f\"Remove prefix `{common_prefix}` from all completions\"\\n\\n    def optional_fn(x: Any) -> Any:\\n        return remove_common_prefix(x, common_prefix, ws_prefix)\\n\\n    return Remediation(\\n        name=\"common_completion_prefix\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'common_completion_prefix_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def common_completion_suffix_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to add a common suffix to the completion if one doesn\\'t already exist in case of classification or conditional generation.\\n    \"\"\"\\n    error_msg = None\\n    immediate_msg = None\\n    optional_msg = None\\n    optional_fn = None  # type: ignore\\n\\n    ft_type = infer_task_type(df)\\n    if ft_type == \"open-ended generation\" or ft_type == \"classification\":\\n        return Remediation(name=\"common_suffix\")\\n\\n    common_suffix = get_common_xfix(df.completion, xfix=\"suffix\")\\n    if (df.completion == common_suffix).all():\\n        error_msg = f\"All completions are identical: `{common_suffix}`\\\\nEnsure completions are different, otherwise the model will just repeat `{common_suffix}`\"\\n        return Remediation(name=\"common_suffix\", error_msg=error_msg)\\n\\n    # Find a suffix which is not contained within the completion otherwise\\n    suggested_suffix = \" [END]\"\\n    suffix_options = [\\n        \"\\\\n\",\\n        \".\",\\n        \" END\",\\n        \"***\",\\n        \"+++\",\\n        \"&&&\",\\n        \"$$$\",\\n        \"@@@\",\\n        \"%%%\",\\n    ]\\n    for suffix_option in suffix_options:\\n        if df.completion.str.contains(suffix_option, regex=False).any():\\n            continue\\n        suggested_suffix = suffix_option\\n        break\\n    display_suggested_suffix = suggested_suffix.replace(\"\\\\n\", \"\\\\\\\\n\")\\n\\n    def add_suffix(x: Any, suffix: Any) -> Any:\\n        x[\"completion\"] += suffix\\n        return x\\n\\n    if common_suffix != \"\":\\n        common_suffix_new_line_handled = common_suffix.replace(\"\\\\n\", \"\\\\\\\\n\")\\n        immediate_msg = f\"\\\\n- All completions end with suffix `{common_suffix_new_line_handled}`\"\\n        if len(common_suffix) > 10:\\n            immediate_msg += f\". This suffix seems very long. Consider replacing with a shorter suffix, such as `{display_suggested_suffix}`\"\\n        if df.completion.str[: -len(common_suffix)].str.contains(common_suffix, regex=False).any():\\n            immediate_msg += f\"\\\\n  WARNING: Some of your completions contain the suffix `{common_suffix}` more than once. We suggest that you review your completions and add a unique ending\"\\n\\n    else:\\n        immediate_msg = \"\\\\n- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\"\\n\\n    if common_suffix == \"\":\\n        optional_msg = f\"Add a suffix ending `{display_suggested_suffix}` to all completions\"\\n\\n        def optional_fn(x: Any) -> Any:\\n            return add_suffix(x, suggested_suffix)\\n\\n    return Remediation(\\n        name=\"common_completion_suffix\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n        error_msg=error_msg,\\n    )\\n\\n',\n",
       "  'function_name': 'common_completion_suffix_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def completions_space_start_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will suggest to add a space at the start of the completion if it doesn\\'t already exist. This helps with tokenization.\\n    \"\"\"\\n\\n    def add_space_start(x: Any) -> Any:\\n        x[\"completion\"] = x[\"completion\"].apply(lambda s: (\"\" if s.startswith(\" \") else \" \") + s)\\n        return x\\n\\n    optional_msg = None\\n    optional_fn = None\\n    immediate_msg = None\\n\\n    if df.completion.str[:1].nunique() != 1 or df.completion.values[0][0] != \" \":\\n        immediate_msg = \"\\\\n- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\"\\n        optional_msg = \"Add a whitespace character to the beginning of the completion\"\\n        optional_fn = add_space_start\\n    return Remediation(\\n        name=\"completion_space_start\",\\n        immediate_msg=immediate_msg,\\n        optional_msg=optional_msg,\\n        optional_fn=optional_fn,\\n    )\\n\\n',\n",
       "  'function_name': 'completions_space_start_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def lower_case_validator(df: pd.DataFrame, column: Any) -> Remediation | None:\\n    \"\"\"\\n    This validator will suggest to lowercase the column values, if more than a third of letters are uppercase.\\n    \"\"\"\\n\\n    def lower_case(x: Any) -> Any:\\n        x[column] = x[column].str.lower()\\n        return x\\n\\n    count_upper = df[column].apply(lambda x: sum(1 for c in x if c.isalpha() and c.isupper())).sum()\\n    count_lower = df[column].apply(lambda x: sum(1 for c in x if c.isalpha() and c.islower())).sum()\\n\\n    if count_upper * 2 > count_lower:\\n        return Remediation(\\n            name=\"lower_case\",\\n            immediate_msg=f\"\\\\n- More than a third of your `{column}` column/key is uppercase. Uppercase {column}s tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\",\\n            optional_msg=f\"Lowercase all your data in column/key `{column}`\",\\n            optional_fn=lower_case,\\n        )\\n    return None\\n\\n',\n",
       "  'function_name': 'lower_case_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def read_any_format(\\n    fname: str, fields: list[str] = [\"prompt\", \"completion\"]\\n) -> tuple[pd.DataFrame | None, Remediation]:\\n    \"\"\"\\n    This function will read a file saved in .csv, .json, .txt, .xlsx or .tsv format using pandas.\\n     - for .xlsx it will read the first sheet\\n     - for .txt it will assume completions and split on newline\\n    \"\"\"\\n    remediation = None\\n    necessary_msg = None\\n    immediate_msg = None\\n    error_msg = None\\n    df = None\\n\\n    if os.path.isfile(fname):\\n        try:\\n            if fname.lower().endswith(\".csv\") or fname.lower().endswith(\".tsv\"):\\n                file_extension_str, separator = (\"CSV\", \",\") if fname.lower().endswith(\".csv\") else (\"TSV\", \"\\\\t\")\\n                immediate_msg = (\\n                    f\"\\\\n- Based on your file extension, your file is formatted as a {file_extension_str} file\"\\n                )\\n                necessary_msg = f\"Your format `{file_extension_str}` will be converted to `JSONL`\"\\n                df = pd.read_csv(fname, sep=separator, dtype=str).fillna(\"\")\\n            elif fname.lower().endswith(\".xlsx\"):\\n                immediate_msg = \"\\\\n- Based on your file extension, your file is formatted as an Excel file\"\\n                necessary_msg = \"Your format `XLSX` will be converted to `JSONL`\"\\n                xls = pd.ExcelFile(fname)\\n                sheets = xls.sheet_names\\n                if len(sheets) > 1:\\n                    immediate_msg += \"\\\\n- Your Excel file contains more than one sheet. Please either save as csv or ensure all data is present in the first sheet. WARNING: Reading only the first sheet...\"\\n                df = pd.read_excel(fname, dtype=str).fillna(\"\")\\n            elif fname.lower().endswith(\".txt\"):\\n                immediate_msg = \"\\\\n- Based on your file extension, you provided a text file\"\\n                necessary_msg = \"Your format `TXT` will be converted to `JSONL`\"\\n                with open(fname, \"r\") as f:\\n                    content = f.read()\\n                    df = pd.DataFrame(\\n                        [[\"\", line] for line in content.split(\"\\\\n\")],\\n                        columns=fields,\\n                        dtype=str,\\n                    ).fillna(\"\")\\n            elif fname.lower().endswith(\".jsonl\"):\\n                df = pd.read_json(fname, lines=True, dtype=str).fillna(\"\")  # type: ignore\\n                if len(df) == 1:  # type: ignore\\n                    # this is NOT what we expect for a .jsonl file\\n                    immediate_msg = \"\\\\n- Your JSONL file appears to be in a JSON format. Your file will be converted to JSONL format\"\\n                    necessary_msg = \"Your format `JSON` will be converted to `JSONL`\"\\n                    df = pd.read_json(fname, dtype=str).fillna(\"\")  # type: ignore\\n                else:\\n                    pass  # this is what we expect for a .jsonl file\\n            elif fname.lower().endswith(\".json\"):\\n                try:\\n                    # to handle case where .json file is actually a .jsonl file\\n                    df = pd.read_json(fname, lines=True, dtype=str).fillna(\"\")  # type: ignore\\n                    if len(df) == 1:  # type: ignore\\n                        # this code path corresponds to a .json file that has one line\\n                        df = pd.read_json(fname, dtype=str).fillna(\"\")  # type: ignore\\n                    else:\\n                        # this is NOT what we expect for a .json file\\n                        immediate_msg = \"\\\\n- Your JSON file appears to be in a JSONL format. Your file will be converted to JSONL format\"\\n                        necessary_msg = \"Your format `JSON` will be converted to `JSONL`\"\\n                except ValueError:\\n                    # this code path corresponds to a .json file that has multiple lines (i.e. it is indented)\\n                    df = pd.read_json(fname, dtype=str).fillna(\"\")  # type: ignore\\n            else:\\n                error_msg = (\\n                    \"Your file must have one of the following extensions: .CSV, .TSV, .XLSX, .TXT, .JSON or .JSONL\"\\n                )\\n                if \".\" in fname:\\n                    error_msg += f\" Your file `{fname}` ends with the extension `.{fname.split(\\'.\\')[-1]}` which is not supported.\"\\n                else:\\n                    error_msg += f\" Your file `{fname}` is missing a file extension.\"\\n\\n        except (ValueError, TypeError):\\n            file_extension_str = fname.split(\".\")[-1].upper()\\n            error_msg = f\"Your file `{fname}` does not appear to be in valid {file_extension_str} format. Please ensure your file is formatted as a valid {file_extension_str} file.\"\\n\\n    else:\\n        error_msg = f\"File {fname} does not exist.\"\\n\\n    remediation = Remediation(\\n        name=\"read_any_format\",\\n        necessary_msg=necessary_msg,\\n        immediate_msg=immediate_msg,\\n        error_msg=error_msg,\\n    )\\n    return df, remediation\\n\\n',\n",
       "  'function_name': 'read_any_format',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def format_inferrer_validator(df: pd.DataFrame) -> Remediation:\\n    \"\"\"\\n    This validator will infer the likely fine-tuning format of the data, and display it to the user if it is classification.\\n    It will also suggest to use ada and explain train/validation split benefits.\\n    \"\"\"\\n    ft_type = infer_task_type(df)\\n    immediate_msg = None\\n    if ft_type == \"classification\":\\n        immediate_msg = f\"\\\\n- Based on your data it seems like you\\'re trying to fine-tune a model for {ft_type}\\\\n- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\\\\n- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\"\\n    return Remediation(name=\"num_examples\", immediate_msg=immediate_msg)\\n\\n',\n",
       "  'function_name': 'format_inferrer_validator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def apply_necessary_remediation(df: OptionalDataFrameT, remediation: Remediation) -> OptionalDataFrameT:\\n    \"\"\"\\n    This function will apply a necessary remediation to a dataframe, or print an error message if one exists.\\n    \"\"\"\\n    if remediation.error_msg is not None:\\n        sys.stderr.write(f\"\\\\n\\\\nERROR in {remediation.name} validator: {remediation.error_msg}\\\\n\\\\nAborting...\")\\n        sys.exit(1)\\n    if remediation.immediate_msg is not None:\\n        sys.stdout.write(remediation.immediate_msg)\\n    if remediation.necessary_fn is not None:\\n        df = remediation.necessary_fn(df)\\n    return df\\n\\n',\n",
       "  'function_name': 'apply_necessary_remediation',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def accept_suggestion(input_text: str, auto_accept: bool) -> bool:\\n    sys.stdout.write(input_text)\\n    if auto_accept:\\n        sys.stdout.write(\"Y\\\\n\")\\n        return True\\n    return input().lower() != \"n\"\\n\\n',\n",
       "  'function_name': 'accept_suggestion',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def apply_optional_remediation(\\n    df: pd.DataFrame, remediation: Remediation, auto_accept: bool\\n) -> tuple[pd.DataFrame, bool]:\\n    \"\"\"\\n    This function will apply an optional remediation to a dataframe, based on the user input.\\n    \"\"\"\\n    optional_applied = False\\n    input_text = f\"- [Recommended] {remediation.optional_msg} [Y/n]: \"\\n    if remediation.optional_msg is not None:\\n        if accept_suggestion(input_text, auto_accept):\\n            assert remediation.optional_fn is not None\\n            df = remediation.optional_fn(df)\\n            optional_applied = True\\n    if remediation.necessary_msg is not None:\\n        sys.stdout.write(f\"- [Necessary] {remediation.necessary_msg}\\\\n\")\\n    return df, optional_applied\\n\\n',\n",
       "  'function_name': 'apply_optional_remediation',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def estimate_fine_tuning_time(df: pd.DataFrame) -> None:\\n    \"\"\"\\n    Estimate the time it\\'ll take to fine-tune the dataset\\n    \"\"\"\\n    ft_format = infer_task_type(df)\\n    expected_time = 1.0\\n    if ft_format == \"classification\":\\n        num_examples = len(df)\\n        expected_time = num_examples * 1.44\\n    else:\\n        size = df.memory_usage(index=True).sum()\\n        expected_time = size * 0.0515\\n\\n    def format_time(time: float) -> str:\\n        if time < 60:\\n            return f\"{round(time, 2)} seconds\"\\n        elif time < 3600:\\n            return f\"{round(time / 60, 2)} minutes\"\\n        elif time < 86400:\\n            return f\"{round(time / 3600, 2)} hours\"\\n        else:\\n            return f\"{round(time / 86400, 2)} days\"\\n\\n    time_string = format_time(expected_time + 140)\\n    sys.stdout.write(\\n        f\"Once your model starts training, it\\'ll approximately take {time_string} to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\\\\n\"\\n    )\\n\\n',\n",
       "  'function_name': 'estimate_fine_tuning_time',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def get_outfnames(fname: str, split: bool) -> list[str]:\\n    suffixes = [\"_train\", \"_valid\"] if split else [\"\"]\\n    i = 0\\n    while True:\\n        index_suffix = f\" ({i})\" if i > 0 else \"\"\\n        candidate_fnames = [f\"{os.path.splitext(fname)[0]}_prepared{suffix}{index_suffix}.jsonl\" for suffix in suffixes]\\n        if not any(os.path.isfile(f) for f in candidate_fnames):\\n            return candidate_fnames\\n        i += 1\\n\\n',\n",
       "  'function_name': 'get_outfnames',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def get_classification_hyperparams(df: pd.DataFrame) -> tuple[int, object]:\\n    n_classes = df.completion.nunique()\\n    pos_class = None\\n    if n_classes == 2:\\n        pos_class = df.completion.value_counts().index[0]\\n    return n_classes, pos_class\\n\\n',\n",
       "  'function_name': 'get_classification_hyperparams',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def write_out_file(df: pd.DataFrame, fname: str, any_remediations: bool, auto_accept: bool) -> None:\\n    \"\"\"\\n    This function will write out a dataframe to a file, if the user would like to proceed, and also offer a fine-tuning command with the newly created file.\\n    For classification it will optionally ask the user if they would like to split the data into train/valid files, and modify the suggested command to include the valid set.\\n    \"\"\"\\n    ft_format = infer_task_type(df)\\n    common_prompt_suffix = get_common_xfix(df.prompt, xfix=\"suffix\")\\n    common_completion_suffix = get_common_xfix(df.completion, xfix=\"suffix\")\\n\\n    split = False\\n    input_text = \"- [Recommended] Would you like to split into training and validation set? [Y/n]: \"\\n    if ft_format == \"classification\":\\n        if accept_suggestion(input_text, auto_accept):\\n            split = True\\n\\n    additional_params = \"\"\\n    common_prompt_suffix_new_line_handled = common_prompt_suffix.replace(\"\\\\n\", \"\\\\\\\\n\")\\n    common_completion_suffix_new_line_handled = common_completion_suffix.replace(\"\\\\n\", \"\\\\\\\\n\")\\n    optional_ending_string = (\\n        f\\' Make sure to include `stop=[\"{common_completion_suffix_new_line_handled}\"]` so that the generated texts ends at the expected place.\\'\\n        if len(common_completion_suffix_new_line_handled) > 0\\n        else \"\"\\n    )\\n\\n    input_text = \"\\\\n\\\\nYour data will be written to a new JSONL file. Proceed [Y/n]: \"\\n\\n    if not any_remediations and not split:\\n        sys.stdout.write(\\n            f\\'\\\\nYou can use your file for fine-tuning:\\\\n> openai api fine_tunes.create -t \"{fname}\"{additional_params}\\\\n\\\\nAfter you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `{common_prompt_suffix_new_line_handled}` for the model to start generating completions, rather than continuing with the prompt.{optional_ending_string}\\\\n\\'\\n        )\\n        estimate_fine_tuning_time(df)\\n\\n    elif accept_suggestion(input_text, auto_accept):\\n        fnames = get_outfnames(fname, split)\\n        if split:\\n            assert len(fnames) == 2 and \"train\" in fnames[0] and \"valid\" in fnames[1]\\n            MAX_VALID_EXAMPLES = 1000\\n            n_train = max(len(df) - MAX_VALID_EXAMPLES, int(len(df) * 0.8))\\n            df_train = df.sample(n=n_train, random_state=42)\\n            df_valid = df.drop(df_train.index)\\n            df_train[[\"prompt\", \"completion\"]].to_json(  # type: ignore\\n                fnames[0], lines=True, orient=\"records\", force_ascii=False, indent=None\\n            )\\n            df_valid[[\"prompt\", \"completion\"]].to_json(\\n                fnames[1], lines=True, orient=\"records\", force_ascii=False, indent=None\\n            )\\n\\n            n_classes, pos_class = get_classification_hyperparams(df)\\n            additional_params += \" --compute_classification_metrics\"\\n            if n_classes == 2:\\n                additional_params += f\\' --classification_positive_class \"{pos_class}\"\\'\\n            else:\\n                additional_params += f\" --classification_n_classes {n_classes}\"\\n        else:\\n            assert len(fnames) == 1\\n            df[[\"prompt\", \"completion\"]].to_json(\\n                fnames[0], lines=True, orient=\"records\", force_ascii=False, indent=None\\n            )\\n\\n        # Add -v VALID_FILE if we split the file into train / valid\\n        files_string = (\"s\" if split else \"\") + \" to `\" + (\"` and `\".join(fnames))\\n        valid_string = f\\' -v \"{fnames[1]}\"\\' if split else \"\"\\n        separator_reminder = (\\n            \"\"\\n            if len(common_prompt_suffix_new_line_handled) == 0\\n            else f\"After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `{common_prompt_suffix_new_line_handled}` for the model to start generating completions, rather than continuing with the prompt.\"\\n        )\\n        sys.stdout.write(\\n            f\\'\\\\nWrote modified file{files_string}`\\\\nFeel free to take a look!\\\\n\\\\nNow use that file when fine-tuning:\\\\n> openai api fine_tunes.create -t \"{fnames[0]}\"{valid_string}{additional_params}\\\\n\\\\n{separator_reminder}{optional_ending_string}\\\\n\\'\\n        )\\n        estimate_fine_tuning_time(df)\\n    else:\\n        sys.stdout.write(\"Aborting... did not write the file\\\\n\")\\n\\n',\n",
       "  'function_name': 'write_out_file',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def infer_task_type(df: pd.DataFrame) -> str:\\n    \"\"\"\\n    Infer the likely fine-tuning task type from the data\\n    \"\"\"\\n    CLASSIFICATION_THRESHOLD = 3  # min_average instances of each class\\n    if sum(df.prompt.str.len()) == 0:\\n        return \"open-ended generation\"\\n\\n    if len(df.completion.unique()) < len(df) / CLASSIFICATION_THRESHOLD:\\n        return \"classification\"\\n\\n    return \"conditional generation\"\\n\\n',\n",
       "  'function_name': 'infer_task_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def get_common_xfix(series: Any, xfix: str = \"suffix\") -> str:\\n    \"\"\"\\n    Finds the longest common suffix or prefix of all the values in a series\\n    \"\"\"\\n    common_xfix = \"\"\\n    while True:\\n        common_xfixes = (\\n            series.str[-(len(common_xfix) + 1) :] if xfix == \"suffix\" else series.str[: len(common_xfix) + 1]\\n        )  # first few or last few characters\\n        if common_xfixes.nunique() != 1:  # we found the character at which we don\\'t have a unique xfix anymore\\n            break\\n        elif common_xfix == common_xfixes.values[0]:  # the entire first row is a prefix of every other row\\n            break\\n        else:  # the first or last few characters are still common across all rows - let\\'s try to add one more\\n            common_xfix = common_xfixes.values[0]\\n    return common_xfix\\n\\n',\n",
       "  'function_name': 'get_common_xfix',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def get_validators() -> list[Validator]:\\n    return [\\n        num_examples_validator,\\n        lambda x: necessary_column_validator(x, \"prompt\"),\\n        lambda x: necessary_column_validator(x, \"completion\"),\\n        additional_column_validator,\\n        non_empty_field_validator,\\n        format_inferrer_validator,\\n        duplicated_rows_validator,\\n        long_examples_validator,\\n        lambda x: lower_case_validator(x, \"prompt\"),\\n        lambda x: lower_case_validator(x, \"completion\"),\\n        common_prompt_suffix_validator,\\n        common_prompt_prefix_validator,\\n        common_completion_prefix_validator,\\n        common_completion_suffix_validator,\\n        completions_space_start_validator,\\n    ]\\n\\n',\n",
       "  'function_name': 'get_validators',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def apply_validators(\\n    df: pd.DataFrame,\\n    fname: str,\\n    remediation: Remediation | None,\\n    validators: list[Validator],\\n    auto_accept: bool,\\n    write_out_file_func: Callable[..., Any],\\n) -> None:\\n    optional_remediations: list[Remediation] = []\\n    if remediation is not None:\\n        optional_remediations.append(remediation)\\n    for validator in validators:\\n        remediation = validator(df)\\n        if remediation is not None:\\n            optional_remediations.append(remediation)\\n            df = apply_necessary_remediation(df, remediation)\\n\\n    any_optional_or_necessary_remediations = any(\\n        [\\n            remediation\\n            for remediation in optional_remediations\\n            if remediation.optional_msg is not None or remediation.necessary_msg is not None\\n        ]\\n    )\\n    any_necessary_applied = any(\\n        [remediation for remediation in optional_remediations if remediation.necessary_msg is not None]\\n    )\\n    any_optional_applied = False\\n\\n    if any_optional_or_necessary_remediations:\\n        sys.stdout.write(\"\\\\n\\\\nBased on the analysis we will perform the following actions:\\\\n\")\\n        for remediation in optional_remediations:\\n            df, optional_applied = apply_optional_remediation(df, remediation, auto_accept)\\n            any_optional_applied = any_optional_applied or optional_applied\\n    else:\\n        sys.stdout.write(\"\\\\n\\\\nNo remediations found.\\\\n\")\\n\\n    any_optional_or_necessary_applied = any_optional_applied or any_necessary_applied\\n\\n    write_out_file_func(df, fname, any_optional_or_necessary_applied, auto_accept)\\n',\n",
       "  'function_name': 'apply_validators',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/_validators.py'},\n",
       " {'code': 'def accumulate_run_step(\\n    *,\\n    event: AssistantStreamEvent,\\n    run_step_snapshots: dict[str, RunStep],\\n) -> None:\\n    if event.event == \"thread.run.step.created\":\\n        run_step_snapshots[event.data.id] = event.data\\n        return\\n\\n    if event.event == \"thread.run.step.delta\":\\n        data = event.data\\n        snapshot = run_step_snapshots[data.id]\\n\\n        if data.delta:\\n            merged = accumulate_delta(\\n                cast(\\n                    \"dict[object, object]\",\\n                    snapshot.model_dump(exclude_unset=True),\\n                ),\\n                cast(\\n                    \"dict[object, object]\",\\n                    data.delta.model_dump(exclude_unset=True),\\n                ),\\n            )\\n            run_step_snapshots[snapshot.id] = cast(RunStep, construct_type(type_=RunStep, value=merged))\\n\\n    return None\\n\\n',\n",
       "  'function_name': 'accumulate_run_step',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/streaming/_assistants.py'},\n",
       " {'code': 'def accumulate_event(\\n    *,\\n    event: AssistantStreamEvent,\\n    current_message_snapshot: Message | None,\\n) -> tuple[Message | None, list[MessageContentDelta]]:\\n    \"\"\"Returns a tuple of message snapshot and newly created text message deltas\"\"\"\\n    if event.event == \"thread.message.created\":\\n        return event.data, []\\n\\n    new_content: list[MessageContentDelta] = []\\n\\n    if event.event != \"thread.message.delta\":\\n        return current_message_snapshot, []\\n\\n    if not current_message_snapshot:\\n        raise RuntimeError(\"Encountered a message delta with no previous snapshot\")\\n\\n    data = event.data\\n    if data.delta.content:\\n        for content_delta in data.delta.content:\\n            try:\\n                block = current_message_snapshot.content[content_delta.index]\\n            except IndexError:\\n                current_message_snapshot.content.insert(\\n                    content_delta.index,\\n                    cast(\\n                        MessageContent,\\n                        construct_type(\\n                            # mypy doesn\\'t allow Content for some reason\\n                            type_=cast(Any, MessageContent),\\n                            value=content_delta.model_dump(exclude_unset=True),\\n                        ),\\n                    ),\\n                )\\n                new_content.append(content_delta)\\n            else:\\n                merged = accumulate_delta(\\n                    cast(\\n                        \"dict[object, object]\",\\n                        block.model_dump(exclude_unset=True),\\n                    ),\\n                    cast(\\n                        \"dict[object, object]\",\\n                        content_delta.model_dump(exclude_unset=True),\\n                    ),\\n                )\\n                current_message_snapshot.content[content_delta.index] = cast(\\n                    MessageContent,\\n                    construct_type(\\n                        # mypy doesn\\'t allow Content for some reason\\n                        type_=cast(Any, MessageContent),\\n                        value=merged,\\n                    ),\\n                )\\n\\n    return current_message_snapshot, new_content\\n\\n',\n",
       "  'function_name': 'accumulate_event',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/streaming/_assistants.py'},\n",
       " {'code': 'def accumulate_delta(acc: dict[object, object], delta: dict[object, object]) -> dict[object, object]:\\n    for key, delta_value in delta.items():\\n        if key not in acc:\\n            acc[key] = delta_value\\n            continue\\n\\n        acc_value = acc[key]\\n        if acc_value is None:\\n            acc[key] = delta_value\\n            continue\\n\\n        # the `index` property is used in arrays of objects so it should\\n        # not be accumulated like other values e.g.\\n        # [{\\'foo\\': \\'bar\\', \\'index\\': 0}]\\n        #\\n        # the same applies to `type` properties as they\\'re used for\\n        # discriminated unions\\n        if key == \"index\" or key == \"type\":\\n            acc[key] = delta_value\\n            continue\\n\\n        if isinstance(acc_value, str) and isinstance(delta_value, str):\\n            acc_value += delta_value\\n        elif isinstance(acc_value, (int, float)) and isinstance(delta_value, (int, float)):\\n            acc_value += delta_value\\n        elif is_dict(acc_value) and is_dict(delta_value):\\n            acc_value = accumulate_delta(acc_value, delta_value)\\n        elif is_list(acc_value) and is_list(delta_value):\\n            # for lists of non-dictionary items we\\'ll only ever get new entries\\n            # in the array, existing entries will never be changed\\n            if all(isinstance(x, (str, int, float)) for x in acc_value):\\n                acc_value.extend(delta_value)\\n                continue\\n\\n            for delta_entry in delta_value:\\n                if not is_dict(delta_entry):\\n                    raise TypeError(f\"Unexpected list delta entry is not a dictionary: {delta_entry}\")\\n\\n                try:\\n                    index = delta_entry[\"index\"]\\n                except KeyError as exc:\\n                    raise RuntimeError(f\"Expected list delta entry to have an `index` key; {delta_entry}\") from exc\\n\\n                if not isinstance(index, int):\\n                    raise TypeError(f\"Unexpected, list delta entry `index` value is not an integer; {index}\")\\n\\n                try:\\n                    acc_entry = acc_value[index]\\n                except IndexError:\\n                    acc_value.insert(index, delta_entry)\\n                else:\\n                    if not is_dict(acc_entry):\\n                        raise TypeError(\"not handled yet\")\\n\\n                    acc_value[index] = accumulate_delta(acc_entry, delta_entry)\\n\\n        acc[key] = acc_value\\n\\n    return acc\\n',\n",
       "  'function_name': 'accumulate_delta',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/lib/streaming/_assistants.py'},\n",
       " {'code': 'def is_annotated_type(typ: type) -> bool:\\n    return get_origin(typ) == Annotated\\n\\n',\n",
       "  'function_name': 'is_annotated_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def is_list_type(typ: type) -> bool:\\n    return (get_origin(typ) or typ) == list\\n\\n',\n",
       "  'function_name': 'is_list_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def is_iterable_type(typ: type) -> bool:\\n    \"\"\"If the given type is `typing.Iterable[T]`\"\"\"\\n    origin = get_origin(typ) or typ\\n    return origin == Iterable or origin == _c_abc.Iterable\\n\\n',\n",
       "  'function_name': 'is_iterable_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def is_union_type(typ: type) -> bool:\\n    return _is_union(get_origin(typ))\\n\\n',\n",
       "  'function_name': 'is_union_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def is_required_type(typ: type) -> bool:\\n    return get_origin(typ) == Required\\n\\n',\n",
       "  'function_name': 'is_required_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def is_typevar(typ: type) -> bool:\\n    # type ignore is required because type checkers\\n    # think this expression will always return False\\n    return type(typ) == TypeVar  # type: ignore\\n\\n',\n",
       "  'function_name': 'is_typevar',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def strip_annotated_type(typ: type) -> type:\\n    if is_required_type(typ) or is_annotated_type(typ):\\n        return strip_annotated_type(cast(type, get_args(typ)[0]))\\n\\n    return typ\\n\\n',\n",
       "  'function_name': 'strip_annotated_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def extract_type_arg(typ: type, index: int) -> type:\\n    args = get_args(typ)\\n    try:\\n        return cast(type, args[index])\\n    except IndexError as err:\\n        raise RuntimeError(f\"Expected type {typ} to have a type argument at index {index} but it did not\") from err\\n\\n',\n",
       "  'function_name': 'extract_type_arg',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def extract_type_var_from_base(\\n    typ: type,\\n    *,\\n    generic_bases: tuple[type, ...],\\n    index: int,\\n    failure_message: str | None = None,\\n) -> type:\\n    \"\"\"Given a type like `Foo[T]`, returns the generic type variable `T`.\\n\\n    This also handles the case where a concrete subclass is given, e.g.\\n    ```py\\n    class MyResponse(Foo[bytes]):\\n        ...\\n\\n    extract_type_var(MyResponse, bases=(Foo,), index=0) -> bytes\\n    ```\\n\\n    And where a generic subclass is given:\\n    ```py\\n    _T = TypeVar(\\'_T\\')\\n    class MyResponse(Foo[_T]):\\n        ...\\n\\n    extract_type_var(MyResponse[bytes], bases=(Foo,), index=0) -> bytes\\n    ```\\n    \"\"\"\\n    cls = cast(object, get_origin(typ) or typ)\\n    if cls in generic_bases:\\n        # we\\'re given the class directly\\n        return extract_type_arg(typ, index)\\n\\n    # if a subclass is given\\n    # ---\\n    # this is needed as __orig_bases__ is not present in the typeshed stubs\\n    # because it is intended to be for internal use only, however there does\\n    # not seem to be a way to resolve generic TypeVars for inherited subclasses\\n    # without using it.\\n    if isinstance(cls, InheritsGeneric):\\n        target_base_class: Any | None = None\\n        for base in cls.__orig_bases__:\\n            if base.__origin__ in generic_bases:\\n                target_base_class = base\\n                break\\n\\n        if target_base_class is None:\\n            raise RuntimeError(\\n                \"Could not find the generic base class;\\\\n\"\\n                \"This should never happen;\\\\n\"\\n                f\"Does {cls} inherit from one of {generic_bases} ?\"\\n            )\\n\\n        extracted = extract_type_arg(target_base_class, index)\\n        if is_typevar(extracted):\\n            # If the extracted type argument is itself a type variable\\n            # then that means the subclass itself is generic, so we have\\n            # to resolve the type argument from the class itself, not\\n            # the base class.\\n            #\\n            # Note: if there is more than 1 type argument, the subclass could\\n            # change the ordering of the type arguments, this is not currently\\n            # supported.\\n            return extract_type_arg(typ, index)\\n\\n        return extracted\\n\\n    raise RuntimeError(failure_message or f\"Could not resolve inner type variable at index {index} for {typ}\")\\n',\n",
       "  'function_name': 'extract_type_var_from_base',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_typing.py'},\n",
       " {'code': 'def _basic_config() -> None:\\n    # e.g. [2023-10-05 14:12:26 - openai._base_client:818 - DEBUG] HTTP Request: POST http://127.0.0.1:4010/foo/bar \"200 OK\"\\n    logging.basicConfig(\\n        format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\\n    )\\n\\n',\n",
       "  'function_name': '_basic_config',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_logs.py'},\n",
       " {'code': 'def setup_logging() -> None:\\n    env = os.environ.get(\"OPENAI_LOG\")\\n    if env == \"debug\":\\n        _basic_config()\\n        logger.setLevel(logging.DEBUG)\\n        httpx_logger.setLevel(logging.DEBUG)\\n    elif env == \"info\":\\n        _basic_config()\\n        logger.setLevel(logging.INFO)\\n        httpx_logger.setLevel(logging.INFO)\\n',\n",
       "  'function_name': 'setup_logging',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_logs.py'},\n",
       " {'code': 'def maybe_transform(\\n    data: object,\\n    expected_type: object,\\n) -> Any | None:\\n    \"\"\"Wrapper over `transform()` that allows `None` to be passed.\\n\\n    See `transform()` for more details.\\n    \"\"\"\\n    if data is None:\\n        return None\\n    return transform(data, expected_type)\\n\\n',\n",
       "  'function_name': 'maybe_transform',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def transform(\\n    data: _T,\\n    expected_type: object,\\n) -> _T:\\n    \"\"\"Transform dictionaries based off of type information from the given type, for example:\\n\\n    ```py\\n    class Params(TypedDict, total=False):\\n        card_id: Required[Annotated[str, PropertyInfo(alias=\"cardID\")]]\\n\\n\\n    transformed = transform({\"card_id\": \"<my card ID>\"}, Params)\\n    # {\\'cardID\\': \\'<my card ID>\\'}\\n    ```\\n\\n    Any keys / data that does not have type information given will be included as is.\\n\\n    It should be noted that the transformations that this function does are not represented in the type system.\\n    \"\"\"\\n    transformed = _transform_recursive(data, annotation=cast(type, expected_type))\\n    return cast(_T, transformed)\\n\\n',\n",
       "  'function_name': 'transform',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def _get_annotated_type(type_: type) -> type | None:\\n    \"\"\"If the given type is an `Annotated` type then it is returned, if not `None` is returned.\\n\\n    This also unwraps the type when applicable, e.g. `Required[Annotated[T, ...]]`\\n    \"\"\"\\n    if is_required_type(type_):\\n        # Unwrap `Required[Annotated[T, ...]]` to `Annotated[T, ...]`\\n        type_ = get_args(type_)[0]\\n\\n    if is_annotated_type(type_):\\n        return type_\\n\\n    return None\\n\\n',\n",
       "  'function_name': '_get_annotated_type',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def _maybe_transform_key(key: str, type_: type) -> str:\\n    \"\"\"Transform the given `data` based on the annotations provided in `type_`.\\n\\n    Note: this function only looks at `Annotated` types that contain `PropertInfo` metadata.\\n    \"\"\"\\n    annotated_type = _get_annotated_type(type_)\\n    if annotated_type is None:\\n        # no `Annotated` definition for this type, no transformation needed\\n        return key\\n\\n    # ignore the first argument as it is the actual type\\n    annotations = get_args(annotated_type)[1:]\\n    for annotation in annotations:\\n        if isinstance(annotation, PropertyInfo) and annotation.alias is not None:\\n            return annotation.alias\\n\\n    return key\\n\\n',\n",
       "  'function_name': '_maybe_transform_key',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def _transform_recursive(\\n    data: object,\\n    *,\\n    annotation: type,\\n    inner_type: type | None = None,\\n) -> object:\\n    \"\"\"Transform the given data against the expected type.\\n\\n    Args:\\n        annotation: The direct type annotation given to the particular piece of data.\\n            This may or may not be wrapped in metadata types, e.g. `Required[T]`, `Annotated[T, ...]` etc\\n\\n        inner_type: If applicable, this is the \"inside\" type. This is useful in certain cases where the outside type\\n            is a container type such as `List[T]`. In that case `inner_type` should be set to `T` so that each entry in\\n            the list can be transformed using the metadata from the container type.\\n\\n            Defaults to the same value as the `annotation` argument.\\n    \"\"\"\\n    if inner_type is None:\\n        inner_type = annotation\\n\\n    stripped_type = strip_annotated_type(inner_type)\\n    if is_typeddict(stripped_type) and is_mapping(data):\\n        return _transform_typeddict(data, stripped_type)\\n\\n    if (\\n        # List[T]\\n        (is_list_type(stripped_type) and is_list(data))\\n        # Iterable[T]\\n        or (is_iterable_type(stripped_type) and is_iterable(data) and not isinstance(data, str))\\n    ):\\n        inner_type = extract_type_arg(stripped_type, 0)\\n        return [_transform_recursive(d, annotation=annotation, inner_type=inner_type) for d in data]\\n\\n    if is_union_type(stripped_type):\\n        # For union types we run the transformation against all subtypes to ensure that everything is transformed.\\n        #\\n        # TODO: there may be edge cases where the same normalized field name will transform to two different names\\n        # in different subtypes.\\n        for subtype in get_args(stripped_type):\\n            data = _transform_recursive(data, annotation=annotation, inner_type=subtype)\\n        return data\\n\\n    if isinstance(data, pydantic.BaseModel):\\n        return model_dump(data, exclude_unset=True)\\n\\n    annotated_type = _get_annotated_type(annotation)\\n    if annotated_type is None:\\n        return data\\n\\n    # ignore the first argument as it is the actual type\\n    annotations = get_args(annotated_type)[1:]\\n    for annotation in annotations:\\n        if isinstance(annotation, PropertyInfo) and annotation.format is not None:\\n            return _format_data(data, annotation.format, annotation.format_template)\\n\\n    return data\\n\\n',\n",
       "  'function_name': '_transform_recursive',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def _format_data(data: object, format_: PropertyFormat, format_template: str | None) -> object:\\n    if isinstance(data, (date, datetime)):\\n        if format_ == \"iso8601\":\\n            return data.isoformat()\\n\\n        if format_ == \"custom\" and format_template is not None:\\n            return data.strftime(format_template)\\n\\n    if format_ == \"base64\" and is_base64_file_input(data):\\n        binary: str | bytes | None = None\\n\\n        if isinstance(data, pathlib.Path):\\n            binary = data.read_bytes()\\n        elif isinstance(data, io.IOBase):\\n            binary = data.read()\\n\\n            if isinstance(binary, str):  # type: ignore[unreachable]\\n                binary = binary.encode()\\n\\n        if not isinstance(binary, bytes):\\n            raise RuntimeError(f\"Could not read bytes from {data}; Received {type(binary)}\")\\n\\n        return base64.b64encode(binary).decode(\"ascii\")\\n\\n    return data\\n\\n',\n",
       "  'function_name': '_format_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def _transform_typeddict(\\n    data: Mapping[str, object],\\n    expected_type: type,\\n) -> Mapping[str, object]:\\n    result: dict[str, object] = {}\\n    annotations = get_type_hints(expected_type, include_extras=True)\\n    for key, value in data.items():\\n        type_ = annotations.get(key)\\n        if type_ is None:\\n            # we do not have a type annotation for this field, leave it as is\\n            result[key] = value\\n        else:\\n            result[_maybe_transform_key(key, type_)] = _transform_recursive(value, annotation=type_)\\n    return result\\n\\n',\n",
       "  'function_name': '_transform_typeddict',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'async def async_maybe_transform(\\n    data: object,\\n    expected_type: object,\\n) -> Any | None:\\n    \"\"\"Wrapper over `async_transform()` that allows `None` to be passed.\\n\\n    See `async_transform()` for more details.\\n    \"\"\"\\n    if data is None:\\n        return None\\n    return await async_transform(data, expected_type)\\n\\n',\n",
       "  'function_name': 'async_maybe_transform',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'async def async_transform(\\n    data: _T,\\n    expected_type: object,\\n) -> _T:\\n    \"\"\"Transform dictionaries based off of type information from the given type, for example:\\n\\n    ```py\\n    class Params(TypedDict, total=False):\\n        card_id: Required[Annotated[str, PropertyInfo(alias=\"cardID\")]]\\n\\n\\n    transformed = transform({\"card_id\": \"<my card ID>\"}, Params)\\n    # {\\'cardID\\': \\'<my card ID>\\'}\\n    ```\\n\\n    Any keys / data that does not have type information given will be included as is.\\n\\n    It should be noted that the transformations that this function does are not represented in the type system.\\n    \"\"\"\\n    transformed = await _async_transform_recursive(data, annotation=cast(type, expected_type))\\n    return cast(_T, transformed)\\n\\n',\n",
       "  'function_name': 'async_transform',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'async def _async_transform_recursive(\\n    data: object,\\n    *,\\n    annotation: type,\\n    inner_type: type | None = None,\\n) -> object:\\n    \"\"\"Transform the given data against the expected type.\\n\\n    Args:\\n        annotation: The direct type annotation given to the particular piece of data.\\n            This may or may not be wrapped in metadata types, e.g. `Required[T]`, `Annotated[T, ...]` etc\\n\\n        inner_type: If applicable, this is the \"inside\" type. This is useful in certain cases where the outside type\\n            is a container type such as `List[T]`. In that case `inner_type` should be set to `T` so that each entry in\\n            the list can be transformed using the metadata from the container type.\\n\\n            Defaults to the same value as the `annotation` argument.\\n    \"\"\"\\n    if inner_type is None:\\n        inner_type = annotation\\n\\n    stripped_type = strip_annotated_type(inner_type)\\n    if is_typeddict(stripped_type) and is_mapping(data):\\n        return await _async_transform_typeddict(data, stripped_type)\\n\\n    if (\\n        # List[T]\\n        (is_list_type(stripped_type) and is_list(data))\\n        # Iterable[T]\\n        or (is_iterable_type(stripped_type) and is_iterable(data) and not isinstance(data, str))\\n    ):\\n        inner_type = extract_type_arg(stripped_type, 0)\\n        return [await _async_transform_recursive(d, annotation=annotation, inner_type=inner_type) for d in data]\\n\\n    if is_union_type(stripped_type):\\n        # For union types we run the transformation against all subtypes to ensure that everything is transformed.\\n        #\\n        # TODO: there may be edge cases where the same normalized field name will transform to two different names\\n        # in different subtypes.\\n        for subtype in get_args(stripped_type):\\n            data = await _async_transform_recursive(data, annotation=annotation, inner_type=subtype)\\n        return data\\n\\n    if isinstance(data, pydantic.BaseModel):\\n        return model_dump(data, exclude_unset=True)\\n\\n    annotated_type = _get_annotated_type(annotation)\\n    if annotated_type is None:\\n        return data\\n\\n    # ignore the first argument as it is the actual type\\n    annotations = get_args(annotated_type)[1:]\\n    for annotation in annotations:\\n        if isinstance(annotation, PropertyInfo) and annotation.format is not None:\\n            return await _async_format_data(data, annotation.format, annotation.format_template)\\n\\n    return data\\n\\n',\n",
       "  'function_name': '_async_transform_recursive',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'async def _async_format_data(data: object, format_: PropertyFormat, format_template: str | None) -> object:\\n    if isinstance(data, (date, datetime)):\\n        if format_ == \"iso8601\":\\n            return data.isoformat()\\n\\n        if format_ == \"custom\" and format_template is not None:\\n            return data.strftime(format_template)\\n\\n    if format_ == \"base64\" and is_base64_file_input(data):\\n        binary: str | bytes | None = None\\n\\n        if isinstance(data, pathlib.Path):\\n            binary = await anyio.Path(data).read_bytes()\\n        elif isinstance(data, io.IOBase):\\n            binary = data.read()\\n\\n            if isinstance(binary, str):  # type: ignore[unreachable]\\n                binary = binary.encode()\\n\\n        if not isinstance(binary, bytes):\\n            raise RuntimeError(f\"Could not read bytes from {data}; Received {type(binary)}\")\\n\\n        return base64.b64encode(binary).decode(\"ascii\")\\n\\n    return data\\n\\n',\n",
       "  'function_name': '_async_format_data',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'async def _async_transform_typeddict(\\n    data: Mapping[str, object],\\n    expected_type: type,\\n) -> Mapping[str, object]:\\n    result: dict[str, object] = {}\\n    annotations = get_type_hints(expected_type, include_extras=True)\\n    for key, value in data.items():\\n        type_ = annotations.get(key)\\n        if type_ is None:\\n            # we do not have a type annotation for this field, leave it as is\\n            result[key] = value\\n        else:\\n            result[_maybe_transform_key(key, type_)] = await _async_transform_recursive(value, annotation=type_)\\n    return result\\n',\n",
       "  'function_name': '_async_transform_typeddict',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_transform.py'},\n",
       " {'code': 'def asyncify(\\n    function: Callable[T_ParamSpec, T_Retval],\\n    *,\\n    cancellable: bool = False,\\n    limiter: anyio.CapacityLimiter | None = None,\\n) -> Callable[T_ParamSpec, Awaitable[T_Retval]]:\\n    \"\"\"\\n    Take a blocking function and create an async one that receives the same\\n    positional and keyword arguments, and that when called, calls the original function\\n    in a worker thread using `anyio.to_thread.run_sync()`. Internally,\\n    `asyncer.asyncify()` uses the same `anyio.to_thread.run_sync()`, but it supports\\n    keyword arguments additional to positional arguments and it adds better support for\\n    autocompletion and inline errors for the arguments of the function called and the\\n    return value.\\n\\n    If the `cancellable` option is enabled and the task waiting for its completion is\\n    cancelled, the thread will still run its course but its return value (or any raised\\n    exception) will be ignored.\\n\\n    Use it like this:\\n\\n    ```Python\\n    def do_work(arg1, arg2, kwarg1=\"\", kwarg2=\"\") -> str:\\n        # Do work\\n        return \"Some result\"\\n\\n\\n    result = await to_thread.asyncify(do_work)(\"spam\", \"ham\", kwarg1=\"a\", kwarg2=\"b\")\\n    print(result)\\n    ```\\n\\n    ## Arguments\\n\\n    `function`: a blocking regular callable (e.g. a function)\\n    `cancellable`: `True` to allow cancellation of the operation\\n    `limiter`: capacity limiter to use to limit the total amount of threads running\\n        (if omitted, the default limiter is used)\\n\\n    ## Return\\n\\n    An async function that takes the same positional and keyword arguments as the\\n    original one, that when called runs the same original function in a thread worker\\n    and returns the result.\\n    \"\"\"\\n\\n    async def wrapper(*args: T_ParamSpec.args, **kwargs: T_ParamSpec.kwargs) -> T_Retval:\\n        partial_f = functools.partial(function, *args, **kwargs)\\n        return await anyio.to_thread.run_sync(partial_f, cancellable=cancellable, limiter=limiter)\\n\\n    return wrapper\\n',\n",
       "  'function_name': 'asyncify',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_sync.py'},\n",
       " {'code': 'def flatten(t: Iterable[Iterable[_T]]) -> list[_T]:\\n    return [item for sublist in t for item in sublist]\\n\\n',\n",
       "  'function_name': 'flatten',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def extract_files(\\n    # TODO: this needs to take Dict but variance issues.....\\n    # create protocol type ?\\n    query: Mapping[str, object],\\n    *,\\n    paths: Sequence[Sequence[str]],\\n) -> list[tuple[str, FileTypes]]:\\n    \"\"\"Recursively extract files from the given dictionary based on specified paths.\\n\\n    A path may look like this [\\'foo\\', \\'files\\', \\'<array>\\', \\'data\\'].\\n\\n    Note: this mutates the given dictionary.\\n    \"\"\"\\n    files: list[tuple[str, FileTypes]] = []\\n    for path in paths:\\n        files.extend(_extract_items(query, path, index=0, flattened_key=None))\\n    return files\\n\\n',\n",
       "  'function_name': 'extract_files',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def _extract_items(\\n    obj: object,\\n    path: Sequence[str],\\n    *,\\n    index: int,\\n    flattened_key: str | None,\\n) -> list[tuple[str, FileTypes]]:\\n    try:\\n        key = path[index]\\n    except IndexError:\\n        if isinstance(obj, NotGiven):\\n            # no value was provided - we can safely ignore\\n            return []\\n\\n        # cyclical import\\n        from .._files import assert_is_file_content\\n\\n        # We have exhausted the path, return the entry we found.\\n        assert_is_file_content(obj, key=flattened_key)\\n        assert flattened_key is not None\\n        return [(flattened_key, cast(FileTypes, obj))]\\n\\n    index += 1\\n    if is_dict(obj):\\n        try:\\n            # We are at the last entry in the path so we must remove the field\\n            if (len(path)) == index:\\n                item = obj.pop(key)\\n            else:\\n                item = obj[key]\\n        except KeyError:\\n            # Key was not present in the dictionary, this is not indicative of an error\\n            # as the given path may not point to a required field. We also do not want\\n            # to enforce required fields as the API may differ from the spec in some cases.\\n            return []\\n        if flattened_key is None:\\n            flattened_key = key\\n        else:\\n            flattened_key += f\"[{key}]\"\\n        return _extract_items(\\n            item,\\n            path,\\n            index=index,\\n            flattened_key=flattened_key,\\n        )\\n    elif is_list(obj):\\n        if key != \"<array>\":\\n            return []\\n\\n        return flatten(\\n            [\\n                _extract_items(\\n                    item,\\n                    path,\\n                    index=index,\\n                    flattened_key=flattened_key + \"[]\" if flattened_key is not None else \"[]\",\\n                )\\n                for item in obj\\n            ]\\n        )\\n\\n    # Something unexpected was passed, just ignore it.\\n    return []\\n\\n',\n",
       "  'function_name': '_extract_items',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_given(obj: NotGivenOr[_T]) -> TypeGuard[_T]:\\n    return not isinstance(obj, NotGiven)\\n\\n',\n",
       "  'function_name': 'is_given',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_tuple(obj: object) -> TypeGuard[tuple[object, ...]]:\\n    return isinstance(obj, tuple)\\n\\n',\n",
       "  'function_name': 'is_tuple',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_tuple_t(obj: _TupleT | object) -> TypeGuard[_TupleT]:\\n    return isinstance(obj, tuple)\\n\\n',\n",
       "  'function_name': 'is_tuple_t',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_sequence(obj: object) -> TypeGuard[Sequence[object]]:\\n    return isinstance(obj, Sequence)\\n\\n',\n",
       "  'function_name': 'is_sequence',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_sequence_t(obj: _SequenceT | object) -> TypeGuard[_SequenceT]:\\n    return isinstance(obj, Sequence)\\n\\n',\n",
       "  'function_name': 'is_sequence_t',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_mapping(obj: object) -> TypeGuard[Mapping[str, object]]:\\n    return isinstance(obj, Mapping)\\n\\n',\n",
       "  'function_name': 'is_mapping',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_mapping_t(obj: _MappingT | object) -> TypeGuard[_MappingT]:\\n    return isinstance(obj, Mapping)\\n\\n',\n",
       "  'function_name': 'is_mapping_t',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_dict(obj: object) -> TypeGuard[dict[object, object]]:\\n    return isinstance(obj, dict)\\n\\n',\n",
       "  'function_name': 'is_dict',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_list(obj: object) -> TypeGuard[list[object]]:\\n    return isinstance(obj, list)\\n\\n',\n",
       "  'function_name': 'is_list',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def is_iterable(obj: object) -> TypeGuard[Iterable[object]]:\\n    return isinstance(obj, Iterable)\\n\\n',\n",
       "  'function_name': 'is_iterable',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def deepcopy_minimal(item: _T) -> _T:\\n    \"\"\"Minimal reimplementation of copy.deepcopy() that will only copy certain object types:\\n\\n    - mappings, e.g. `dict`\\n    - list\\n\\n    This is done for performance reasons.\\n    \"\"\"\\n    if is_mapping(item):\\n        return cast(_T, {k: deepcopy_minimal(v) for k, v in item.items()})\\n    if is_list(item):\\n        return cast(_T, [deepcopy_minimal(entry) for entry in item])\\n    return item\\n\\n',\n",
       "  'function_name': 'deepcopy_minimal',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def human_join(seq: Sequence[str], *, delim: str = \", \", final: str = \"or\") -> str:\\n    size = len(seq)\\n    if size == 0:\\n        return \"\"\\n\\n    if size == 1:\\n        return seq[0]\\n\\n    if size == 2:\\n        return f\"{seq[0]} {final} {seq[1]}\"\\n\\n    return delim.join(seq[:-1]) + f\" {final} {seq[-1]}\"\\n\\n',\n",
       "  'function_name': 'human_join',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def quote(string: str) -> str:\\n    \"\"\"Add single quotation marks around the given string. Does *not* do any escaping.\"\"\"\\n    return f\"\\'{string}\\'\"\\n\\n',\n",
       "  'function_name': 'quote',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def required_args(*variants: Sequence[str]) -> Callable[[CallableT], CallableT]:\\n    \"\"\"Decorator to enforce a given set of arguments or variants of arguments are passed to the decorated function.\\n\\n    Useful for enforcing runtime validation of overloaded functions.\\n\\n    Example usage:\\n    ```py\\n    @overload\\n    def foo(*, a: str) -> str:\\n        ...\\n\\n\\n    @overload\\n    def foo(*, b: bool) -> str:\\n        ...\\n\\n\\n    # This enforces the same constraints that a static type checker would\\n    # i.e. that either a or b must be passed to the function\\n    @required_args([\"a\"], [\"b\"])\\n    def foo(*, a: str | None = None, b: bool | None = None) -> str:\\n        ...\\n    ```\\n    \"\"\"\\n\\n    def inner(func: CallableT) -> CallableT:\\n        params = inspect.signature(func).parameters\\n        positional = [\\n            name\\n            for name, param in params.items()\\n            if param.kind\\n            in {\\n                param.POSITIONAL_ONLY,\\n                param.POSITIONAL_OR_KEYWORD,\\n            }\\n        ]\\n\\n        @functools.wraps(func)\\n        def wrapper(*args: object, **kwargs: object) -> object:\\n            given_params: set[str] = set()\\n            for i, _ in enumerate(args):\\n                try:\\n                    given_params.add(positional[i])\\n                except IndexError:\\n                    raise TypeError(\\n                        f\"{func.__name__}() takes {len(positional)} argument(s) but {len(args)} were given\"\\n                    ) from None\\n\\n            for key in kwargs.keys():\\n                given_params.add(key)\\n\\n            for variant in variants:\\n                matches = all((param in given_params for param in variant))\\n                if matches:\\n                    break\\n            else:  # no break\\n                if len(variants) > 1:\\n                    variations = human_join(\\n                        [\"(\" + human_join([quote(arg) for arg in variant], final=\"and\") + \")\" for variant in variants]\\n                    )\\n                    msg = f\"Missing required arguments; Expected either {variations} arguments to be given\"\\n                else:\\n                    assert len(variants) > 0\\n\\n                    # TODO: this error message is not deterministic\\n                    missing = list(set(variants[0]) - given_params)\\n                    if len(missing) > 1:\\n                        msg = f\"Missing required arguments: {human_join([quote(arg) for arg in missing])}\"\\n                    else:\\n                        msg = f\"Missing required argument: {quote(missing[0])}\"\\n                raise TypeError(msg)\\n            return func(*args, **kwargs)\\n\\n        return wrapper  # type: ignore\\n\\n    return inner\\n\\n',\n",
       "  'function_name': 'required_args',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def strip_not_given(obj: None) -> None:\\n    ...\\n\\n',\n",
       "  'function_name': 'strip_not_given',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def strip_not_given(obj: Mapping[_K, _V | NotGiven]) -> dict[_K, _V]:\\n    ...\\n\\n',\n",
       "  'function_name': 'strip_not_given',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def strip_not_given(obj: object) -> object:\\n    ...\\n\\n',\n",
       "  'function_name': 'strip_not_given',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def strip_not_given(obj: object | None) -> object:\\n    \"\"\"Remove all top-level keys where their values are instances of `NotGiven`\"\"\"\\n    if obj is None:\\n        return None\\n\\n    if not is_mapping(obj):\\n        return obj\\n\\n    return {key: value for key, value in obj.items() if not isinstance(value, NotGiven)}\\n\\n',\n",
       "  'function_name': 'strip_not_given',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def coerce_integer(val: str) -> int:\\n    return int(val, base=10)\\n\\n',\n",
       "  'function_name': 'coerce_integer',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def coerce_float(val: str) -> float:\\n    return float(val)\\n\\n',\n",
       "  'function_name': 'coerce_float',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def coerce_boolean(val: str) -> bool:\\n    return val == \"true\" or val == \"1\" or val == \"on\"\\n\\n',\n",
       "  'function_name': 'coerce_boolean',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def maybe_coerce_integer(val: str | None) -> int | None:\\n    if val is None:\\n        return None\\n    return coerce_integer(val)\\n\\n',\n",
       "  'function_name': 'maybe_coerce_integer',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def maybe_coerce_float(val: str | None) -> float | None:\\n    if val is None:\\n        return None\\n    return coerce_float(val)\\n\\n',\n",
       "  'function_name': 'maybe_coerce_float',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def maybe_coerce_boolean(val: str | None) -> bool | None:\\n    if val is None:\\n        return None\\n    return coerce_boolean(val)\\n\\n',\n",
       "  'function_name': 'maybe_coerce_boolean',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def removeprefix(string: str, prefix: str) -> str:\\n    \"\"\"Remove a prefix from a string.\\n\\n    Backport of `str.removeprefix` for Python < 3.9\\n    \"\"\"\\n    if string.startswith(prefix):\\n        return string[len(prefix) :]\\n    return string\\n\\n',\n",
       "  'function_name': 'removeprefix',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def removesuffix(string: str, suffix: str) -> str:\\n    \"\"\"Remove a suffix from a string.\\n\\n    Backport of `str.removesuffix` for Python < 3.9\\n    \"\"\"\\n    if string.endswith(suffix):\\n        return string[: -len(suffix)]\\n    return string\\n\\n',\n",
       "  'function_name': 'removesuffix',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def file_from_path(path: str) -> FileTypes:\\n    contents = Path(path).read_bytes()\\n    file_name = os.path.basename(path)\\n    return (file_name, contents)\\n\\n',\n",
       "  'function_name': 'file_from_path',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def get_required_header(headers: HeadersLike, header: str) -> str:\\n    lower_header = header.lower()\\n    if isinstance(headers, Mapping):\\n        headers = cast(Headers, headers)\\n        for k, v in headers.items():\\n            if k.lower() == lower_header and isinstance(v, str):\\n                return v\\n\\n    \"\"\" to deal with the case where the header looks like Stainless-Event-Id \"\"\"\\n    intercaps_header = re.sub(r\"([^\\\\w])(\\\\w)\", lambda pat: pat.group(1) + pat.group(2).upper(), header.capitalize())\\n\\n    for normalized_header in [header, lower_header, header.upper(), intercaps_header]:\\n        value = headers.get(normalized_header)\\n        if value:\\n            return value\\n\\n    raise ValueError(f\"Could not find {header} header\")\\n\\n',\n",
       "  'function_name': 'get_required_header',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def get_async_library() -> str:\\n    try:\\n        return sniffio.current_async_library()\\n    except Exception:\\n        return \"false\"\\n\\n',\n",
       "  'function_name': 'get_async_library',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def lru_cache(*, maxsize: int | None = 128) -> Callable[[CallableT], CallableT]:\\n    \"\"\"A version of functools.lru_cache that retains the type signature\\n    for the wrapped function arguments.\\n    \"\"\"\\n    wrapper = functools.lru_cache(  # noqa: TID251\\n        maxsize=maxsize,\\n    )\\n    return cast(Any, wrapper)  # type: ignore[no-any-return]\\n',\n",
       "  'function_name': 'lru_cache',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_utils.py'},\n",
       " {'code': 'def consume_sync_iterator(iterator: Iterator[Any]) -> None:\\n    for _ in iterator:\\n        ...\\n\\n',\n",
       "  'function_name': 'consume_sync_iterator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_streams.py'},\n",
       " {'code': 'async def consume_async_iterator(iterator: AsyncIterator[Any]) -> None:\\n    async for _ in iterator:\\n        ...\\n',\n",
       "  'function_name': 'consume_async_iterator',\n",
       "  'filepath': '/home/moonpatel/openai-python/src/openai/_utils/_streams.py'},\n",
       " {'code': 'async def main() -> None:\\n    stream = await client.completions.create(\\n        model=\"gpt-3.5-turbo-instruct\",\\n        prompt=\"Say this is a test\",\\n        stream=True,\\n    )\\n    async for completion in stream:\\n        print(completion.choices[0].text, end=\"\")\\n    print()\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/async_demo.py'},\n",
       " {'code': 'def main() -> None:\\n    client = openai.OpenAI()\\n\\n    assistant = client.beta.assistants.create(\\n        name=\"Math Tutor\",\\n        instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\\n        tools=[{\"type\": \"code_interpreter\"}],\\n        model=\"gpt-4-1106-preview\",\\n    )\\n\\n    try:\\n        question = \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\\n\\n        thread = client.beta.threads.create(\\n            messages=[\\n                {\\n                    \"role\": \"user\",\\n                    \"content\": question,\\n                },\\n            ]\\n        )\\n        print(f\"Question: {question}\\\\n\")\\n\\n        with client.beta.threads.runs.stream(\\n            thread_id=thread.id,\\n            assistant_id=assistant.id,\\n            instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\\n            event_handler=EventHandler(),\\n        ) as stream:\\n            stream.until_done()\\n            print()\\n    finally:\\n        client.beta.assistants.delete(assistant.id)\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/assistant_stream_helpers.py'},\n",
       " {'code': 'def sync_main() -> None:\\n    client = OpenAI()\\n    response = client.completions.create(\\n        model=\"gpt-3.5-turbo-instruct\",\\n        prompt=\"1,2,3,\",\\n        max_tokens=5,\\n        temperature=0,\\n        stream=True,\\n    )\\n\\n    # You can manually control iteration over the response\\n    first = next(response)\\n    print(f\"got response data: {first.to_json()}\")\\n\\n    # Or you could automatically iterate through all of data.\\n    # Note that the for loop will not exit until *all* of the data has been processed.\\n    for data in response:\\n        print(data.to_json())\\n\\n',\n",
       "  'function_name': 'sync_main',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/streaming.py'},\n",
       " {'code': 'async def async_main() -> None:\\n    client = AsyncOpenAI()\\n    response = await client.completions.create(\\n        model=\"gpt-3.5-turbo-instruct\",\\n        prompt=\"1,2,3,\",\\n        max_tokens=5,\\n        temperature=0,\\n        stream=True,\\n    )\\n\\n    # You can manually control iteration over the response.\\n    # In Python 3.10+ you can also use the `await anext(response)` builtin instead\\n    first = await response.__anext__()\\n    print(f\"got response data: {first.to_json()}\")\\n\\n    # Or you could automatically iterate through all of data.\\n    # Note that the for loop will not exit until *all* of the data has been processed.\\n    async for data in response:\\n        print(data.to_json())\\n\\n',\n",
       "  'function_name': 'async_main',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/streaming.py'},\n",
       " {'code': 'def main() -> None:\\n    stream_to_speakers()\\n\\n    # Create text-to-speech audio file\\n    with openai.audio.speech.with_streaming_response.create(\\n        model=\"tts-1\",\\n        voice=\"alloy\",\\n        input=\"the quick brown fox jumped over the lazy dogs\",\\n    ) as response:\\n        response.stream_to_file(speech_file_path)\\n\\n    # Create transcription from audio file\\n    transcription = openai.audio.transcriptions.create(\\n        model=\"whisper-1\",\\n        file=speech_file_path,\\n    )\\n    print(transcription.text)\\n\\n    # Create translation from audio file\\n    translation = openai.audio.translations.create(\\n        model=\"whisper-1\",\\n        file=speech_file_path,\\n    )\\n    print(translation.text)\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/audio.py'},\n",
       " {'code': 'def stream_to_speakers() -> None:\\n    import pyaudio\\n\\n    player_stream = pyaudio.PyAudio().open(format=pyaudio.paInt16, channels=1, rate=24000, output=True)\\n\\n    start_time = time.time()\\n\\n    with openai.audio.speech.with_streaming_response.create(\\n        model=\"tts-1\",\\n        voice=\"alloy\",\\n        response_format=\"pcm\",  # similar to WAV, but without a header chunk at the start.\\n        input=\"\"\"I see skies of blue and clouds of white\\n                The bright blessed days, the dark sacred nights\\n                And I think to myself\\n                What a wonderful world\"\"\",\\n    ) as response:\\n        print(f\"Time to first byte: {int((time.time() - start_time) * 1000)}ms\")\\n        for chunk in response.iter_bytes(chunk_size=1024):\\n            player_stream.write(chunk)\\n\\n    print(f\"Done in {int((time.time() - start_time) * 1000)}ms.\")\\n\\n',\n",
       "  'function_name': 'stream_to_speakers',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/audio.py'},\n",
       " {'code': 'def main() -> None:\\n    # Generate an image based on the prompt\\n    response = openai.images.generate(prompt=prompt, model=model)\\n\\n    # Prints response containing a URL link to image\\n    print(response)\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/examples/picture.py'},\n",
       " {'code': 'def format_str(\\n    src: str,\\n) -> tuple[str, Sequence[CodeBlockError]]:\\n    errors: list[CodeBlockError] = []\\n\\n    @contextlib.contextmanager\\n    def _collect_error(match: Match[str]) -> Generator[None, None, None]:\\n        try:\\n            yield\\n        except Exception as e:\\n            errors.append(CodeBlockError(match.start(), e))\\n\\n    def _md_match(match: Match[str]) -> str:\\n        code = textwrap.dedent(match[\"code\"])\\n        with _collect_error(match):\\n            code = format_code_block(code)\\n        code = textwrap.indent(code, match[\"indent\"])\\n        return f\\'{match[\"before\"]}{code}{match[\"after\"]}\\'\\n\\n    def _pycon_match(match: Match[str]) -> str:\\n        code = \"\"\\n        fragment = cast(Optional[str], None)\\n\\n        def finish_fragment() -> None:\\n            nonlocal code\\n            nonlocal fragment\\n\\n            if fragment is not None:\\n                with _collect_error(match):\\n                    fragment = format_code_block(fragment)\\n                fragment_lines = fragment.splitlines()\\n                code += f\"{PYCON_PREFIX}{fragment_lines[0]}\\\\n\"\\n                for line in fragment_lines[1:]:\\n                    # Skip blank lines to handle Black adding a blank above\\n                    # functions within blocks. A blank line would end the REPL\\n                    # continuation prompt.\\n                    #\\n                    # >>> if True:\\n                    # ...     def f():\\n                    # ...         pass\\n                    # ...\\n                    if line:\\n                        code += f\"{PYCON_CONTINUATION_PREFIX} {line}\\\\n\"\\n                if fragment_lines[-1].startswith(\" \"):\\n                    code += f\"{PYCON_CONTINUATION_PREFIX}\\\\n\"\\n                fragment = None\\n\\n        indentation = None\\n        for line in match[\"code\"].splitlines():\\n            orig_line, line = line, line.lstrip()\\n            if indentation is None and line:\\n                indentation = len(orig_line) - len(line)\\n            continuation_match = PYCON_CONTINUATION_RE.match(line)\\n            if continuation_match and fragment is not None:\\n                fragment += line[continuation_match.end() :] + \"\\\\n\"\\n            else:\\n                finish_fragment()\\n                if line.startswith(PYCON_PREFIX):\\n                    fragment = line[len(PYCON_PREFIX) :] + \"\\\\n\"\\n                else:\\n                    code += orig_line[indentation:] + \"\\\\n\"\\n        finish_fragment()\\n        return code\\n\\n    def _md_pycon_match(match: Match[str]) -> str:\\n        code = _pycon_match(match)\\n        code = textwrap.indent(code, match[\"indent\"])\\n        return f\\'{match[\"before\"]}{code}{match[\"after\"]}\\'\\n\\n    src = MD_RE.sub(_md_match, src)\\n    src = MD_PYCON_RE.sub(_md_pycon_match, src)\\n    return src, errors\\n\\n',\n",
       "  'function_name': 'format_str',\n",
       "  'filepath': '/home/moonpatel/openai-python/bin/ruffen-docs.py'},\n",
       " {'code': 'def format_code_block(code: str) -> str:\\n    return subprocess.check_output(\\n        [\\n            sys.executable,\\n            \"-m\",\\n            \"ruff\",\\n            \"format\",\\n            \"--stdin-filename=script.py\",\\n            f\"--line-length={DEFAULT_LINE_LENGTH}\",\\n        ],\\n        encoding=\"utf-8\",\\n        input=code,\\n    )\\n\\n',\n",
       "  'function_name': 'format_code_block',\n",
       "  'filepath': '/home/moonpatel/openai-python/bin/ruffen-docs.py'},\n",
       " {'code': 'def format_file(\\n    filename: str,\\n    skip_errors: bool,\\n) -> int:\\n    with open(filename, encoding=\"UTF-8\") as f:\\n        contents = f.read()\\n    new_contents, errors = format_str(contents)\\n    for error in errors:\\n        lineno = contents[: error.offset].count(\"\\\\n\") + 1\\n        print(f\"{filename}:{lineno}: code block parse error {error.exc}\")\\n    if errors and not skip_errors:\\n        return 1\\n    if contents != new_contents:\\n        print(f\"{filename}: Rewriting...\")\\n        with open(filename, \"w\", encoding=\"UTF-8\") as f:\\n            f.write(new_contents)\\n        return 0\\n    else:\\n        return 0\\n\\n',\n",
       "  'function_name': 'format_file',\n",
       "  'filepath': '/home/moonpatel/openai-python/bin/ruffen-docs.py'},\n",
       " {'code': 'def main(argv: Sequence[str] | None = None) -> int:\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\n        \"-l\",\\n        \"--line-length\",\\n        type=int,\\n        default=DEFAULT_LINE_LENGTH,\\n    )\\n    parser.add_argument(\\n        \"-S\",\\n        \"--skip-string-normalization\",\\n        action=\"store_true\",\\n    )\\n    parser.add_argument(\"-E\", \"--skip-errors\", action=\"store_true\")\\n    parser.add_argument(\"filenames\", nargs=\"*\")\\n    args = parser.parse_args(argv)\\n\\n    retv = 0\\n    for filename in args.filenames:\\n        retv |= format_file(filename, skip_errors=args.skip_errors)\\n    return retv\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/bin/ruffen-docs.py'},\n",
       " {'code': 'def should_run_sync() -> bool:\\n    dev_lock = Path(__file__).parent.parent.joinpath(\"requirements-dev.lock\")\\n\\n    for line in dev_lock.read_text().splitlines():\\n        if not line or line.startswith(\"#\") or line.startswith(\"-e\"):\\n            continue\\n\\n        dep, lock_version = line.split(\"==\")\\n\\n        try:\\n            version = importlib_metadata.version(dep)\\n\\n            if lock_version != version:\\n                print(f\"mismatch for {dep} current={version} lock={lock_version}\")\\n                return True\\n        except Exception:\\n            print(f\"could not import {dep}\")\\n            return True\\n\\n    return False\\n\\n',\n",
       "  'function_name': 'should_run_sync',\n",
       "  'filepath': '/home/moonpatel/openai-python/bin/check-env-state.py'},\n",
       " {'code': 'def main() -> None:\\n    if should_run_sync():\\n        exit(1)\\n    else:\\n        exit(0)\\n\\n',\n",
       "  'function_name': 'main',\n",
       "  'filepath': '/home/moonpatel/openai-python/bin/check-env-state.py'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading\n",
    "\n",
    "root_dir = Path.home()\n",
    "\n",
    "REPO_NAME = 'openai-python'\n",
    "\n",
    "code_root = root_dir / REPO_NAME\n",
    "\n",
    "all_funcs = extract_functions_from_repo(code_root)\n",
    "\n",
    "all_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def test_pydantic_v1(session: nox.Session) -&gt; ...</td>\n",
       "      <td>test_pydantic_v1</td>\n",
       "      <td>/home/moonpatel/openai-python/noxfile.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>async def transform(\\n    data: _T,\\n    expec...</td>\n",
       "      <td>transform</td>\n",
       "      <td>/home/moonpatel/openai-python/tests/test_trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>async def test_top_level_alias(use_async: bool...</td>\n",
       "      <td>test_top_level_alias</td>\n",
       "      <td>/home/moonpatel/openai-python/tests/test_trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code         function_name  \\\n",
       "0  def test_pydantic_v1(session: nox.Session) -> ...      test_pydantic_v1   \n",
       "1  async def transform(\\n    data: _T,\\n    expec...             transform   \n",
       "2  async def test_top_level_alias(use_async: bool...  test_top_level_alias   \n",
       "\n",
       "                                            filepath  \n",
       "0           /home/moonpatel/openai-python/noxfile.py  \n",
       "1  /home/moonpatel/openai-python/tests/test_trans...  \n",
       "2  /home/moonpatel/openai-python/tests/test_trans...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_funcs[:3])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"\",\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "\n",
    "df['code_embedding'] = df['code'].apply(lambda x: client.embeddings.create(input=x, model=\"text-embedding-3-small\").data[0].embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "      <th>code_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def test_pydantic_v1(session: nox.Session) -&gt; ...</td>\n",
       "      <td>test_pydantic_v1</td>\n",
       "      <td>/home/moonpatel/openai-python/noxfile.py</td>\n",
       "      <td>[0.015476400032639503, 0.008651133626699448, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>async def transform(\\n    data: _T,\\n    expec...</td>\n",
       "      <td>transform</td>\n",
       "      <td>/home/moonpatel/openai-python/tests/test_trans...</td>\n",
       "      <td>[-0.004722085315734148, 0.030964845791459084, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>async def test_top_level_alias(use_async: bool...</td>\n",
       "      <td>test_top_level_alias</td>\n",
       "      <td>/home/moonpatel/openai-python/tests/test_trans...</td>\n",
       "      <td>[0.004833014216274023, -0.019827749580144882, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code         function_name  \\\n",
       "0  def test_pydantic_v1(session: nox.Session) -> ...      test_pydantic_v1   \n",
       "1  async def transform(\\n    data: _T,\\n    expec...             transform   \n",
       "2  async def test_top_level_alias(use_async: bool...  test_top_level_alias   \n",
       "\n",
       "                                            filepath  \\\n",
       "0           /home/moonpatel/openai-python/noxfile.py   \n",
       "1  /home/moonpatel/openai-python/tests/test_trans...   \n",
       "2  /home/moonpatel/openai-python/tests/test_trans...   \n",
       "\n",
       "                                      code_embedding  \n",
       "0  [0.015476400032639503, 0.008651133626699448, 0...  \n",
       "1  [-0.004722085315734148, 0.030964845791459084, ...  \n",
       "2  [0.004833014216274023, -0.019827749580144882, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "    embedding = client.embeddings.create(input=code_query, model='text-embedding-3-small').data[0].embedding\n",
    "    df['similarities'] = df['code_embedding'].apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    print(df['similarities'])\n",
    "\n",
    "    if pprint:\n",
    "        for r in res.iterrows():\n",
    "            print(f\"{r[1].filepath}:{r[1].function_name}  score={round(r[1].similarities, 3)}\")\n",
    "            print(\"\\n\".join(r[1].code.split(\"\\n\")[:n_lines]))\n",
    "            print('-' * 70)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.114624\n",
      "1    0.172824\n",
      "2    0.070478\n",
      "Name: similarities, dtype: float64\n",
      "/home/moonpatel/openai-python/tests/test_transform.py:transform  score=0.173\n",
      "async def transform(\n",
      "    data: _T,\n",
      "    expected_type: object,\n",
      "    use_async: bool,\n",
      ") -> _T:\n",
      "    if use_async:\n",
      "        return await _async_transform(data, expected_type=expected_type)\n",
      "----------------------------------------------------------------------\n",
      "/home/moonpatel/openai-python/noxfile.py:test_pydantic_v1  score=0.115\n",
      "def test_pydantic_v1(session: nox.Session) -> None:\n",
      "    session.install(\"-r\", \"requirements-dev.lock\")\n",
      "    session.install(\"pydantic<2\")\n",
      "\n",
      "    session.run(\"pytest\", \"--showlocals\", \"--ignore=tests/functional\", *session.posargs)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "/home/moonpatel/openai-python/tests/test_transform.py:test_top_level_alias  score=0.07\n",
      "async def test_top_level_alias(use_async: bool) -> None:\n",
      "    assert await transform({\"foo_bar\": \"hello\"}, expected_type=Foo1, use_async=use_async) == {\"fooBar\": \"hello\"}\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "      <th>code_embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>async def transform(\\n    data: _T,\\n    expec...</td>\n",
       "      <td>transform</td>\n",
       "      <td>/home/moonpatel/openai-python/tests/test_trans...</td>\n",
       "      <td>[-0.004722085315734148, 0.030964845791459084, ...</td>\n",
       "      <td>0.172824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def test_pydantic_v1(session: nox.Session) -&gt; ...</td>\n",
       "      <td>test_pydantic_v1</td>\n",
       "      <td>/home/moonpatel/openai-python/noxfile.py</td>\n",
       "      <td>[0.015476400032639503, 0.008651133626699448, 0...</td>\n",
       "      <td>0.114624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>async def test_top_level_alias(use_async: bool...</td>\n",
       "      <td>test_top_level_alias</td>\n",
       "      <td>/home/moonpatel/openai-python/tests/test_trans...</td>\n",
       "      <td>[0.004833014216274023, -0.019827749580144882, ...</td>\n",
       "      <td>0.070478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code         function_name  \\\n",
       "1  async def transform(\\n    data: _T,\\n    expec...             transform   \n",
       "0  def test_pydantic_v1(session: nox.Session) -> ...      test_pydantic_v1   \n",
       "2  async def test_top_level_alias(use_async: bool...  test_top_level_alias   \n",
       "\n",
       "                                            filepath  \\\n",
       "1  /home/moonpatel/openai-python/tests/test_trans...   \n",
       "0           /home/moonpatel/openai-python/noxfile.py   \n",
       "2  /home/moonpatel/openai-python/tests/test_trans...   \n",
       "\n",
       "                                      code_embedding  similarities  \n",
       "1  [-0.004722085315734148, 0.030964845791459084, ...      0.172824  \n",
       "0  [0.015476400032639503, 0.008651133626699448, 0...      0.114624  \n",
       "2  [0.004833014216274023, -0.019827749580144882, ...      0.070478  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = search_functions(df, 'fine-tuning input data validation logic', n=3)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
